{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将大模型load进来\n",
    "\n",
    "并且取出对应的词嵌入\n",
    "\n",
    "```\n",
    "Qwen2Config {\n",
    "  \"_name_or_path\": \"D:/models/Qwen2.5-7B-Instruct\",\n",
    "  \"architectures\": [\n",
    "    \"Qwen2ForCausalLM\"\n",
    "  ],\n",
    "  \"attention_dropout\": 0.0,\n",
    "  \"bos_token_id\": 151643,\n",
    "  \"eos_token_id\": 151645,\n",
    "  \"hidden_act\": \"silu\",\n",
    "  \"hidden_size\": 3584,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 18944,\n",
    "  \"max_position_embeddings\": 32768,\n",
    "  \"max_window_layers\": 28,\n",
    "  \"model_type\": \"qwen2\",\n",
    "  \"num_attention_heads\": 28,\n",
    "  \"num_hidden_layers\": 28,\n",
    "  \"num_key_value_heads\": 4,\n",
    "  \"rms_norm_eps\": 1e-06,\n",
    "  \"rope_scaling\": null,\n",
    "  \"rope_theta\": 1000000.0,\n",
    "  \"sliding_window\": null,\n",
    "  \"tie_word_embeddings\": false,\n",
    "  \"torch_dtype\": \"bfloat16\",\n",
    "  \"transformers_version\": \"4.45.2\",\n",
    "  \"use_cache\": true,\n",
    "  \"use_sliding_window\": false,\n",
    "  \"vocab_size\": 152064\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"D:/models/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 13.58it/s]\n",
      "Some weights of Qwen2ForCausalLM were not initialized from the model checkpoint at D:/models/Qwen2.5-7B-Instruct and are newly initialized: ['model.vae.decoder_bn_layers.0.bias', 'model.vae.decoder_bn_layers.0.num_batches_tracked', 'model.vae.decoder_bn_layers.0.running_mean', 'model.vae.decoder_bn_layers.0.running_var', 'model.vae.decoder_bn_layers.0.weight', 'model.vae.decoder_bn_layers.1.bias', 'model.vae.decoder_bn_layers.1.num_batches_tracked', 'model.vae.decoder_bn_layers.1.running_mean', 'model.vae.decoder_bn_layers.1.running_var', 'model.vae.decoder_bn_layers.1.weight', 'model.vae.decoder_bn_layers.2.bias', 'model.vae.decoder_bn_layers.2.num_batches_tracked', 'model.vae.decoder_bn_layers.2.running_mean', 'model.vae.decoder_bn_layers.2.running_var', 'model.vae.decoder_bn_layers.2.weight', 'model.vae.decoder_bn_layers.3.bias', 'model.vae.decoder_bn_layers.3.num_batches_tracked', 'model.vae.decoder_bn_layers.3.running_mean', 'model.vae.decoder_bn_layers.3.running_var', 'model.vae.decoder_bn_layers.3.weight', 'model.vae.decoder_bn_layers.4.bias', 'model.vae.decoder_bn_layers.4.num_batches_tracked', 'model.vae.decoder_bn_layers.4.running_mean', 'model.vae.decoder_bn_layers.4.running_var', 'model.vae.decoder_bn_layers.4.weight', 'model.vae.decoder_bn_layers.5.bias', 'model.vae.decoder_bn_layers.5.num_batches_tracked', 'model.vae.decoder_bn_layers.5.running_mean', 'model.vae.decoder_bn_layers.5.running_var', 'model.vae.decoder_bn_layers.5.weight', 'model.vae.decoder_input.bias', 'model.vae.decoder_input.weight', 'model.vae.decoder_lstm_layers.0.bias_hh_l0', 'model.vae.decoder_lstm_layers.0.bias_hh_l0_reverse', 'model.vae.decoder_lstm_layers.0.bias_ih_l0', 'model.vae.decoder_lstm_layers.0.bias_ih_l0_reverse', 'model.vae.decoder_lstm_layers.0.weight_hh_l0', 'model.vae.decoder_lstm_layers.0.weight_hh_l0_reverse', 'model.vae.decoder_lstm_layers.0.weight_ih_l0', 'model.vae.decoder_lstm_layers.0.weight_ih_l0_reverse', 'model.vae.decoder_lstm_layers.1.bias_hh_l0', 'model.vae.decoder_lstm_layers.1.bias_hh_l0_reverse', 'model.vae.decoder_lstm_layers.1.bias_ih_l0', 'model.vae.decoder_lstm_layers.1.bias_ih_l0_reverse', 'model.vae.decoder_lstm_layers.1.weight_hh_l0', 'model.vae.decoder_lstm_layers.1.weight_hh_l0_reverse', 'model.vae.decoder_lstm_layers.1.weight_ih_l0', 'model.vae.decoder_lstm_layers.1.weight_ih_l0_reverse', 'model.vae.decoder_lstm_layers.2.bias_hh_l0', 'model.vae.decoder_lstm_layers.2.bias_hh_l0_reverse', 'model.vae.decoder_lstm_layers.2.bias_ih_l0', 'model.vae.decoder_lstm_layers.2.bias_ih_l0_reverse', 'model.vae.decoder_lstm_layers.2.weight_hh_l0', 'model.vae.decoder_lstm_layers.2.weight_hh_l0_reverse', 'model.vae.decoder_lstm_layers.2.weight_ih_l0', 'model.vae.decoder_lstm_layers.2.weight_ih_l0_reverse', 'model.vae.decoder_lstm_layers.3.bias_hh_l0', 'model.vae.decoder_lstm_layers.3.bias_hh_l0_reverse', 'model.vae.decoder_lstm_layers.3.bias_ih_l0', 'model.vae.decoder_lstm_layers.3.bias_ih_l0_reverse', 'model.vae.decoder_lstm_layers.3.weight_hh_l0', 'model.vae.decoder_lstm_layers.3.weight_hh_l0_reverse', 'model.vae.decoder_lstm_layers.3.weight_ih_l0', 'model.vae.decoder_lstm_layers.3.weight_ih_l0_reverse', 'model.vae.decoder_lstm_layers.4.bias_hh_l0', 'model.vae.decoder_lstm_layers.4.bias_hh_l0_reverse', 'model.vae.decoder_lstm_layers.4.bias_ih_l0', 'model.vae.decoder_lstm_layers.4.bias_ih_l0_reverse', 'model.vae.decoder_lstm_layers.4.weight_hh_l0', 'model.vae.decoder_lstm_layers.4.weight_hh_l0_reverse', 'model.vae.decoder_lstm_layers.4.weight_ih_l0', 'model.vae.decoder_lstm_layers.4.weight_ih_l0_reverse', 'model.vae.decoder_lstm_layers.5.bias_hh_l0', 'model.vae.decoder_lstm_layers.5.bias_hh_l0_reverse', 'model.vae.decoder_lstm_layers.5.bias_ih_l0', 'model.vae.decoder_lstm_layers.5.bias_ih_l0_reverse', 'model.vae.decoder_lstm_layers.5.weight_hh_l0', 'model.vae.decoder_lstm_layers.5.weight_hh_l0_reverse', 'model.vae.decoder_lstm_layers.5.weight_ih_l0', 'model.vae.decoder_lstm_layers.5.weight_ih_l0_reverse', 'model.vae.encoder_bn_layers.0.bias', 'model.vae.encoder_bn_layers.0.num_batches_tracked', 'model.vae.encoder_bn_layers.0.running_mean', 'model.vae.encoder_bn_layers.0.running_var', 'model.vae.encoder_bn_layers.0.weight', 'model.vae.encoder_bn_layers.1.bias', 'model.vae.encoder_bn_layers.1.num_batches_tracked', 'model.vae.encoder_bn_layers.1.running_mean', 'model.vae.encoder_bn_layers.1.running_var', 'model.vae.encoder_bn_layers.1.weight', 'model.vae.encoder_bn_layers.2.bias', 'model.vae.encoder_bn_layers.2.num_batches_tracked', 'model.vae.encoder_bn_layers.2.running_mean', 'model.vae.encoder_bn_layers.2.running_var', 'model.vae.encoder_bn_layers.2.weight', 'model.vae.encoder_bn_layers.3.bias', 'model.vae.encoder_bn_layers.3.num_batches_tracked', 'model.vae.encoder_bn_layers.3.running_mean', 'model.vae.encoder_bn_layers.3.running_var', 'model.vae.encoder_bn_layers.3.weight', 'model.vae.encoder_bn_layers.4.bias', 'model.vae.encoder_bn_layers.4.num_batches_tracked', 'model.vae.encoder_bn_layers.4.running_mean', 'model.vae.encoder_bn_layers.4.running_var', 'model.vae.encoder_bn_layers.4.weight', 'model.vae.encoder_bn_layers.5.bias', 'model.vae.encoder_bn_layers.5.num_batches_tracked', 'model.vae.encoder_bn_layers.5.running_mean', 'model.vae.encoder_bn_layers.5.running_var', 'model.vae.encoder_bn_layers.5.weight', 'model.vae.encoder_lstm_layers.0.bias_hh_l0', 'model.vae.encoder_lstm_layers.0.bias_hh_l0_reverse', 'model.vae.encoder_lstm_layers.0.bias_hh_l1', 'model.vae.encoder_lstm_layers.0.bias_hh_l1_reverse', 'model.vae.encoder_lstm_layers.0.bias_ih_l0', 'model.vae.encoder_lstm_layers.0.bias_ih_l0_reverse', 'model.vae.encoder_lstm_layers.0.bias_ih_l1', 'model.vae.encoder_lstm_layers.0.bias_ih_l1_reverse', 'model.vae.encoder_lstm_layers.0.weight_hh_l0', 'model.vae.encoder_lstm_layers.0.weight_hh_l0_reverse', 'model.vae.encoder_lstm_layers.0.weight_hh_l1', 'model.vae.encoder_lstm_layers.0.weight_hh_l1_reverse', 'model.vae.encoder_lstm_layers.0.weight_ih_l0', 'model.vae.encoder_lstm_layers.0.weight_ih_l0_reverse', 'model.vae.encoder_lstm_layers.0.weight_ih_l1', 'model.vae.encoder_lstm_layers.0.weight_ih_l1_reverse', 'model.vae.encoder_lstm_layers.1.bias_hh_l0', 'model.vae.encoder_lstm_layers.1.bias_hh_l0_reverse', 'model.vae.encoder_lstm_layers.1.bias_hh_l1', 'model.vae.encoder_lstm_layers.1.bias_hh_l1_reverse', 'model.vae.encoder_lstm_layers.1.bias_ih_l0', 'model.vae.encoder_lstm_layers.1.bias_ih_l0_reverse', 'model.vae.encoder_lstm_layers.1.bias_ih_l1', 'model.vae.encoder_lstm_layers.1.bias_ih_l1_reverse', 'model.vae.encoder_lstm_layers.1.weight_hh_l0', 'model.vae.encoder_lstm_layers.1.weight_hh_l0_reverse', 'model.vae.encoder_lstm_layers.1.weight_hh_l1', 'model.vae.encoder_lstm_layers.1.weight_hh_l1_reverse', 'model.vae.encoder_lstm_layers.1.weight_ih_l0', 'model.vae.encoder_lstm_layers.1.weight_ih_l0_reverse', 'model.vae.encoder_lstm_layers.1.weight_ih_l1', 'model.vae.encoder_lstm_layers.1.weight_ih_l1_reverse', 'model.vae.encoder_lstm_layers.2.bias_hh_l0', 'model.vae.encoder_lstm_layers.2.bias_hh_l0_reverse', 'model.vae.encoder_lstm_layers.2.bias_hh_l1', 'model.vae.encoder_lstm_layers.2.bias_hh_l1_reverse', 'model.vae.encoder_lstm_layers.2.bias_ih_l0', 'model.vae.encoder_lstm_layers.2.bias_ih_l0_reverse', 'model.vae.encoder_lstm_layers.2.bias_ih_l1', 'model.vae.encoder_lstm_layers.2.bias_ih_l1_reverse', 'model.vae.encoder_lstm_layers.2.weight_hh_l0', 'model.vae.encoder_lstm_layers.2.weight_hh_l0_reverse', 'model.vae.encoder_lstm_layers.2.weight_hh_l1', 'model.vae.encoder_lstm_layers.2.weight_hh_l1_reverse', 'model.vae.encoder_lstm_layers.2.weight_ih_l0', 'model.vae.encoder_lstm_layers.2.weight_ih_l0_reverse', 'model.vae.encoder_lstm_layers.2.weight_ih_l1', 'model.vae.encoder_lstm_layers.2.weight_ih_l1_reverse', 'model.vae.encoder_lstm_layers.3.bias_hh_l0', 'model.vae.encoder_lstm_layers.3.bias_hh_l0_reverse', 'model.vae.encoder_lstm_layers.3.bias_hh_l1', 'model.vae.encoder_lstm_layers.3.bias_hh_l1_reverse', 'model.vae.encoder_lstm_layers.3.bias_ih_l0', 'model.vae.encoder_lstm_layers.3.bias_ih_l0_reverse', 'model.vae.encoder_lstm_layers.3.bias_ih_l1', 'model.vae.encoder_lstm_layers.3.bias_ih_l1_reverse', 'model.vae.encoder_lstm_layers.3.weight_hh_l0', 'model.vae.encoder_lstm_layers.3.weight_hh_l0_reverse', 'model.vae.encoder_lstm_layers.3.weight_hh_l1', 'model.vae.encoder_lstm_layers.3.weight_hh_l1_reverse', 'model.vae.encoder_lstm_layers.3.weight_ih_l0', 'model.vae.encoder_lstm_layers.3.weight_ih_l0_reverse', 'model.vae.encoder_lstm_layers.3.weight_ih_l1', 'model.vae.encoder_lstm_layers.3.weight_ih_l1_reverse', 'model.vae.encoder_lstm_layers.4.bias_hh_l0', 'model.vae.encoder_lstm_layers.4.bias_hh_l0_reverse', 'model.vae.encoder_lstm_layers.4.bias_hh_l1', 'model.vae.encoder_lstm_layers.4.bias_hh_l1_reverse', 'model.vae.encoder_lstm_layers.4.bias_ih_l0', 'model.vae.encoder_lstm_layers.4.bias_ih_l0_reverse', 'model.vae.encoder_lstm_layers.4.bias_ih_l1', 'model.vae.encoder_lstm_layers.4.bias_ih_l1_reverse', 'model.vae.encoder_lstm_layers.4.weight_hh_l0', 'model.vae.encoder_lstm_layers.4.weight_hh_l0_reverse', 'model.vae.encoder_lstm_layers.4.weight_hh_l1', 'model.vae.encoder_lstm_layers.4.weight_hh_l1_reverse', 'model.vae.encoder_lstm_layers.4.weight_ih_l0', 'model.vae.encoder_lstm_layers.4.weight_ih_l0_reverse', 'model.vae.encoder_lstm_layers.4.weight_ih_l1', 'model.vae.encoder_lstm_layers.4.weight_ih_l1_reverse', 'model.vae.encoder_lstm_layers.5.bias_hh_l0', 'model.vae.encoder_lstm_layers.5.bias_hh_l0_reverse', 'model.vae.encoder_lstm_layers.5.bias_hh_l1', 'model.vae.encoder_lstm_layers.5.bias_hh_l1_reverse', 'model.vae.encoder_lstm_layers.5.bias_ih_l0', 'model.vae.encoder_lstm_layers.5.bias_ih_l0_reverse', 'model.vae.encoder_lstm_layers.5.bias_ih_l1', 'model.vae.encoder_lstm_layers.5.bias_ih_l1_reverse', 'model.vae.encoder_lstm_layers.5.weight_hh_l0', 'model.vae.encoder_lstm_layers.5.weight_hh_l0_reverse', 'model.vae.encoder_lstm_layers.5.weight_hh_l1', 'model.vae.encoder_lstm_layers.5.weight_hh_l1_reverse', 'model.vae.encoder_lstm_layers.5.weight_ih_l0', 'model.vae.encoder_lstm_layers.5.weight_ih_l0_reverse', 'model.vae.encoder_lstm_layers.5.weight_ih_l1', 'model.vae.encoder_lstm_layers.5.weight_ih_l1_reverse', 'model.vae.fc_mu.bias', 'model.vae.fc_mu.weight', 'model.vae.fc_var.bias', 'model.vae.fc_var.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from qwen_source import  Qwen2ForCausalLM\n",
    "model =  Qwen2ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype = 'auto',\n",
    "    device_map = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 10\n",
    "lr = 5e-5\n",
    "sequence_length = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_optim_dict = []\n",
    "qwen_optim_dict = []\n",
    "for name, param in model.named_parameters():\n",
    "    # split out the layer number\n",
    "    name_l = name.split('.')\n",
    "    layer_num = name_l[2] if name_l[1] == 'layers' else None\n",
    "    if name_l[1] == \"embed_tokens\":\n",
    "        param.requires_grad = False\n",
    "    elif name_l[1] == \"vae\":\n",
    "        vae_optim_dict.append({\"params\" : param, 'lr' : lr})\n",
    "    \n",
    "    if layer_num is not None:\n",
    "        # frozen the lower layers to preserve the features\n",
    "        # train the higher layers to fit into the specific downstream task\n",
    "        layer_num = int(layer_num)\n",
    "        if layer_num < 23:\n",
    "            param.requires_grad = False\n",
    "        elif layer_num >= 23:\n",
    "            param.requires_grad = True\n",
    "            qwen_optim_dict.append({\"params\" : param, 'lr' : lr})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': Parameter containing:\n",
       "  tensor([[ 0.0073,  0.0101, -0.0306,  ...,  0.0028,  0.0239,  0.0106],\n",
       "          [-0.0176, -0.0137, -0.0250,  ...,  0.0030,  0.0254,  0.0051],\n",
       "          [-0.0236, -0.0111,  0.0029,  ..., -0.0104,  0.0015, -0.0266],\n",
       "          ...,\n",
       "          [-0.0084,  0.0072,  0.0063,  ...,  0.0117, -0.0132,  0.0099],\n",
       "          [ 0.0349,  0.0206, -0.0105,  ..., -0.0013,  0.0186, -0.0165],\n",
       "          [ 0.0112, -0.0164, -0.0049,  ..., -0.0074, -0.0020,  0.0082]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 0.1582, -0.5000, -0.0292,  ..., -0.8125,  0.3203,  0.0640],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0046,  0.0066,  0.0040,  ...,  0.0113,  0.0115, -0.0036],\n",
       "          [ 0.0237, -0.0014,  0.0182,  ...,  0.0095, -0.0041, -0.0096],\n",
       "          [-0.0044,  0.0003,  0.0153,  ..., -0.0053,  0.0097,  0.0131],\n",
       "          ...,\n",
       "          [ 0.0049, -0.0004, -0.0128,  ...,  0.0021, -0.0256,  0.0140],\n",
       "          [ 0.0109,  0.0138, -0.0211,  ...,  0.0381, -0.0066, -0.0270],\n",
       "          [ 0.0176,  0.0239,  0.0197,  ..., -0.0014, -0.0142, -0.0037]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([-8.4766e-01,  7.3242e-02,  8.3984e-02,  5.3516e-01,  2.7344e-01,\n",
       "          -5.0781e-01, -3.0859e-01,  5.8899e-03,  5.6250e-01,  1.5723e-01,\n",
       "          -1.6016e-01,  4.3701e-02, -1.8555e-02, -1.3477e-01, -3.4180e-01,\n",
       "          -7.9102e-02,  1.2305e-01,  6.1719e-01,  1.7773e-01,  7.0312e-02,\n",
       "          -9.1797e-01, -1.5039e-01,  2.1362e-02,  2.7930e-01, -8.2422e-01,\n",
       "          -2.8125e-01,  2.4414e-02, -1.2266e+00,  2.2070e-01, -3.9307e-02,\n",
       "          -5.7422e-01, -4.8340e-02, -1.3086e-01,  3.7891e-01, -3.7109e-02,\n",
       "          -1.2061e-01, -1.3438e+00,  4.2725e-02, -3.3594e-01, -1.2061e-01,\n",
       "          -9.3262e-02, -5.9814e-02,  3.8147e-04, -2.9102e-01,  2.1777e-01,\n",
       "          -2.1875e-01, -1.1523e-01, -1.5625e-01,  1.2988e-01, -9.1309e-02,\n",
       "           2.4048e-02,  2.4023e-01, -2.3926e-01,  8.0566e-02, -7.6172e-02,\n",
       "           1.4453e-01,  2.2217e-02, -6.4844e-01,  7.5195e-02, -1.2256e-01,\n",
       "          -2.6367e-01,  6.4062e-01, -3.4531e+00, -1.6113e-01, -5.3223e-02,\n",
       "          -4.6875e-01, -2.2754e-01,  7.3047e-01,  1.2793e-01,  4.4922e-01,\n",
       "           1.8066e-01, -2.2070e-01,  6.5234e-01, -7.3242e-02,  6.6797e-01,\n",
       "          -2.4219e-01, -3.1250e-02,  1.6309e-01,  1.0938e+00, -1.4062e-01,\n",
       "          -1.8164e-01, -2.5000e-01, -4.1504e-02,  1.5320e-02,  3.0859e-01,\n",
       "          -9.9609e-02, -6.4453e-02, -3.9062e-01, -7.9297e-01,  2.5000e-01,\n",
       "           1.1572e-01, -1.1230e-01,  2.4902e-02,  1.4355e-01, -1.3984e+00,\n",
       "          -9.6191e-02, -6.5918e-02, -2.0996e-01, -2.5024e-02, -2.1387e-01,\n",
       "           4.4922e-01, -7.1289e-02, -1.7188e-01,  8.3008e-03, -7.3242e-02,\n",
       "          -6.4392e-03,  1.2734e+00, -7.7637e-02, -2.9102e-01, -2.0996e-01,\n",
       "           2.6953e-01,  4.6143e-02,  3.0938e+00,  1.3855e-02, -2.0020e-02,\n",
       "          -6.7383e-02, -1.1670e-01,  1.5723e-01, -2.1484e-01,  2.4609e-01,\n",
       "          -1.8848e-01, -1.7090e-01, -3.7305e-01, -5.3516e-01,  2.1777e-01,\n",
       "          -1.2988e-01, -3.6250e+00,  4.3945e-01,  1.9043e-01,  5.3516e-01,\n",
       "           5.0391e-01,  1.2891e-01, -7.5195e-02, -2.1875e-01, -1.5039e-01,\n",
       "           5.2490e-02,  6.8359e-02, -2.3242e-01, -2.0447e-03, -5.7031e-01,\n",
       "           1.6016e-01,  6.9824e-02, -2.7734e-01, -3.7500e-01, -1.5820e-01,\n",
       "          -3.8281e-01,  5.5908e-02, -4.7070e-01,  1.0469e+00,  3.9453e-01,\n",
       "          -2.3730e-01,  2.5757e-02, -8.0078e-01,  1.1426e-01,  1.5859e+00,\n",
       "           9.7656e-02,  4.3945e-01, -3.3203e-02, -2.3730e-01,  2.4531e+00,\n",
       "          -8.3984e-02,  1.2695e-01,  2.7734e-01,  1.1865e-01, -1.4844e-01,\n",
       "          -4.3213e-02, -3.3936e-02,  3.4424e-02, -7.2754e-02,  4.1602e-01,\n",
       "          -3.7079e-03,  3.8672e-01, -8.2812e-01, -1.5137e-01,  3.7344e+00,\n",
       "          -1.9336e-01,  2.4658e-02,  2.6172e-01,  2.5391e-01, -1.3477e-01,\n",
       "          -2.1289e-01, -2.4658e-02, -1.7871e-01, -1.6309e-01,  1.8066e-01,\n",
       "          -3.8281e-01,  6.7188e-01,  5.8594e-01, -6.3477e-02, -2.4219e+00,\n",
       "          -1.1279e-01, -4.1602e-01,  6.3281e-01,  9.8633e-02, -1.9727e-01,\n",
       "          -2.5000e-01,  2.5586e-01, -6.8750e-01,  3.8086e-01, -3.3789e-01,\n",
       "           2.1680e-01, -7.8125e-01, -3.1055e-01, -3.8672e-01,  1.4941e-01,\n",
       "           4.1016e-01, -9.6484e-01,  3.5156e-02, -3.5156e-01, -9.6875e-01,\n",
       "           2.0605e-01, -1.9238e-01, -2.0312e-01, -9.8877e-03, -8.3984e-01,\n",
       "           5.6250e-01, -2.2217e-02,  9.2969e-01, -2.7734e-01,  1.5918e-01,\n",
       "           9.2969e-01, -5.4688e-02,  1.7578e-01, -1.7871e-01, -7.4219e-02,\n",
       "           1.1377e-01,  5.2734e-01,  2.5195e-01, -1.7090e-01, -6.5918e-02,\n",
       "           2.1719e+00, -8.3496e-02,  2.8711e-01,  2.5625e+00, -1.7773e-01,\n",
       "           1.3672e-01,  1.2109e-01, -9.2285e-02, -8.7500e-01, -5.9375e-01,\n",
       "          -1.5625e-01, -1.9922e-01,  2.3828e-01,  2.2266e-01,  6.6016e-01,\n",
       "           9.7046e-03, -3.7305e-01, -1.3477e-01,  4.6875e-01, -3.3594e-01,\n",
       "          -1.4453e-01,  6.1279e-02,  2.0410e-01,  3.6094e+00, -5.7422e-01,\n",
       "           6.0547e-01,  6.3281e-01,  1.9141e-01, -2.1484e-01, -4.7461e-01,\n",
       "           5.3906e-01, -2.7930e-01, -2.2754e-01, -6.8359e-01, -1.7578e-01,\n",
       "          -2.8906e-01, -1.7676e-01,  1.1597e-02,  1.3086e-01,  1.4941e-01,\n",
       "          -1.1963e-01,  3.8330e-02,  1.0156e+00, -2.1680e-01,  2.2363e-01,\n",
       "          -9.9219e-01, -3.9062e-02, -7.0312e-02,  8.7402e-02,  3.2471e-02,\n",
       "           1.7285e-01,  1.4258e-01,  7.5000e-01, -1.7285e-01,  1.0889e-01,\n",
       "           8.0859e-01, -1.2451e-01,  2.1094e-01,  6.8359e-02, -1.1094e+00,\n",
       "           6.6895e-02, -1.5918e-01, -3.9307e-02, -5.5176e-02,  1.6953e+00,\n",
       "          -3.9844e-01,  2.2461e-01, -1.2793e-01, -2.2559e-01,  3.7109e-01,\n",
       "          -1.8652e-01,  8.8281e-01, -2.1875e+00,  4.9219e-01, -8.2520e-02,\n",
       "          -1.1182e-01, -7.9346e-03,  5.8105e-02, -2.2363e-01, -8.5449e-02,\n",
       "          -1.3379e-01, -6.0791e-02,  1.4062e-01, -5.3906e-01,  5.7617e-02,\n",
       "           3.5352e-01, -3.3438e+00,  2.7539e-01, -2.5391e-01,  3.4375e-01,\n",
       "          -2.5781e-01, -3.8867e-01,  3.6133e-01,  1.0498e-01, -1.6895e-01,\n",
       "           1.6992e-01,  8.2422e-01,  5.5420e-02,  4.6875e-02, -2.7734e-01,\n",
       "          -8.0859e-01, -1.8359e-01,  5.5469e-01, -3.2031e-01,  3.9062e-01,\n",
       "           3.9648e-01, -1.5039e-01, -1.4648e-01,  1.6895e-01,  3.5742e-01,\n",
       "           3.4961e-01, -9.4141e-01,  5.6641e-01,  7.5684e-02, -3.1250e-01,\n",
       "           1.2344e+00, -1.9434e-01, -4.8828e-01,  7.4707e-02, -4.7266e-01,\n",
       "          -2.8711e-01,  1.7422e+00, -1.3770e-01,  2.5586e-01, -4.1602e-01,\n",
       "           9.4604e-03,  2.5586e-01,  2.9688e-01,  2.7100e-02,  1.9043e-01,\n",
       "          -2.2754e-01, -2.7734e-01,  1.9297e+00,  2.7148e-01,  2.1875e-01,\n",
       "           2.5195e-01, -5.7812e-01,  4.0234e-01, -1.0205e-01, -1.0156e-01,\n",
       "          -1.1353e-02, -9.9609e-02, -1.0645e-01, -6.7383e-02,  7.0801e-02,\n",
       "          -6.7871e-02,  5.0781e-01,  9.6094e-01,  6.7871e-02, -4.2114e-03,\n",
       "           3.2969e+00, -3.4180e-01,  1.0938e-01, -8.4375e-01,  2.8711e-01,\n",
       "          -8.0469e-01,  4.7656e-01, -1.0547e+00,  4.4727e-01, -3.0859e-01,\n",
       "           2.8711e-01,  1.3125e+00,  2.5586e-01,  3.3691e-02,  3.9795e-02,\n",
       "          -2.2070e-01, -2.4512e-01,  1.5078e+00, -2.8906e-01, -2.4902e-01,\n",
       "          -4.0039e-01,  1.4062e-01, -2.9883e-01,  1.2891e-01, -2.1680e-01,\n",
       "           6.5430e-02,  2.9688e-01,  2.6758e-01, -9.1553e-03, -2.1582e-01,\n",
       "           3.0859e-01,  3.1641e-01, -6.7444e-03,  2.2461e-01,  1.3770e-01,\n",
       "          -2.4316e-01,  1.7383e-01,  1.9062e+00, -2.9883e-01, -1.3965e-01,\n",
       "          -2.2266e-01,  4.6692e-03,  9.2188e-01, -1.1035e-01,  5.6250e-01,\n",
       "          -5.4199e-02, -1.7773e-01, -9.8633e-02, -7.1777e-02,  4.1797e-01,\n",
       "           1.0303e-01, -1.7676e-01, -2.6172e-01,  2.1484e-02, -3.6328e-01,\n",
       "          -5.2812e+00, -4.3640e-03, -3.6621e-02, -1.9629e-01,  3.0823e-03,\n",
       "           2.1582e-01, -1.6797e-01, -6.7188e-01,  1.5703e+00, -9.7656e-02,\n",
       "           9.2285e-02,  8.9355e-02,  5.8350e-02, -1.2891e+00,  8.9355e-02,\n",
       "           4.4727e-01, -1.6797e-01,  2.7734e-01, -1.1047e-02,  3.1250e-02,\n",
       "           1.4844e-01,  6.2109e-01,  1.7871e-01, -3.9062e-01, -7.1484e-01,\n",
       "           3.8281e-01, -1.0889e-01,  2.0703e-01, -4.2188e-01, -1.7090e-01,\n",
       "           1.6328e+00, -1.5723e-01, -4.5117e-01,  2.1973e-01, -1.9922e-01,\n",
       "           7.0801e-02,  1.6875e+00,  6.6797e-01, -1.0596e-01,  1.0000e+00,\n",
       "          -1.6797e-01,  3.8867e-01,  7.2656e-01,  2.9297e-01, -3.5938e-01,\n",
       "           1.3574e-01,  9.6484e-01,  2.6758e-01, -2.8564e-02, -2.9883e-01,\n",
       "          -1.7578e-01,  2.9883e-01, -4.2480e-02, -2.5391e-01,  2.9297e-01,\n",
       "           1.9531e-01,  1.9238e-01,  4.7852e-01,  1.9922e-01,  3.2812e-01,\n",
       "          -7.6660e-02,  1.8359e-01, -2.6172e-01,  1.1670e-01,  1.1719e+00,\n",
       "           2.3145e-01, -5.1270e-02,  8.3496e-02, -8.2031e-02, -4.6289e-01,\n",
       "          -2.2461e-01, -3.6914e-01,  8.1641e-01,  3.6328e-01, -1.0156e-01,\n",
       "           4.0039e-01,  8.0859e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0056, -0.0250, -0.0047,  ...,  0.0152,  0.0084,  0.0014],\n",
       "          [-0.0027,  0.0311,  0.0170,  ..., -0.0009, -0.0142,  0.0012],\n",
       "          [-0.0190, -0.0043, -0.0061,  ..., -0.0432,  0.0125, -0.0055],\n",
       "          ...,\n",
       "          [-0.0084,  0.0288, -0.0045,  ...,  0.0383,  0.0015, -0.0244],\n",
       "          [ 0.0183,  0.0522, -0.0243,  ...,  0.0305, -0.0170,  0.0053],\n",
       "          [-0.0060, -0.0010,  0.0206,  ...,  0.0557, -0.0544,  0.0038]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 4.1504e-02, -5.2979e-02, -3.6865e-02, -1.3733e-02,  1.1902e-02,\n",
       "          -2.3804e-02, -1.0547e-01, -8.4473e-02, -6.2988e-02,  6.8359e-02,\n",
       "          -1.4038e-02, -5.9570e-02,  6.0547e-01, -2.7313e-03, -5.9570e-02,\n",
       "          -3.6865e-02,  7.8125e-02,  1.2061e-01,  3.8818e-02, -7.3730e-02,\n",
       "          -5.0293e-02,  3.1982e-02,  1.5442e-02, -6.1523e-02,  3.5400e-02,\n",
       "           6.9336e-02,  9.9609e-02, -7.7637e-02,  3.6133e-02,  6.0120e-03,\n",
       "           8.2031e-02, -6.3965e-02,  3.5645e-02, -8.6670e-03,  7.3242e-03,\n",
       "           4.9072e-02,  1.9897e-02, -3.1250e-02, -9.5825e-03,  4.6875e-02,\n",
       "          -9.8419e-04,  1.2329e-02,  1.0449e-01,  6.6895e-02,  7.4768e-03,\n",
       "           3.7354e-02,  3.5400e-02,  5.9082e-02,  1.1353e-02,  9.7656e-02,\n",
       "           6.0547e-02, -8.8501e-03,  4.7607e-03, -1.4832e-02,  1.3123e-02,\n",
       "           1.2891e-01, -3.3691e-02, -2.1362e-02, -6.4941e-02, -3.4180e-02,\n",
       "           1.8799e-02,  2.0599e-03,  5.5176e-02, -7.8125e-02,  4.9805e-02,\n",
       "          -4.9561e-02, -1.4648e-02, -5.7373e-02,  6.7871e-02,  4.4189e-02,\n",
       "          -1.4893e-02,  1.7578e-02,  1.8799e-02, -1.0742e-01, -1.6235e-02,\n",
       "           6.3477e-02,  3.7842e-02,  9.5703e-02, -3.8574e-02,  3.5645e-02,\n",
       "          -4.4434e-02,  1.7822e-02, -4.8584e-02,  4.4556e-03, -1.2817e-02,\n",
       "           1.5076e-02, -3.0640e-02, -3.7689e-03,  1.0803e-02, -5.3711e-02,\n",
       "          -8.3008e-03,  5.0293e-02, -1.5869e-02,  4.5471e-03, -4.9805e-02,\n",
       "           2.4414e-02,  1.2793e-01, -5.0049e-02, -3.0029e-02,  4.8828e-02,\n",
       "           1.7822e-02,  3.6377e-02,  3.1250e-02,  2.9907e-02, -1.3733e-02,\n",
       "          -7.9590e-02, -6.9824e-02, -3.4668e-02, -4.5898e-02,  1.5442e-02,\n",
       "          -1.4453e-01,  1.3062e-02, -2.7222e-02,  1.4404e-02,  6.5430e-02,\n",
       "           2.2217e-02, -2.3926e-02,  2.4414e-03,  2.4170e-02,  3.0884e-02,\n",
       "          -7.4707e-02, -1.7944e-02, -4.6692e-03, -1.2573e-02,  2.2461e-02,\n",
       "           2.2705e-02, -4.7607e-02,  3.5645e-02, -7.6660e-02,  8.4961e-02,\n",
       "          -2.0508e-02, -2.8198e-02, -1.5198e-02, -5.5176e-02, -7.3730e-02,\n",
       "          -1.6724e-02,  1.0254e-02, -3.8574e-02, -4.8828e-02, -9.2773e-02,\n",
       "           4.2236e-02, -2.9297e-02, -4.0771e-02,  9.0332e-03,  3.7354e-02,\n",
       "           5.6152e-02,  9.8633e-02,  9.9121e-02, -4.8828e-02, -5.9814e-02,\n",
       "          -5.1270e-02,  9.4604e-03,  1.8311e-02, -8.1055e-02,  2.1118e-02,\n",
       "          -9.7168e-02,  4.0283e-02, -1.0156e-01,  1.4648e-02,  2.8198e-02,\n",
       "           8.2520e-02, -1.4648e-01,  3.1494e-02,  4.7607e-03,  3.5400e-02,\n",
       "           2.7222e-02, -1.2512e-02, -4.5654e-02,  1.5723e-01,  1.4282e-02,\n",
       "          -1.2891e-01,  7.1289e-02,  1.2031e+00,  2.5513e-02, -1.7944e-02,\n",
       "           2.4048e-02,  3.7354e-02, -1.9684e-03, -7.5989e-03, -1.0864e-02,\n",
       "           2.3926e-02, -8.9722e-03,  4.5410e-02,  1.5918e-01,  1.4954e-02,\n",
       "           4.2725e-02, -4.9316e-02, -1.6403e-03,  3.7109e-02,  8.9844e-02,\n",
       "          -1.1780e-02, -1.2329e-02,  3.9551e-02, -8.9355e-02, -2.7924e-03,\n",
       "           1.1670e-01, -1.7212e-02,  4.6875e-02, -2.8839e-03,  1.3916e-02,\n",
       "          -1.3428e-02, -4.0283e-02, -2.2583e-02, -3.8330e-02,  3.1738e-02,\n",
       "           5.7861e-02, -7.9590e-02, -1.7700e-02, -5.4688e-02, -1.0925e-02,\n",
       "          -5.0049e-02,  4.5654e-02, -1.0303e-01, -2.4414e-02, -3.8330e-02,\n",
       "          -3.1494e-02, -9.3750e-02,  1.2109e-01,  3.2715e-02,  1.9409e-02,\n",
       "          -1.5320e-02,  5.5664e-02, -2.2095e-02,  3.8086e-02, -4.5654e-02,\n",
       "          -4.6387e-02,  9.3262e-02,  5.2734e-02, -1.7395e-03, -1.4465e-02,\n",
       "          -6.1035e-02,  1.2695e-02,  4.6875e-02,  4.2480e-02,  2.5940e-03,\n",
       "          -2.5195e-01, -7.5195e-02, -4.6875e-02, -3.9307e-02,  6.4453e-02,\n",
       "           1.5723e-01, -7.3242e-03,  6.9336e-02, -2.3193e-02, -1.0010e-01,\n",
       "          -6.1523e-02,  2.3926e-02, -7.4219e-02,  6.4392e-03, -4.8828e-03,\n",
       "          -1.0400e-01, -8.7402e-02, -4.3701e-02, -5.1025e-02,  7.9590e-02,\n",
       "           1.7700e-02,  3.4180e-02, -1.2402e-01,  6.6223e-03,  2.8442e-02,\n",
       "          -1.6016e-01,  4.9805e-02, -1.1133e-01, -1.2207e-01,  4.8584e-02,\n",
       "           1.4160e-02,  1.3281e-01,  1.4709e-02, -1.6113e-01, -5.0537e-02,\n",
       "          -1.3379e-01, -4.1260e-02,  8.1787e-03,  8.3008e-02, -9.1309e-02,\n",
       "          -6.5430e-02, -1.1719e-02, -1.2891e-01, -2.0752e-02, -5.1758e-02,\n",
       "          -1.0547e-01,  2.7313e-03, -1.3281e-01,  1.6846e-02,  6.1279e-02,\n",
       "          -4.1809e-03, -5.0049e-02,  6.3477e-02,  4.5410e-02, -1.0071e-03,\n",
       "          -2.8534e-03, -1.1230e-02,  2.0020e-02, -7.3853e-03,  3.6774e-03,\n",
       "           2.8687e-02, -2.6562e-01,  7.7637e-02, -1.2402e-01,  1.2158e-01,\n",
       "           1.1914e-01,  2.2827e-02,  1.1084e-01, -1.4221e-02,  1.3245e-02,\n",
       "           6.1279e-02,  3.6377e-02,  8.3984e-02,  8.3008e-02,  6.4453e-02,\n",
       "           5.1270e-02,  1.0681e-02, -7.9102e-02, -3.1738e-02, -4.5776e-03,\n",
       "           5.0049e-02, -3.1494e-02,  6.6406e-02,  9.0820e-02, -1.4526e-02,\n",
       "           8.2031e-02, -5.1025e-02,  9.2773e-02, -5.7861e-02,  2.0630e-02,\n",
       "           3.0151e-02,  1.0840e-01, -1.5332e-01, -1.7090e-02,  6.6895e-02,\n",
       "           1.4062e-01, -6.2256e-03, -4.8584e-02,  1.6309e-01, -7.5195e-02,\n",
       "          -3.8574e-02, -1.0547e-01,  4.1992e-02, -1.7578e-02, -1.2146e-02,\n",
       "          -5.0659e-03, -4.1199e-03,  1.7969e-01, -8.2520e-02, -1.5137e-02,\n",
       "          -1.9165e-02, -6.3477e-02,  1.8457e-01, -9.7168e-02, -8.2031e-02,\n",
       "          -4.4434e-02, -5.0781e-02, -2.2430e-03,  8.6426e-02,  7.1289e-02,\n",
       "          -4.6387e-02, -3.9795e-02,  9.3262e-02, -3.6377e-02,  8.4473e-02,\n",
       "          -4.5654e-02, -2.0874e-02, -7.9102e-02, -1.6211e-01,  2.0630e-02,\n",
       "          -3.4332e-03, -7.1777e-02,  1.0010e-02,  6.4941e-02, -4.4434e-02,\n",
       "          -1.5182e-03,  1.5039e-01, -8.0078e-02,  3.5156e-02, -7.4219e-02,\n",
       "           1.2146e-02,  2.2583e-02, -1.5015e-02, -1.0742e-01, -5.5908e-02,\n",
       "           1.4355e-01, -4.3457e-02, -3.1738e-02, -1.0254e-01,  1.8945e-01,\n",
       "          -2.9688e-01, -1.6504e-01, -2.8320e-01,  1.6895e-01,  2.2266e-01,\n",
       "           2.8516e-01,  1.7676e-01, -5.4199e-02, -2.0605e-01, -1.0449e-01,\n",
       "           1.7969e-01,  3.5938e-01,  6.7578e-01,  2.6758e-01,  1.2695e-01,\n",
       "           2.0605e-01,  2.7539e-01, -1.8457e-01, -2.2656e-01,  2.4609e-01,\n",
       "           1.6992e-01,  2.0898e-01,  1.6211e-01, -8.5938e-02, -9.1797e-02,\n",
       "           5.8350e-02,  1.0010e-01, -1.5430e-01, -1.0352e-01,  1.2305e-01,\n",
       "          -8.2031e-02,  1.9629e-01, -2.4512e-01,  1.1768e-01,  1.3770e-01,\n",
       "           3.1250e-01,  1.9824e-01, -1.2695e-01,  1.2256e-01, -1.5723e-01,\n",
       "          -3.0078e-01,  1.8652e-01,  3.6133e-01,  1.2793e-01,  7.9590e-02,\n",
       "           2.6562e-01,  3.2227e-01,  1.1523e-01, -3.3594e-01, -2.6172e-01,\n",
       "           1.4453e-01,  1.7090e-01,  6.8848e-02, -1.6992e-01, -1.5332e-01,\n",
       "           1.8457e-01,  2.8442e-02, -2.4805e-01,  1.0303e-01,  2.0605e-01,\n",
       "           2.2656e-01,  3.4375e-01,  9.6191e-02, -1.5198e-02, -1.6968e-02,\n",
       "           1.3367e-02, -3.9844e-01, -9.5825e-03, -1.9043e-02,  2.7344e-01,\n",
       "          -7.5195e-02,  1.3367e-02, -1.0254e-01, -2.1289e-01,  2.2754e-01,\n",
       "          -6.2012e-02,  1.4551e-01,  2.7734e-01,  2.3926e-01, -3.1445e-01,\n",
       "           1.6602e-01,  1.6309e-01, -9.9121e-02, -2.0801e-01, -1.4355e-01,\n",
       "           2.9688e-01, -1.0156e-01, -2.7344e-01,  1.9922e-01, -3.4637e-03,\n",
       "           2.1387e-01, -7.3828e-01, -6.3965e-02,  1.2598e-01,  1.0400e-01,\n",
       "           6.7188e+00,  1.2500e-01, -2.0874e-02, -1.1768e-01,  2.2852e-01,\n",
       "           2.3047e-01, -1.5625e-01,  9.2285e-02,  8.5938e-02,  2.5977e-01,\n",
       "          -1.8652e-01, -2.4512e-01, -1.2817e-02, -1.6895e-01,  1.3281e-01,\n",
       "           5.9082e-02,  7.6660e-02,  2.5781e-01,  2.9883e-01, -3.9795e-02,\n",
       "           2.3242e-01,  3.3789e-01,  1.8750e-01, -1.3086e-01, -3.3203e-01,\n",
       "          -4.1406e-01,  1.4941e-01, -2.3633e-01, -2.3047e-01,  5.9082e-02,\n",
       "          -1.2158e-01,  9.2773e-02], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0113, -0.0003,  0.0002,  ...,  0.0015,  0.0242,  0.0030],\n",
       "          [ 0.0135, -0.0188,  0.0110,  ...,  0.0206,  0.0312, -0.0009],\n",
       "          [ 0.0194,  0.0120, -0.0199,  ...,  0.0044, -0.0129,  0.0023],\n",
       "          ...,\n",
       "          [-0.0320,  0.0156, -0.0177,  ...,  0.0145, -0.0018,  0.0223],\n",
       "          [ 0.0032,  0.0087, -0.0013,  ..., -0.0034, -0.0059, -0.0085],\n",
       "          [ 0.0208, -0.0165,  0.0135,  ..., -0.0147,  0.0079, -0.0047]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-1.2756e-02,  7.0190e-03,  8.1787e-03,  ...,  1.9287e-02,\n",
       "           -2.0020e-02,  6.6528e-03],\n",
       "          [-8.9722e-03, -1.8997e-03,  1.2695e-02,  ..., -1.4343e-02,\n",
       "            2.6550e-03, -5.4321e-03],\n",
       "          [ 2.5146e-02,  2.0264e-02,  2.0386e-02,  ..., -1.2817e-02,\n",
       "            1.2329e-02, -3.1586e-03],\n",
       "          ...,\n",
       "          [-5.6152e-03, -3.2196e-03,  1.8677e-02,  ..., -8.6670e-03,\n",
       "            2.3315e-02, -1.1658e-02],\n",
       "          [ 1.9165e-02, -1.8311e-02,  4.8637e-05,  ...,  2.3682e-02,\n",
       "           -9.2316e-04, -1.1902e-02],\n",
       "          [ 3.6469e-03, -2.5269e-02,  3.3722e-03,  ...,  3.6133e-02,\n",
       "           -2.7344e-02, -1.0803e-02]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0146, -0.0120, -0.0087,  ...,  0.0151, -0.0049, -0.0001],\n",
       "          [ 0.0070,  0.0028,  0.0183,  ..., -0.0073, -0.0219, -0.0258],\n",
       "          [-0.0029,  0.0125, -0.0051,  ...,  0.0280, -0.0085, -0.0046],\n",
       "          ...,\n",
       "          [ 0.0200, -0.0427,  0.0003,  ..., -0.0330,  0.0064, -0.0063],\n",
       "          [-0.0393, -0.0248, -0.0231,  ..., -0.0166, -0.0023, -0.0031],\n",
       "          [-0.0101,  0.0041, -0.0081,  ...,  0.0265, -0.0239, -0.0056]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0294,  0.0047, -0.0050,  ..., -0.0122, -0.0186, -0.0075],\n",
       "          [ 0.0371,  0.0162,  0.0109,  ...,  0.0047,  0.0337,  0.0201],\n",
       "          [-0.0270,  0.0027,  0.0065,  ...,  0.0176, -0.0210, -0.0014],\n",
       "          ...,\n",
       "          [ 0.0247,  0.0052,  0.0223,  ...,  0.0014, -0.0254, -0.0051],\n",
       "          [-0.0146, -0.0211,  0.0144,  ...,  0.0254, -0.0381, -0.0114],\n",
       "          [ 0.0194,  0.0208, -0.0149,  ..., -0.0031, -0.0068, -0.0131]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([1.0078, 0.7422, 0.9688,  ..., 0.7891, 0.8516, 0.8516],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([1.5625, 1.5391, 1.6953,  ..., 1.3047, 1.6016, 1.6016],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0130,  0.0115,  0.0070,  ..., -0.0024, -0.0003,  0.0049],\n",
       "          [ 0.0148, -0.0135, -0.0228,  ..., -0.0013,  0.0188,  0.0264],\n",
       "          [ 0.0156, -0.0093, -0.0315,  ...,  0.0045,  0.0069,  0.0036],\n",
       "          ...,\n",
       "          [ 0.0010, -0.0261,  0.0027,  ...,  0.0015,  0.0076, -0.0201],\n",
       "          [-0.0240, -0.0046, -0.0315,  ...,  0.0229, -0.0306, -0.0186],\n",
       "          [ 0.0019,  0.0062,  0.0422,  ..., -0.0148, -0.0396,  0.0020]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([-2.2188,  0.6523,  0.6719,  ...,  1.2656,  1.9609,  0.1406],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0094,  0.0092,  0.0055,  ..., -0.0023,  0.0120, -0.0039],\n",
       "          [ 0.0107,  0.0292, -0.0042,  ...,  0.0078, -0.0137, -0.0225],\n",
       "          [-0.0046,  0.0036,  0.0023,  ...,  0.0129,  0.0160, -0.0121],\n",
       "          ...,\n",
       "          [-0.0034, -0.0012, -0.0018,  ...,  0.0084,  0.0095,  0.0076],\n",
       "          [ 0.0030,  0.0187, -0.0216,  ..., -0.0040, -0.0053, -0.0050],\n",
       "          [ 0.0101,  0.0216,  0.0143,  ..., -0.0067, -0.0101,  0.0017]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 2.0625e+00, -6.0120e-03, -7.7344e-01,  7.6172e-02, -1.2500e+00,\n",
       "           1.4551e-01,  2.9688e-01, -8.1055e-02,  1.7383e-01,  4.2578e-01,\n",
       "           9.3384e-03, -8.0566e-02, -2.6978e-02, -6.5918e-02, -8.6914e-02,\n",
       "           2.0410e-01,  3.0469e-01,  1.9434e-01,  1.9434e-01, -4.2969e-02,\n",
       "           6.4941e-02,  1.3281e-01,  2.8125e-01, -7.2266e-02,  1.5938e+00,\n",
       "          -9.4727e-02,  7.1716e-03,  6.2256e-02, -1.9531e-01,  5.5078e-01,\n",
       "           2.3535e-01,  6.6797e-01, -8.7891e-02, -2.0117e-01, -3.5742e-01,\n",
       "           4.0625e-01, -1.9629e-01,  1.3281e-01,  4.2188e-01, -4.9219e-01,\n",
       "           2.5195e-01, -5.3223e-02,  1.8066e-01, -4.1016e-01, -1.8359e-01,\n",
       "           1.8555e-01,  1.9165e-02, -1.0156e-01, -1.2793e-01,  2.2070e-01,\n",
       "          -1.3379e-01, -3.8086e-01,  4.4375e+00,  2.1191e-01,  4.5703e-01,\n",
       "           1.1182e-01,  1.4844e-01,  1.2256e-01, -1.2891e-01,  1.8750e-01,\n",
       "           3.3984e-01,  1.9609e+00,  5.6641e-01, -6.0938e-01, -6.7578e-01,\n",
       "           4.5166e-02,  7.5781e-01, -1.8945e-01, -2.7344e-01, -3.5352e-01,\n",
       "           1.2598e-01, -2.4609e-01, -1.2656e+00,  1.6602e-01, -7.0190e-03,\n",
       "           1.4844e+00,  1.5991e-02,  2.3828e-01, -1.3359e+00,  2.2363e-01,\n",
       "           3.1445e-01, -2.9883e-01, -1.5723e-01, -2.7734e-01,  1.5078e+00,\n",
       "          -1.3770e-01, -3.9258e-01,  1.4258e-01, -3.2422e-01,  2.3926e-01,\n",
       "           2.7344e-01, -1.2598e-01,  1.9141e-01, -3.0060e-03, -2.9297e-01,\n",
       "           3.8672e-01,  1.4453e-01,  2.5586e-01, -3.7891e-01, -1.4551e-01,\n",
       "          -1.8945e-01,  3.4912e-02,  9.2285e-02, -8.7891e-02,  2.9297e-01,\n",
       "          -1.4062e-01, -4.2578e-01, -1.3574e-01,  8.1250e-01,  1.5430e-01,\n",
       "          -1.1719e-01,  8.9355e-02,  1.3086e-01,  3.8672e-01, -1.8359e-01,\n",
       "           3.2471e-02, -1.8594e+00,  1.3770e-01, -5.1172e-01, -7.2754e-02,\n",
       "          -2.3281e+00, -1.3281e-01, -2.4023e-01, -7.3047e-01,  7.8906e-01,\n",
       "           2.9102e-01, -6.1328e-01, -1.0000e+00,  2.8564e-02,  2.9175e-02,\n",
       "           4.9316e-02,  1.4160e-02,  2.7539e-01, -7.3730e-02,  2.3926e-01,\n",
       "           5.9326e-02, -7.8613e-02, -1.1279e-01, -3.5938e-01,  8.2031e-02,\n",
       "          -2.4658e-02,  3.1641e-01, -5.2344e-01, -4.5703e-01,  7.8125e-02,\n",
       "          -3.0469e-01, -6.4844e-01, -8.9844e-02,  2.1875e-01, -2.7148e-01,\n",
       "           1.5039e-01,  1.4941e-01, -3.0664e-01, -3.5156e-02,  1.8311e-02,\n",
       "          -7.6562e-01,  1.4746e-01, -1.1768e-01, -6.5625e-01, -3.3789e-01,\n",
       "          -3.2617e-01, -5.5078e-01,  3.5352e-01,  8.2031e-02,  8.5156e-01,\n",
       "           2.4414e-01,  2.4219e-01,  2.0898e-01,  4.4531e-01,  5.6250e-01,\n",
       "          -2.9907e-02, -7.1777e-02, -4.8438e-01, -1.3086e-01, -3.0273e-02,\n",
       "          -4.8584e-02,  2.8320e-01, -2.6367e-01,  3.0273e-01,  2.9297e-01,\n",
       "           6.1340e-03, -2.8711e-01, -2.1875e-01, -5.3516e-01,  2.8125e-01,\n",
       "           2.9297e-01,  2.4316e-01,  2.8516e-01, -2.0215e-01,  2.1094e+00,\n",
       "           5.8594e-01, -1.4746e-01,  8.9844e-02,  1.9727e-01,  1.9629e-01,\n",
       "           2.5000e-01,  1.1377e-01, -2.9102e-01,  3.6865e-02, -4.2969e-02,\n",
       "           2.6953e-01,  4.5898e-01, -1.8262e-01,  1.8945e-01,  4.3359e-01,\n",
       "          -2.0801e-01,  2.5757e-02, -2.8711e-01,  5.1562e-01, -1.0449e-01,\n",
       "           3.0029e-02,  4.0430e-01,  3.0273e-01, -6.7969e-01,  4.7607e-02,\n",
       "          -1.6699e-01, -9.1406e-01,  1.2988e-01, -1.9141e-01, -3.8867e-01,\n",
       "           1.6846e-02, -1.6211e-01, -9.8438e-01, -1.8750e-01,  5.7373e-02,\n",
       "          -4.2578e-01,  8.8867e-02, -3.6621e-02, -9.2188e-01,  6.2561e-03,\n",
       "          -4.2773e-01, -2.8076e-02,  5.6885e-02, -1.5234e+00, -4.2236e-02,\n",
       "           3.1250e-01,  3.0859e-01,  4.1406e-01, -5.0781e-01, -6.2891e-01,\n",
       "          -2.1562e+00,  1.9141e-01, -1.6113e-01, -1.5332e-01, -1.0449e-01,\n",
       "           5.4688e-01, -4.5312e-01, -2.7148e-01,  1.0986e-02,  4.6094e-01,\n",
       "          -1.3906e+00,  1.6113e-01,  1.5430e-01,  3.3906e+00,  2.6562e-01,\n",
       "          -1.6875e+00,  2.4902e-01, -4.4531e-01,  4.3945e-01, -5.0391e-01,\n",
       "          -5.3906e-01, -3.5352e-01,  1.0156e+00,  7.2266e-01,  5.3467e-02,\n",
       "           3.6914e-01,  1.7969e-01, -1.2061e-01, -8.7891e-01, -8.3496e-02,\n",
       "          -7.6172e-02, -2.0996e-01,  3.4570e-01,  6.9824e-02,  7.6172e-02,\n",
       "          -4.0625e-01,  1.3965e-01,  2.9102e-01, -2.5391e-01,  5.8984e-01,\n",
       "          -1.7285e-01,  4.8633e-01, -8.8672e-01,  1.3184e-01,  3.0469e-01,\n",
       "           1.0547e+00, -2.3145e-01, -1.6602e-01,  3.1055e-01, -2.0874e-02,\n",
       "          -1.9824e-01,  2.2461e-01,  6.1279e-02, -1.6250e+00,  1.6797e-01,\n",
       "           1.0693e-01,  1.0742e-01,  5.4932e-02,  1.3867e-01,  3.0469e-01,\n",
       "          -2.5195e-01, -7.6172e-01, -1.0010e-02,  3.0469e-01,  4.9414e-01,\n",
       "           4.0820e-01,  5.1953e-01,  5.7812e-01, -5.2734e-01, -8.5547e-01,\n",
       "           3.2500e+00,  8.2520e-02,  3.2422e-01, -2.6758e-01, -2.3750e+00,\n",
       "          -1.1963e-01, -4.7461e-01,  6.3281e-01, -3.5547e-01, -5.3516e-01,\n",
       "          -1.2031e+00,  5.4688e-01, -1.0000e+00, -3.3984e-01, -2.3242e-01,\n",
       "          -5.5469e-01,  2.4316e-01,  3.9453e-01, -1.5234e-01, -8.5547e-01,\n",
       "           1.0010e-01,  6.1328e-01,  3.7500e-01, -1.0625e+00,  4.7656e-01,\n",
       "           4.3457e-02,  4.5508e-01,  1.6309e-01,  9.0625e-01, -1.2305e-01,\n",
       "           1.8066e-01, -2.1289e-01, -1.1094e+00, -2.0801e-01,  4.6143e-02,\n",
       "           6.1523e-02, -1.9531e-02, -5.8984e-01, -3.3789e-01, -3.5938e-01,\n",
       "           2.2949e-01, -1.0547e-01, -2.0117e-01, -5.5469e-01,  4.7070e-01,\n",
       "           1.5039e-01,  1.4221e-02,  4.4141e-01, -1.4343e-02,  2.8320e-01,\n",
       "          -2.5977e-01, -3.3398e-01, -3.1250e-01, -2.5586e-01, -7.8613e-02,\n",
       "           2.8711e-01,  2.6172e-01, -3.0859e-01,  4.5508e-01,  4.3750e-01,\n",
       "          -5.1562e-01,  7.3828e-01, -2.1875e-01,  5.9375e-01,  4.5625e+00,\n",
       "           8.6328e-01, -2.8125e-01,  9.9609e-01,  2.3438e-01, -2.7344e-01,\n",
       "           5.5469e-01, -1.2109e+00, -2.1719e+00, -1.6250e+00,  2.3535e-01,\n",
       "           3.3594e-01,  4.5703e-01,  2.1875e-01, -4.9414e-01, -8.8379e-02,\n",
       "          -3.9062e-01, -4.2725e-02, -2.5586e-01, -6.3965e-02,  2.7710e-02,\n",
       "          -7.6172e-01, -8.9355e-02, -5.1172e-01,  4.8828e-02,  3.2031e-01,\n",
       "           2.9175e-02,  9.2285e-02,  1.3574e-01,  3.1250e-01, -3.6377e-02,\n",
       "           2.7539e-01,  4.0234e-01,  1.7090e-02, -2.8711e-01,  6.0938e-01,\n",
       "           4.5410e-02,  9.3750e-02, -2.8320e-01, -2.9102e-01, -9.3750e-02,\n",
       "           1.7188e+00,  4.4141e-01, -6.0938e-01, -5.4297e-01,  2.7344e-01,\n",
       "           1.5820e-01, -1.5000e+00,  3.4961e-01,  6.6895e-02, -1.8750e+00,\n",
       "           1.6504e-01, -4.6094e-01,  2.0410e-01, -7.9102e-02,  8.5938e-02,\n",
       "           2.8125e+00, -2.8906e-01,  4.5898e-01,  1.2012e-01,  2.9297e-01,\n",
       "          -4.8047e-01, -1.1816e-01, -8.0469e-01, -2.3242e-01,  2.9883e-01,\n",
       "           4.4922e-01, -3.8281e-01, -5.5469e-01,  3.7842e-02, -6.3672e-01,\n",
       "           2.3750e+00, -7.4219e-01, -1.4551e-01,  4.9219e-01, -1.8262e-01,\n",
       "          -2.3730e-01,  2.8516e-01,  1.7676e-01,  9.8145e-02,  3.4766e-01,\n",
       "           1.3867e-01, -6.6016e-01,  1.1963e-01,  2.9297e-01,  1.5430e-01,\n",
       "           1.2354e-01,  1.4551e-01,  3.3203e-01,  5.3125e-01, -8.7500e-01,\n",
       "           4.1992e-02, -1.1914e-01,  9.9609e-01, -1.5430e-01,  4.0234e-01,\n",
       "           1.0547e+00, -2.0215e-01, -5.7031e-01,  1.1797e+00,  1.3184e-01,\n",
       "          -3.7109e-01, -6.0547e-01, -1.0312e+00,  5.2246e-02, -2.6562e-01,\n",
       "           9.5703e-01, -8.0859e-01,  1.6406e-01, -5.6396e-02, -2.8809e-02,\n",
       "           1.0254e-01, -1.3867e-01, -1.3379e-01,  1.1406e+00,  7.9102e-02,\n",
       "          -1.4453e-01, -1.0925e-02, -1.6846e-02, -1.9238e-01, -3.6328e-01,\n",
       "           8.7402e-02,  3.4180e-01, -5.2002e-02,  5.8203e-01, -1.4648e-02,\n",
       "          -5.5859e-01, -4.6680e-01,  6.3281e-01,  5.1562e-01,  3.5156e-01,\n",
       "           7.2754e-02, -2.1582e-01, -5.2344e-01, -1.0625e+00,  3.8906e+00,\n",
       "          -1.6641e+00, -8.0078e-02], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0088,  0.0283, -0.0015,  ..., -0.0371,  0.0398, -0.0439],\n",
       "          [ 0.0376,  0.0159, -0.0214,  ...,  0.0104, -0.0248,  0.0391],\n",
       "          [-0.0153, -0.0181, -0.0104,  ..., -0.0051,  0.0098, -0.0203],\n",
       "          ...,\n",
       "          [ 0.0192, -0.0452, -0.0147,  ...,  0.0156, -0.0118,  0.0253],\n",
       "          [ 0.0110,  0.0011, -0.0096,  ...,  0.0070, -0.0120,  0.0151],\n",
       "          [-0.0075,  0.0256, -0.0437,  ..., -0.0262,  0.0184, -0.0145]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 2.8125e-01, -2.5635e-02,  1.6797e-01, -5.8984e-01, -1.1084e-01,\n",
       "           1.4648e-01, -1.3477e-01,  2.1582e-01, -2.2754e-01,  4.9805e-02,\n",
       "          -1.7383e-01, -1.3477e-01, -1.8750e-01,  3.0640e-02,  2.0312e-01,\n",
       "           4.8340e-02, -7.8613e-02,  1.0254e-01,  3.3203e-01,  1.6602e-01,\n",
       "           3.7695e-01,  1.6016e-01,  4.1016e-01,  1.9727e-01,  5.2734e-01,\n",
       "          -1.0645e-01,  2.9297e-01, -5.8105e-02,  6.9336e-02, -6.0303e-02,\n",
       "           3.6621e-02,  1.2500e-01,  9.0820e-02,  1.5869e-02, -1.9897e-02,\n",
       "           6.8750e-01,  1.2695e-01,  9.6191e-02, -5.5908e-02, -3.3398e-01,\n",
       "          -5.2490e-02, -1.2817e-02, -1.3477e-01, -1.1914e-01,  1.2158e-01,\n",
       "           2.6367e-02, -4.0625e-01,  6.2500e-02,  2.3145e-01,  2.6953e-01,\n",
       "          -9.7656e-02, -4.1406e-01,  2.4219e-01, -2.5000e-01,  4.1406e-01,\n",
       "          -1.2207e-01, -3.2031e-01,  1.9219e+00,  1.8457e-01,  3.9258e-01,\n",
       "           4.0234e-01, -1.9238e-01, -3.1641e-01,  3.0469e-01,  3.3594e-01,\n",
       "          -9.7656e-02,  1.0156e-01, -9.0942e-03, -7.6172e-02,  1.1914e-01,\n",
       "           2.7954e-02,  3.0469e-01, -3.9844e-01, -1.7188e-01,  6.9824e-02,\n",
       "           2.0312e-01,  3.3203e-02, -5.1953e-01, -1.2988e-01, -3.1641e-01,\n",
       "          -5.9375e-01,  2.2363e-01, -2.6367e-01,  4.6094e-01,  2.2095e-02,\n",
       "          -2.9883e-01,  3.6719e-01,  2.2363e-01, -2.2168e-01, -2.7539e-01,\n",
       "          -1.6211e-01,  1.0547e-01, -1.4648e-01, -8.6719e-01,  2.3145e-01,\n",
       "           2.2656e-01, -3.2227e-01, -1.3184e-01, -3.5400e-02,  2.6367e-01,\n",
       "          -7.5684e-02,  7.6562e-01,  1.5430e-01, -2.1191e-01, -3.3203e-02,\n",
       "           2.2266e-01,  1.7383e-01,  2.3828e-01, -1.5234e-01, -1.4648e-01,\n",
       "          -2.2852e-01, -7.9346e-03, -5.2734e-01,  8.4961e-02,  4.3213e-02,\n",
       "          -3.7109e-01, -4.0588e-03, -1.9629e-01, -1.8945e-01, -3.1641e-01,\n",
       "           3.1641e-01,  3.4375e-01,  1.5723e-01,  3.0396e-02, -4.3359e-01,\n",
       "           3.2227e-01,  4.1602e-01, -6.3965e-02,  1.3379e-01,  1.4551e-01,\n",
       "           9.2285e-02,  7.0312e-02,  7.9956e-03,  1.1230e-01, -4.7363e-02,\n",
       "          -2.0215e-01,  1.4062e-01,  7.0801e-02,  2.1777e-01, -4.0039e-02,\n",
       "           2.5781e-01, -1.4648e-01,  1.6309e-01, -2.5757e-02,  2.1582e-01,\n",
       "           3.0762e-02, -1.0437e-02, -9.0332e-02, -4.0039e-02, -2.6562e-01,\n",
       "           4.7363e-02,  5.7129e-02, -3.5858e-04,  1.7090e-02, -7.9102e-02,\n",
       "           8.3496e-02, -6.8359e-03,  1.6992e-01, -8.0078e-02,  4.7607e-02,\n",
       "          -1.4160e-01,  1.7334e-02,  1.0742e-01, -3.0151e-02, -8.7891e-02,\n",
       "           2.0142e-02,  8.9355e-02, -1.3086e-01,  6.3477e-02,  2.5000e-01,\n",
       "          -2.6489e-02,  9.1309e-02,  5.8105e-02, -4.0771e-02, -4.1260e-02,\n",
       "          -2.1484e-01, -7.2754e-02, -6.8848e-02,  2.2827e-02,  1.4453e-01,\n",
       "          -1.9238e-01,  1.0693e-01,  6.4941e-02,  1.2354e-01, -1.2988e-01,\n",
       "           6.2500e-02,  1.0498e-01, -2.0020e-02,  4.2236e-02,  4.2480e-02,\n",
       "          -1.3867e-01,  1.2500e-01, -6.6895e-02, -1.1523e-01, -4.3457e-02,\n",
       "          -5.2734e-02, -1.5430e-01,  5.3955e-02,  1.2988e-01,  3.2715e-02,\n",
       "          -1.0498e-01,  7.2754e-02, -4.5898e-02,  5.2490e-02,  1.2598e-01,\n",
       "          -2.5269e-02, -6.3782e-03,  7.4707e-02, -5.0049e-03,  2.2705e-02,\n",
       "           3.0762e-02, -1.7285e-01,  3.5156e-02,  2.7344e-02, -8.9722e-03,\n",
       "          -3.9307e-02,  8.2520e-02, -1.1768e-01, -1.1670e-01,  6.8848e-02,\n",
       "           9.4238e-02,  1.2793e-01, -2.3926e-01, -1.2793e-01,  1.5918e-01,\n",
       "           6.3477e-03,  6.8359e-03, -1.8652e-01,  3.3875e-03, -3.1006e-02,\n",
       "          -5.1270e-02, -2.7344e-02, -3.7842e-02, -6.1279e-02,  1.5625e-02,\n",
       "          -1.6113e-01, -1.0986e-01, -1.5234e-01,  1.4453e-01,  1.3916e-02,\n",
       "          -5.8125e+00,  2.2070e-01,  5.7129e-02, -5.0049e-02,  1.9629e-01,\n",
       "           1.2158e-01,  8.9355e-02,  4.8096e-02, -7.3730e-02, -4.1992e-02,\n",
       "          -5.0049e-02, -2.1118e-02, -7.9346e-04, -2.8809e-02, -8.2031e-02,\n",
       "           2.2559e-01, -1.4801e-03,  1.6724e-02, -6.0425e-03, -1.2988e-01,\n",
       "          -7.2266e-02, -2.5635e-02,  7.5989e-03, -4.8340e-02, -1.3867e-01,\n",
       "           6.8359e-02, -1.5335e-03,  2.8320e-02, -7.4707e-02,  4.8584e-02,\n",
       "           2.7588e-02, -9.8145e-02, -1.2207e-01,  2.4292e-02,  4.3640e-03,\n",
       "           4.2114e-03, -4.9072e-02, -9.1553e-03,  3.7689e-03, -7.8613e-02,\n",
       "           9.8633e-02,  1.5747e-02,  4.4922e-02,  5.7617e-02,  9.7656e-03,\n",
       "           9.7168e-02,  3.5156e-02,  2.0386e-02, -2.0508e-02, -7.3730e-02,\n",
       "           2.2339e-02, -3.0640e-02, -5.4688e-02,  1.0315e-02,  5.2002e-02,\n",
       "          -9.3384e-03, -5.8350e-02,  1.1865e-01,  7.2266e-02,  2.4536e-02,\n",
       "           7.7637e-02, -6.1035e-02, -1.2390e-02, -6.6895e-02, -3.3447e-02,\n",
       "          -3.9062e-02,  1.7944e-02,  7.4707e-02,  3.9307e-02, -4.2725e-02,\n",
       "           8.8867e-02, -4.8340e-02, -3.6621e-02,  1.9531e-02,  6.6895e-02,\n",
       "          -9.5703e-02,  1.1572e-01, -7.1777e-02, -1.0559e-02, -3.1250e-01,\n",
       "           1.8311e-02, -6.1035e-02,  2.1240e-02, -9.2285e-02,  4.6387e-02,\n",
       "          -3.9307e-02,  5.2734e-01,  4.3945e-02, -6.8848e-02, -2.7466e-02,\n",
       "           9.3262e-02,  5.3955e-02, -1.4258e-01, -4.9072e-02,  1.6357e-02,\n",
       "           7.0801e-02, -5.6641e-02, -1.2891e-01,  4.3945e-02,  1.3184e-01,\n",
       "          -2.8564e-02,  4.0283e-03,  8.8379e-02,  8.8867e-02, -4.5471e-03,\n",
       "           1.2878e-02, -6.2988e-02, -3.9368e-03,  5.8899e-03, -5.6641e-02,\n",
       "           1.3770e-01,  1.1250e+00,  4.1504e-03,  3.7354e-02,  6.6895e-02,\n",
       "          -5.8594e-03,  2.3804e-02,  7.1777e-02,  1.7944e-02, -6.5918e-02,\n",
       "          -1.2061e-01,  6.6406e-02, -6.9336e-02, -6.2866e-03, -8.1055e-02,\n",
       "           8.4305e-04,  4.2480e-02, -2.0264e-02,  3.6865e-02, -1.0156e-01,\n",
       "          -4.2725e-02,  4.0527e-02, -1.7676e-01, -5.0293e-02,  4.4189e-02,\n",
       "          -1.8066e-02,  1.9531e-02,  7.8735e-03, -3.4668e-02, -7.0312e-02,\n",
       "          -1.2109e-01,  7.5684e-02, -2.5757e-02,  3.8818e-02,  1.4648e-01,\n",
       "           1.1719e-01,  9.2285e-02,  2.0020e-02,  5.1025e-02,  8.9355e-02,\n",
       "           5.4199e-02,  9.3262e-02,  9.8145e-02, -9.2285e-02, -1.2354e-01,\n",
       "           4.1016e-02, -4.8340e-02,  2.4048e-02,  5.2490e-02, -8.6426e-02,\n",
       "          -2.2949e-02,  5.1270e-02, -1.3574e-01,  8.0566e-02,  2.8320e-02,\n",
       "          -1.0205e-01,  3.5156e-02,  1.0010e-01, -1.6602e-01, -1.9727e-01,\n",
       "          -5.5664e-02, -1.2598e-01, -1.0645e-01, -1.9775e-02, -2.9907e-02,\n",
       "           3.4180e-02, -1.6895e-01, -1.2500e-01,  6.8848e-02,  3.0029e-02,\n",
       "           1.4893e-02,  1.4746e-01,  4.1748e-02,  2.0215e-01, -2.8839e-03,\n",
       "           2.3746e-04,  1.6357e-02,  1.7871e-01,  1.1670e-01,  1.0986e-01,\n",
       "           9.5703e-02, -5.2490e-02, -1.1182e-01, -7.8125e-02,  5.8350e-02,\n",
       "          -2.3926e-02,  6.3965e-02, -3.5400e-02, -6.1035e-03, -6.6895e-02,\n",
       "          -4.7363e-02, -4.3945e-02, -3.4912e-02,  6.7383e-02,  1.6479e-02,\n",
       "           6.2256e-02, -4.6631e-02,  5.8105e-02,  2.5513e-02, -1.2109e-01,\n",
       "          -1.1133e-01,  5.1514e-02,  2.4292e-02,  1.2256e-01, -5.1025e-02,\n",
       "           2.7466e-03, -1.9531e-02, -4.1748e-02, -1.9336e-01,  5.3955e-02,\n",
       "          -3.4668e-02, -5.1758e-02,  1.4160e-01,  1.1377e-01, -1.3000e-02,\n",
       "          -1.0645e-01, -1.5991e-02,  6.1035e-02,  3.4424e-02,  7.8125e-02,\n",
       "           1.0645e-01, -5.8105e-02,  5.8594e-02, -3.8818e-02,  9.2773e-02,\n",
       "           8.7891e-02,  3.3691e-02, -1.0559e-02,  1.0889e-01,  1.5332e-01,\n",
       "           5.8838e-02, -9.2773e-02, -1.4453e-01, -1.1621e-01,  9.7656e-03,\n",
       "          -5.2002e-02, -1.0205e-01,  6.4697e-03,  8.6426e-02,  2.2217e-02,\n",
       "          -5.7373e-02, -1.6113e-01, -9.6680e-02, -1.8555e-01, -7.5195e-02,\n",
       "          -4.0588e-03,  1.3281e-01,  1.1047e-02, -1.1719e-02,  1.5332e-01,\n",
       "           5.6396e-02, -6.6895e-02, -5.4688e-02,  7.5684e-02,  2.2217e-02,\n",
       "          -1.2158e-01,  8.1543e-02, -1.3867e-01,  1.0889e-01, -4.2969e-02,\n",
       "          -7.9102e-02, -1.8945e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0233,  0.0111,  0.0020,  ..., -0.0136,  0.0255, -0.0056],\n",
       "          [ 0.0053, -0.0205, -0.0260,  ...,  0.0018,  0.0219, -0.0210],\n",
       "          [ 0.0287, -0.0189, -0.0046,  ...,  0.0361, -0.0085,  0.0118],\n",
       "          ...,\n",
       "          [ 0.0087,  0.0129,  0.0320,  ...,  0.0076,  0.0225,  0.0194],\n",
       "          [-0.0038,  0.0039, -0.0092,  ...,  0.0148,  0.0038, -0.0052],\n",
       "          [ 0.0065, -0.0064,  0.0012,  ...,  0.0095, -0.0031, -0.0282]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0104,  0.0109,  0.0231,  ...,  0.0229,  0.0422,  0.0167],\n",
       "          [-0.0289,  0.0004,  0.0281,  ..., -0.0044, -0.0079,  0.0093],\n",
       "          [ 0.0026, -0.0081, -0.0210,  ..., -0.0061, -0.0084, -0.0016],\n",
       "          ...,\n",
       "          [ 0.0087,  0.0008, -0.0048,  ...,  0.0038, -0.0150,  0.0162],\n",
       "          [-0.0094,  0.0031,  0.0084,  ...,  0.0073, -0.0059,  0.0393],\n",
       "          [ 0.0165,  0.0112, -0.0182,  ...,  0.0104, -0.0002,  0.0371]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0220, -0.0309, -0.0267,  ...,  0.0121,  0.0110,  0.0271],\n",
       "          [ 0.0081,  0.0276,  0.0098,  ..., -0.0192,  0.0325,  0.0258],\n",
       "          [-0.0179,  0.0044,  0.0016,  ..., -0.0039, -0.0114, -0.0069],\n",
       "          ...,\n",
       "          [ 0.0141,  0.0142, -0.0004,  ...,  0.0120,  0.0075,  0.0018],\n",
       "          [ 0.0391, -0.0116,  0.0171,  ..., -0.0143,  0.0134, -0.0116],\n",
       "          [ 0.0076, -0.0090,  0.0007,  ...,  0.0292,  0.0033, -0.0021]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0117,  0.0121, -0.0107,  ...,  0.0079,  0.0028,  0.0146],\n",
       "          [ 0.0034, -0.0093,  0.0059,  ..., -0.0005, -0.0095,  0.0133],\n",
       "          [-0.0092, -0.0097,  0.0081,  ...,  0.0327, -0.0303,  0.0094],\n",
       "          ...,\n",
       "          [ 0.0049, -0.0267,  0.0083,  ..., -0.0170,  0.0018,  0.0150],\n",
       "          [ 0.0159,  0.0204,  0.0032,  ..., -0.0208,  0.0146,  0.0432],\n",
       "          [ 0.0211,  0.0107,  0.0190,  ...,  0.0045, -0.0269,  0.0199]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([1.0469, 0.8359, 1.0625,  ..., 0.8516, 0.9648, 0.9688],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([1.5938, 1.5938, 1.7031,  ..., 1.3906, 1.6328, 1.6328],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0064,  0.0007, -0.0141,  ...,  0.0155,  0.0085,  0.0074],\n",
       "          [-0.0015, -0.0096, -0.0131,  ...,  0.0092,  0.0065, -0.0053],\n",
       "          [ 0.0205,  0.0055, -0.0003,  ...,  0.0081, -0.0080, -0.0211],\n",
       "          ...,\n",
       "          [-0.0017,  0.0021, -0.0166,  ..., -0.0137, -0.0088,  0.0073],\n",
       "          [-0.0065, -0.0060,  0.0298,  ...,  0.0173,  0.0386, -0.0058],\n",
       "          [-0.0265,  0.0128,  0.0282,  ...,  0.0093, -0.0008, -0.0137]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 0.1758,  0.7188, -0.0552,  ...,  0.1660, -4.9688, -2.0938],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-2.0508e-02, -6.6223e-03, -1.2573e-02,  ..., -1.6022e-03,\n",
       "           -5.4932e-03, -1.7548e-03],\n",
       "          [ 1.0300e-03, -1.0315e-02,  1.1597e-02,  ..., -4.6082e-03,\n",
       "           -2.8801e-04, -1.0376e-02],\n",
       "          [-3.1433e-03,  8.1177e-03, -2.6855e-03,  ..., -1.9287e-02,\n",
       "           -1.9409e-02, -2.0386e-02],\n",
       "          ...,\n",
       "          [-1.7285e-05, -3.1586e-03,  6.0730e-03,  ...,  8.3008e-03,\n",
       "            2.6855e-03, -7.5150e-04],\n",
       "          [-1.8433e-02,  1.6708e-03,  3.9101e-04,  ..., -3.7956e-04,\n",
       "            1.0437e-02,  1.1978e-03],\n",
       "          [ 1.5442e-02, -9.5367e-05, -1.0437e-02,  ...,  8.3618e-03,\n",
       "           -1.0498e-02,  3.2654e-03]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 1.6328e+00,  4.4336e-01, -2.6953e-01, -2.1875e-01,  1.4844e-01,\n",
       "           2.6562e-01, -3.9062e-01,  1.2031e+00,  2.7148e-01, -2.4121e-01,\n",
       "           7.8613e-02,  3.6719e-01, -5.3955e-02, -5.6641e-02, -7.9102e-02,\n",
       "          -1.9238e-01, -1.4531e+00, -3.2654e-03,  1.7776e-03, -1.9531e-01,\n",
       "           1.3062e-02,  6.7383e-02,  2.0508e-01, -1.5527e-01,  1.8164e-01,\n",
       "          -2.5000e-01,  3.7354e-02,  3.6133e-01,  9.1309e-02,  1.5335e-03,\n",
       "           2.5781e-01,  6.4941e-02,  1.7578e+00, -1.2109e-01, -1.3379e-01,\n",
       "           2.3594e+00,  1.2268e-02,  3.9551e-02,  2.7954e-02, -1.4954e-02,\n",
       "          -1.3281e-01, -2.2266e-01,  4.0625e-01,  2.9688e-01,  2.2705e-02,\n",
       "           6.1279e-02,  6.5430e-02,  4.8828e-01,  9.1309e-02, -6.5430e-02,\n",
       "          -9.3262e-02, -9.0332e-02, -2.8125e-01,  5.3711e-02, -3.8672e-01,\n",
       "          -1.6211e-01, -8.3008e-02, -2.0508e-01, -1.3379e-01,  6.2500e+00,\n",
       "           3.1055e-01,  9.0332e-02,  6.6895e-02,  1.8066e-01, -2.2949e-02,\n",
       "           2.5781e-01,  1.0596e-01, -4.2773e-01, -1.2109e+00,  8.5156e-01,\n",
       "           3.6914e-01,  2.1191e-01,  1.6113e-01,  1.0840e-01,  1.2578e+00,\n",
       "          -3.8818e-02,  4.3555e-01, -1.4688e+00,  1.6016e-01,  2.6953e-01,\n",
       "           2.4219e-01, -1.5820e-01,  1.4160e-02, -1.8281e+00,  4.7607e-02,\n",
       "           1.5156e+00,  6.6406e-02,  3.3203e-01,  1.0254e-01,  2.6123e-02,\n",
       "           1.0449e-01,  2.3438e+00,  1.0254e-01, -9.6436e-03, -1.3379e-01,\n",
       "          -1.3574e-01, -3.5352e-01,  3.5645e-02, -1.1377e-01, -2.7539e-01,\n",
       "           1.7383e-01, -9.1797e-02, -1.2598e-01, -1.2988e-01, -1.3574e-01,\n",
       "          -6.9824e-02, -3.1055e-01,  1.4258e-01,  1.6724e-02,  4.4336e-01,\n",
       "          -1.2305e-01,  3.1982e-02,  5.1514e-02,  2.6611e-02,  2.6562e-01,\n",
       "          -9.5825e-03,  2.7930e-01, -3.1738e-02,  2.7148e-01,  4.1797e-01,\n",
       "          -6.1719e-01, -4.4727e-01,  4.0234e-01,  8.3125e+00,  3.0469e-01,\n",
       "          -2.8320e-01,  1.2422e+00, -8.2520e-02, -1.3965e-01,  2.3828e-01,\n",
       "           1.6699e-01, -1.2012e-01,  2.3828e-01, -2.3730e-01,  2.2461e-01,\n",
       "           9.6680e-02, -1.0498e-01, -5.7861e-02, -1.3965e-01,  1.8066e-02,\n",
       "          -2.0410e-01,  1.5625e-01, -1.5332e-01, -4.7656e-01,  9.3750e-02,\n",
       "           4.3555e-01,  2.7734e-01,  1.5820e-01, -4.8047e-01, -5.1562e-01,\n",
       "           2.1875e-01, -2.3340e-01, -4.2578e-01,  6.7871e-02, -7.8906e-01,\n",
       "          -7.8125e-01, -3.3447e-02,  4.6289e-01,  4.7607e-02,  1.4062e+00,\n",
       "           1.0205e-01,  1.1084e-01, -1.0840e-01, -2.7734e-01,  1.2500e+00,\n",
       "          -1.9531e-02,  9.7656e-01,  7.7734e-01,  2.1362e-02,  1.2024e-02,\n",
       "          -1.7969e+00, -9.2773e-03,  7.9102e-02,  2.8516e-01, -1.0234e+00,\n",
       "           4.1602e-01,  4.1406e-01,  3.0273e-01,  1.3867e-01, -4.4727e-01,\n",
       "          -1.1816e-01, -1.1406e+00,  1.3379e-01,  1.0059e-01,  3.0469e-01,\n",
       "          -1.0986e-01,  2.9663e-02,  2.4316e-01,  4.2578e-01, -1.1172e+00,\n",
       "          -8.6426e-02, -2.8711e-01,  2.0020e-01, -9.7168e-02,  1.2988e-01,\n",
       "           2.4414e-01, -6.3965e-02, -1.3477e-01, -1.8164e-01,  3.2617e-01,\n",
       "          -5.9326e-02,  3.3203e-01,  1.5625e-01,  2.3047e-01,  3.2617e-01,\n",
       "           1.9727e-01,  8.3496e-02, -2.4536e-02, -1.2891e-01,  1.1572e-01,\n",
       "           2.1875e-01, -9.1553e-03,  2.1875e-01,  2.7539e-01, -1.2598e-01,\n",
       "           5.7422e-01,  2.5195e-01,  6.8359e-02, -2.4805e-01,  3.4961e-01,\n",
       "           1.7480e-01, -3.5156e-01,  2.0117e-01,  3.7500e-01, -6.1719e-01,\n",
       "           9.9121e-02, -2.9492e-01,  1.2500e-01,  6.6016e-01,  7.0312e-02,\n",
       "          -2.1191e-01, -2.0264e-02,  1.0400e-01, -2.5977e-01,  1.5137e-01,\n",
       "           5.1270e-02, -1.8945e-01, -3.5156e-02, -1.3594e+00,  5.5542e-03,\n",
       "           2.5781e-01,  2.5635e-02,  2.8687e-02, -9.0820e-02,  5.7031e-01,\n",
       "          -2.8125e+00,  2.6172e-01, -5.8594e-02, -4.0234e-01,  4.0820e-01,\n",
       "          -4.5312e-01,  1.0059e-01, -8.5156e-01, -1.5234e+00, -2.2363e-01,\n",
       "           7.0703e-01, -1.6328e+00, -7.7148e-02,  9.4604e-03, -3.1128e-02,\n",
       "           1.4453e-01,  1.7969e-01, -1.2970e-03,  2.5781e-01,  9.8145e-02,\n",
       "          -2.1387e-01,  5.7373e-02, -1.4609e+00,  1.8555e-01, -1.1865e-01,\n",
       "          -3.3447e-02, -2.2705e-02, -2.4707e-01, -3.1982e-02,  4.2725e-03,\n",
       "          -1.9922e+00,  1.8158e-03,  9.0820e-02,  4.5654e-02, -2.0508e-01,\n",
       "           1.4258e-01, -1.6113e-02, -1.6895e-01, -1.2012e-01, -2.2461e-01,\n",
       "          -3.2031e-01,  6.3477e-02,  1.8677e-02,  9.3262e-02,  6.8359e-02,\n",
       "          -7.1777e-02, -6.9336e-02,  6.2109e-01, -4.1016e-02, -2.3730e-01,\n",
       "           1.2256e-01, -7.3242e-02, -3.3203e-02, -1.9824e-01,  1.3574e-01,\n",
       "          -2.3633e-01, -1.7944e-02, -1.1658e-02,  2.5977e-01, -1.1035e-01,\n",
       "          -3.5000e+00, -6.7871e-02,  2.1484e-01, -5.4932e-02,  2.0142e-02,\n",
       "          -1.9727e-01,  4.5117e-01, -1.2573e-02,  1.4551e-01,  1.2109e-01,\n",
       "          -1.5234e+00,  5.7422e-01, -5.3955e-02, -1.2695e-01, -8.9453e-01,\n",
       "           1.9434e-01,  2.4219e-01, -1.2578e+00, -3.7305e-01,  1.3574e-01,\n",
       "          -1.3203e+00, -6.7383e-02, -1.1641e+00, -3.8867e-01, -5.4199e-02,\n",
       "          -7.7057e-04,  7.0801e-02,  2.4414e-01, -1.3203e+00,  1.4355e-01,\n",
       "          -1.1406e+00,  1.4453e-01, -1.4648e-01, -6.4453e-02,  2.1289e-01,\n",
       "           2.4805e-01, -1.0547e-01,  1.4221e-02, -1.8750e+00,  6.6895e-02,\n",
       "          -1.4648e-01, -1.8652e-01, -1.0681e-02, -1.8594e+00, -1.8921e-02,\n",
       "           2.0020e-02, -2.6123e-02, -3.0078e-01,  3.4375e-01, -2.5586e-01,\n",
       "           1.9932e-04,  2.0312e+00,  1.3504e-03, -1.9531e-01, -1.1182e-01,\n",
       "          -2.4707e-01,  2.6172e-01, -1.5918e-01,  9.1309e-02,  1.1377e-01,\n",
       "          -9.3262e-02,  3.2959e-02,  1.5918e-01, -2.4567e-03,  2.9102e-01,\n",
       "          -9.4727e-02, -1.2793e-01,  3.3203e-01,  1.4941e-01, -1.6797e-01,\n",
       "           1.8359e-01,  2.2070e-01,  2.1484e-02, -2.5781e-01,  1.8164e-01,\n",
       "          -3.1445e-01, -7.4219e-01, -4.2969e-01,  2.6094e+00,  8.3594e-01,\n",
       "           1.8750e-01, -2.8320e-01,  7.5684e-02,  1.0205e-01,  4.2969e-02,\n",
       "          -3.9062e-01, -1.6309e-01, -1.0693e-01, -8.8379e-02, -8.8672e-01,\n",
       "           4.0039e-01, -1.6016e-01,  1.0469e+00,  2.6367e-01,  1.5430e-01,\n",
       "           2.8906e-01, -7.0703e-01,  2.6953e-01, -1.2988e-01, -8.0469e-01,\n",
       "          -2.0996e-01, -4.2188e-01, -3.9062e-01, -5.1172e-01,  1.4746e-01,\n",
       "           4.5312e-01,  2.1606e-02,  1.8359e-01,  4.5654e-02, -1.4062e+00,\n",
       "           1.9141e-01, -1.7188e-01,  4.0283e-02,  1.1250e+00,  2.9419e-02,\n",
       "          -3.7891e-01, -2.4316e-01,  3.3789e-01, -8.0469e-01,  1.9141e-01,\n",
       "          -8.3984e-02, -1.8281e+00,  1.0693e-01, -3.4180e-01, -3.5352e-01,\n",
       "           1.2988e-01,  1.5234e-01,  3.3789e-01, -2.5977e-01,  3.8867e-01,\n",
       "          -1.0498e-01, -4.1260e-02, -1.5625e-01,  1.3281e-01,  2.6953e-01,\n",
       "           1.4355e-01,  3.6914e-01,  4.0039e-02,  4.5703e-01,  2.0410e-01,\n",
       "          -5.2344e-01, -4.4336e-01,  1.0234e+00, -9.3262e-02, -1.1719e-01,\n",
       "          -2.6172e-01, -4.4141e-01,  7.1484e-01, -4.1016e-02, -7.7637e-02,\n",
       "          -6.0938e-01, -8.6426e-02,  2.1191e-01,  3.0664e-01,  6.8359e-02,\n",
       "           2.4023e-01, -2.0996e-01, -1.3550e-02, -2.4121e-01, -7.1875e-01,\n",
       "           3.6914e-01,  6.0938e-01, -4.6387e-02,  1.3965e-01,  2.1875e-01,\n",
       "          -2.5757e-02, -1.7422e+00, -3.3691e-02, -2.0020e-01,  3.5889e-02,\n",
       "          -9.8145e-02, -1.8616e-03, -3.3203e-02,  6.2500e-01,  8.7891e-01,\n",
       "          -4.7607e-03, -2.8687e-02,  1.3516e+00, -4.2969e-02,  1.6406e-01,\n",
       "           8.6426e-02, -1.5039e-01, -2.1387e-01,  1.0498e-01,  8.7891e-02,\n",
       "          -2.0996e-01,  1.4587e-02, -2.0142e-02,  1.6113e-01,  3.3984e-01,\n",
       "          -6.7578e-01,  1.7188e-01, -5.7422e-01,  1.1182e-01, -5.3906e-01,\n",
       "          -5.5859e-01, -6.3965e-02,  1.6992e-01, -2.3730e-01, -5.6250e+00,\n",
       "           2.4414e-02,  6.3281e-01,  2.5195e-01,  3.3789e-01, -5.4688e-01,\n",
       "           1.9922e-01, -3.3594e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0157, -0.0061, -0.0123,  ...,  0.0339,  0.0664, -0.0008],\n",
       "          [ 0.0106,  0.0025, -0.0400,  ...,  0.0234, -0.0175, -0.0413],\n",
       "          [ 0.0483, -0.0203, -0.0275,  ...,  0.0292,  0.0500, -0.0042],\n",
       "          ...,\n",
       "          [-0.0045,  0.0232,  0.0103,  ..., -0.0146,  0.0173,  0.0066],\n",
       "          [-0.0267,  0.0275, -0.0143,  ..., -0.0115,  0.0179,  0.0093],\n",
       "          [ 0.0243, -0.0128, -0.0205,  ..., -0.0149, -0.0244,  0.0042]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([-6.7188e-01, -5.0391e-01, -3.4961e-01, -5.4626e-03, -3.6133e-01,\n",
       "          -6.6016e-01, -1.2891e-01,  2.6953e-01,  3.4570e-01,  4.5508e-01,\n",
       "          -4.5166e-02,  9.4727e-02,  6.2012e-02,  4.1992e-01, -1.8457e-01,\n",
       "           2.4121e-01, -3.9844e-01,  4.0039e-02,  1.2109e-01, -6.6016e-01,\n",
       "           2.5781e-01, -1.1572e-01,  3.6914e-01, -1.1377e-01,  1.0449e-01,\n",
       "          -2.8516e-01, -1.4648e-01, -6.0547e-01,  1.2891e-01, -4.9805e-01,\n",
       "           3.4766e-01, -3.5547e-01,  5.5469e-01,  2.4902e-01,  2.7734e-01,\n",
       "           1.7944e-02,  1.9336e-01,  3.6133e-01, -6.5625e-01,  4.7852e-01,\n",
       "           1.7480e-01, -5.7373e-03, -1.8555e-02, -2.4121e-01,  7.3242e-02,\n",
       "          -2.0801e-01, -8.9844e-02, -2.5000e-01, -4.1016e-01, -7.3047e-01,\n",
       "           3.4375e-01,  1.3184e-01, -7.5781e-01, -7.4219e-02,  8.8379e-02,\n",
       "          -9.5703e-01, -3.2031e-01,  2.6172e-01, -2.4512e-01, -2.6172e-01,\n",
       "           9.8438e-01, -2.5195e-01, -2.2070e-01, -3.6377e-02,  7.3438e-01,\n",
       "          -6.5430e-02,  2.9785e-02,  1.8652e-01, -5.2734e-01,  3.0859e-01,\n",
       "          -3.1641e-01, -2.4023e-01, -1.7773e-01,  3.8086e-01,  3.3398e-01,\n",
       "           2.2754e-01, -3.6719e-01,  6.5234e-01,  1.0605e-03, -1.6602e-01,\n",
       "           4.1992e-01, -1.2817e-02, -3.4912e-02, -6.5234e-01,  3.4766e-01,\n",
       "           1.2500e-01, -9.8633e-02, -1.3672e-01, -1.9336e-01,  1.5723e-01,\n",
       "          -2.0264e-02,  3.1445e-01,  4.9023e-01,  1.2695e-01,  3.0762e-02,\n",
       "           1.1035e-01, -9.1309e-02,  3.9648e-01,  1.5430e-01, -4.1016e-01,\n",
       "           9.9487e-03,  5.3125e-01,  1.3965e-01,  1.5137e-01, -1.3477e-01,\n",
       "           2.3242e-01, -2.1387e-01,  1.6797e-01,  1.3977e-02,  3.0469e-01,\n",
       "          -2.2656e-01,  1.4941e-01, -3.6914e-01, -5.3125e-01, -1.2451e-01,\n",
       "          -4.4861e-03, -3.4766e-01, -1.0132e-02, -2.0996e-01, -4.5703e-01,\n",
       "          -3.6328e-01,  3.3203e-02,  2.5195e-01, -2.7344e-01,  6.3281e-01,\n",
       "          -4.9219e-01, -4.9414e-01, -2.0508e-01,  3.9795e-02,  7.0801e-02,\n",
       "           1.5527e-01,  2.2095e-02,  1.2061e-01,  5.5664e-02,  6.8359e-02,\n",
       "          -6.8359e-02,  7.8125e-02,  1.1230e-01,  6.0303e-02, -1.2109e-01,\n",
       "          -3.6621e-02,  2.9688e-01, -2.6855e-02,  8.0566e-02, -2.1973e-01,\n",
       "          -3.0640e-02, -2.4414e-02, -1.8677e-02,  1.6113e-01,  5.2490e-03,\n",
       "           4.6631e-02,  7.4219e-02, -1.9043e-02, -8.5449e-02,  1.1182e-01,\n",
       "          -1.5918e-01,  1.6895e-01, -3.2959e-02,  2.2363e-01,  3.0518e-02,\n",
       "           3.6865e-02, -1.2891e-01, -2.0508e-01,  2.2583e-02, -1.3574e-01,\n",
       "          -4.8340e-02,  7.0801e-02, -1.1349e-04,  7.8201e-04,  1.0681e-02,\n",
       "          -1.4453e-01,  5.0049e-02,  5.6396e-02,  4.9072e-02,  6.3965e-02,\n",
       "          -1.1230e-01,  8.3984e-02, -8.4839e-03, -7.9956e-03,  7.1289e-02,\n",
       "          -1.2793e-01, -8.5449e-02,  6.1523e-02, -1.1475e-01, -1.1963e-01,\n",
       "           2.1680e-01,  4.9805e-02,  8.8379e-02, -1.4062e-01,  6.5918e-02,\n",
       "          -1.5039e-01,  1.1414e-02,  7.5195e-02, -1.1841e-02, -2.0504e-04,\n",
       "           1.1353e-02, -2.1362e-02,  7.0801e-02,  1.6113e-01,  1.0742e-01,\n",
       "           1.7090e-02, -1.3611e-02, -1.7212e-02,  9.0820e-02,  2.1387e-01,\n",
       "           1.9165e-02,  1.5076e-02, -1.0559e-02, -1.6113e-01,  4.3945e-02,\n",
       "          -7.6172e-02, -2.1729e-02, -8.9355e-02, -8.9844e-02,  1.6309e-01,\n",
       "           7.8125e-02,  5.3955e-02, -7.3730e-02, -1.0742e-01, -1.0547e-01,\n",
       "          -7.4219e-02, -3.7109e-02, -4.4678e-02, -2.1118e-02, -1.2988e-01,\n",
       "           1.4587e-02,  3.5156e-02, -6.8359e-02,  1.3672e-01, -1.0107e-01,\n",
       "           5.4932e-03,  2.6758e-01,  1.1230e-01, -6.0059e-02, -1.6846e-02,\n",
       "          -1.1963e-02, -1.4551e-01,  5.1270e-02,  8.8379e-02, -1.6968e-02,\n",
       "           6.4087e-03,  9.8633e-02,  4.6875e-02, -6.7383e-02, -2.3560e-02,\n",
       "           1.0059e-01,  7.5684e-02,  1.1963e-02,  4.1992e-02,  1.4954e-02,\n",
       "          -3.2812e-01, -3.6621e-02, -4.2969e-02,  2.9144e-03,  7.9102e-02,\n",
       "          -1.3916e-02, -1.1816e-01, -1.3281e-01, -9.0820e-02, -5.4932e-02,\n",
       "          -1.7090e-02,  1.1377e-01,  1.0449e-01, -7.7148e-02, -8.3984e-02,\n",
       "          -8.1875e+00, -6.6406e-02,  2.2461e-01, -1.2695e-01, -2.9053e-02,\n",
       "           2.7344e-02, -7.7637e-02, -4.1992e-02,  5.7861e-02, -4.5410e-02,\n",
       "           2.0117e-01, -1.0303e-01,  1.5820e-01,  4.1260e-02,  2.2168e-01,\n",
       "           1.0449e-01,  1.0791e-01, -6.0791e-02,  1.3867e-01, -8.3008e-02,\n",
       "           1.3281e-01, -1.8359e-01, -1.3379e-01, -4.1504e-02, -3.7842e-02,\n",
       "          -7.4219e-02,  6.6406e-02,  5.3955e-02, -2.4316e-01,  6.5918e-02,\n",
       "           1.8921e-02,  7.7148e-02,  1.4771e-02,  1.8677e-02,  7.8613e-02,\n",
       "          -1.2500e-01,  5.7861e-02, -1.9531e-01, -2.4609e-01, -1.5430e-01,\n",
       "          -1.4062e-01, -5.5908e-02, -1.4941e-01,  1.3733e-02, -1.7188e-01,\n",
       "           4.2480e-02, -1.3184e-01,  1.5820e-01, -1.1670e-01,  1.3281e-01,\n",
       "           5.2246e-02,  1.8652e-01,  1.0156e-01, -1.4844e-01, -1.5430e-01,\n",
       "           1.0254e-01,  7.0496e-03, -1.3965e-01,  9.0820e-02, -5.3955e-02,\n",
       "           1.2500e-01, -4.3945e-02, -6.0059e-02,  1.2695e-02,  8.7402e-02,\n",
       "          -9.8145e-02, -7.9590e-02, -8.3496e-02,  1.1719e-01,  1.7578e-01,\n",
       "          -1.4844e-01, -9.8633e-02, -4.1504e-02, -9.2773e-03,  4.5654e-02,\n",
       "          -7.6660e-02, -2.4707e-01, -4.8584e-02, -1.6699e-01,  1.1279e-01,\n",
       "          -1.8457e-01,  3.7842e-02, -4.4189e-02,  2.0874e-02,  5.2490e-03,\n",
       "           1.3281e-01,  7.0312e-02,  1.0889e-01, -1.2305e-01, -9.3750e-02,\n",
       "          -2.1387e-01,  6.2500e-02,  2.1289e-01, -2.2363e-01,  1.9727e-01,\n",
       "           8.0078e-02, -1.1816e-01,  9.1553e-04, -8.3984e-02,  4.7852e-02,\n",
       "           9.2163e-03, -3.4375e-01, -2.4170e-02,  3.7994e-03, -8.0566e-02,\n",
       "           7.7637e-02, -9.8145e-02,  6.2012e-02,  3.6865e-02, -3.7109e-02,\n",
       "           3.1738e-02, -1.0437e-02,  9.6680e-02,  9.1553e-03, -6.5430e-02,\n",
       "           1.6895e-01, -1.6406e-01, -1.6113e-02,  1.3379e-01, -1.6602e-01,\n",
       "          -6.4453e-02,  2.1680e-01,  1.5918e-01, -1.5747e-02,  4.5898e-02,\n",
       "          -6.5430e-02,  1.6992e-01,  3.0273e-02,  2.0996e-01,  1.8848e-01,\n",
       "           4.6631e-02, -2.7710e-02, -9.3262e-02,  8.3984e-02,  1.4832e-02,\n",
       "          -1.1865e-01, -2.5195e-01,  8.5938e-02, -1.7578e-01,  1.0352e-01,\n",
       "           2.5000e-01,  8.2520e-02,  1.4453e-01,  1.3379e-01, -6.3782e-03,\n",
       "          -1.8652e-01,  3.1006e-02,  4.9561e-02, -5.9509e-03,  1.2891e-01,\n",
       "          -6.6406e-02,  2.0020e-02, -2.7954e-02,  2.2852e-01,  8.4473e-02,\n",
       "          -1.1230e-01, -1.4062e-01,  5.1270e-02, -1.2695e-01,  5.5908e-02,\n",
       "          -4.2969e-01, -1.8555e-01,  9.4238e-02,  1.8066e-01, -8.0078e-02,\n",
       "           8.7891e-02,  1.4355e-01,  1.2109e-01, -5.1514e-02, -1.0791e-01,\n",
       "           3.4668e-02,  5.4199e-02,  1.0889e-01,  1.0547e-01, -8.1055e-02,\n",
       "          -1.4746e-01, -1.2598e-01,  1.7480e-01,  3.8574e-02, -1.0559e-02,\n",
       "          -5.5664e-02,  7.9590e-02,  1.1963e-01, -8.5449e-02,  1.1572e-01,\n",
       "           8.0078e-02,  1.1816e-01, -3.9551e-02, -1.5430e-01,  1.6479e-02,\n",
       "           1.7456e-02,  8.7402e-02,  1.6895e-01, -2.1289e-01, -1.8945e-01,\n",
       "           8.1055e-02,  7.3730e-02, -1.3184e-01,  5.7373e-02, -1.5723e-01,\n",
       "           5.7129e-02, -5.5469e-01,  7.4219e-02,  8.2520e-02, -6.4453e-02,\n",
       "          -8.0566e-02,  1.7944e-02,  8.0872e-04, -6.7383e-02,  1.2695e-01,\n",
       "          -1.8262e-01, -1.2695e-01, -5.4443e-02,  1.5723e-01, -7.6660e-02,\n",
       "          -2.8125e-01, -7.7820e-03,  1.2402e-01, -1.3281e-01, -1.0547e-01,\n",
       "           3.6865e-02,  3.6133e-02, -2.3340e-01, -6.8359e-02, -2.2266e-01,\n",
       "           1.1035e-01, -1.2268e-02, -1.6602e-01,  1.0059e-01, -1.1572e-01,\n",
       "           1.9824e-01,  1.1133e-01, -3.2812e-01, -8.1787e-03,  6.2988e-02,\n",
       "           1.8066e-01,  3.1494e-02, -4.3213e-02, -7.5195e-02,  8.0566e-02,\n",
       "           3.6865e-02, -1.3770e-01,  8.7891e-02, -4.2236e-02,  1.2390e-02,\n",
       "           2.3926e-02,  1.3965e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 1.5747e-02, -8.7891e-03,  1.1719e-02,  ..., -1.2451e-02,\n",
       "            3.1281e-03, -2.5635e-03],\n",
       "          [-8.1787e-03, -1.0071e-03, -4.7913e-03,  ...,  4.2969e-02,\n",
       "            2.3560e-02,  1.3504e-03],\n",
       "          [ 1.9287e-02,  1.8555e-02,  1.2589e-03,  ...,  2.2697e-04,\n",
       "           -9.6893e-04, -4.5166e-03],\n",
       "          ...,\n",
       "          [ 3.0823e-03,  1.2878e-02, -3.6133e-02,  ..., -1.7212e-02,\n",
       "            1.1902e-02, -1.1475e-02],\n",
       "          [-1.0254e-02,  3.5553e-03,  1.8311e-02,  ...,  2.3438e-02,\n",
       "           -1.9897e-02,  1.8066e-02],\n",
       "          [ 6.8188e-05,  2.7344e-02,  2.0874e-02,  ...,  3.7842e-02,\n",
       "           -8.5449e-03, -1.1230e-02]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0112,  0.0066, -0.0151,  ..., -0.0028,  0.0339, -0.0160],\n",
       "          [ 0.0007,  0.0127,  0.0166,  ..., -0.0062, -0.0219, -0.0054],\n",
       "          [-0.0310, -0.0104,  0.0018,  ..., -0.0119,  0.0071, -0.0057],\n",
       "          ...,\n",
       "          [ 0.0183, -0.0066,  0.0221,  ...,  0.0172,  0.0103, -0.0099],\n",
       "          [-0.0247, -0.0214,  0.0156,  ...,  0.0110, -0.0208, -0.0001],\n",
       "          [-0.0055, -0.0145, -0.0111,  ..., -0.0203, -0.0129,  0.0209]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0065,  0.0112,  0.0240,  ..., -0.0269, -0.0060, -0.0098],\n",
       "          [-0.0033, -0.0209, -0.0013,  ...,  0.0183, -0.0090,  0.0106],\n",
       "          [ 0.0003,  0.0177,  0.0225,  ..., -0.0308, -0.0134,  0.0220],\n",
       "          ...,\n",
       "          [ 0.0156,  0.0086,  0.0320,  ...,  0.0201,  0.0259, -0.0037],\n",
       "          [ 0.0225, -0.0327,  0.0011,  ..., -0.0017,  0.0009, -0.0009],\n",
       "          [ 0.0026, -0.0176,  0.0267,  ..., -0.0403, -0.0251, -0.0152]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 7.8735e-03,  2.4292e-02, -1.8001e-05,  ...,  2.4902e-02,\n",
       "           -1.5564e-02,  1.8433e-02],\n",
       "          [-2.2095e-02, -2.6941e-05, -1.3855e-02,  ...,  1.3245e-02,\n",
       "           -1.1780e-02,  4.8523e-03],\n",
       "          [ 1.0986e-02, -3.5400e-02, -1.8921e-02,  ...,  2.9663e-02,\n",
       "            2.0020e-02, -3.1250e-02],\n",
       "          ...,\n",
       "          [ 3.2715e-02, -9.8267e-03,  2.5024e-02,  ..., -6.6223e-03,\n",
       "            2.9907e-02, -3.4180e-03],\n",
       "          [ 1.0315e-02, -9.5367e-04,  3.0396e-02,  ..., -1.7578e-02,\n",
       "           -1.0498e-02,  3.5400e-02],\n",
       "          [-1.4160e-02,  2.0752e-02,  2.4170e-02,  ..., -2.4872e-03,\n",
       "            1.3123e-02, -3.1090e-04]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([0.8828, 0.7578, 0.9297,  ..., 0.8438, 0.8867, 0.8516],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([1.7422, 1.7109, 1.7969,  ..., 1.5469, 1.7578, 1.7578],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0154,  0.0430, -0.0275,  ..., -0.0018, -0.0220, -0.0022],\n",
       "          [ 0.0067,  0.0508, -0.0165,  ..., -0.0078, -0.0035, -0.0039],\n",
       "          [ 0.0092, -0.0126,  0.0209,  ...,  0.0072, -0.0175,  0.0016],\n",
       "          ...,\n",
       "          [-0.0105,  0.0010, -0.0172,  ..., -0.0219,  0.0043, -0.0122],\n",
       "          [-0.0184,  0.0181,  0.0101,  ...,  0.0021,  0.0029,  0.0188],\n",
       "          [-0.0008, -0.0095,  0.0094,  ...,  0.0120,  0.0060,  0.0023]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([-0.1738, -0.1729,  0.1973,  ..., -0.3633,  0.2871, -0.0349],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0043,  0.0012,  0.0004,  ...,  0.0006,  0.0072,  0.0003],\n",
       "          [ 0.0003,  0.0052, -0.0019,  ...,  0.0013, -0.0136, -0.0093],\n",
       "          [-0.0117, -0.0055, -0.0035,  ..., -0.0186,  0.0096, -0.0106],\n",
       "          ...,\n",
       "          [-0.0121,  0.0190,  0.0226,  ...,  0.0086,  0.0093,  0.0099],\n",
       "          [ 0.0339, -0.0216, -0.0107,  ...,  0.0140,  0.0076, -0.0005],\n",
       "          [ 0.0204,  0.0057, -0.0261,  ..., -0.0197, -0.0017, -0.0098]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([-1.7285e-01, -3.4375e-01,  2.9688e-01, -2.6367e-02,  2.7344e-01,\n",
       "          -3.4961e-01, -5.0293e-02,  3.8477e-01,  3.6621e-02,  3.6328e-01,\n",
       "           1.0840e-01,  1.4355e-01, -1.0352e-01, -1.8457e-01,  1.3672e-01,\n",
       "           1.2988e-01, -3.0664e-01, -3.3594e-01, -9.1797e-02, -4.0625e-01,\n",
       "          -4.4922e-01, -2.3633e-01, -5.4297e-01,  4.7363e-02,  3.0664e-01,\n",
       "          -3.0273e-01,  1.3184e-01,  1.7969e-01, -1.9336e-01, -6.1328e-01,\n",
       "           2.2070e-01,  1.4141e+00, -2.0215e-01,  7.8516e-01,  2.6758e-01,\n",
       "          -1.2695e-01, -1.1719e-01,  8.0469e-01,  7.1289e-02,  1.1484e+00,\n",
       "          -2.9492e-01,  7.7637e-02,  2.6758e-01,  3.4766e-01, -1.9141e-01,\n",
       "          -3.8086e-01,  8.5156e-01,  2.1582e-01, -2.5000e+00,  2.6367e-01,\n",
       "           4.3945e-01, -6.5918e-02,  9.9121e-02, -7.8125e-03, -6.4844e-01,\n",
       "           6.3281e-01,  8.4961e-02, -1.8750e-01, -1.5938e+00,  8.8672e-01,\n",
       "          -9.2285e-02,  1.2158e-01,  8.5156e-01, -1.9688e+00, -3.7695e-01,\n",
       "          -1.9238e-01,  2.1680e-01,  3.8477e-01, -2.9883e-01, -5.0781e-02,\n",
       "          -1.3379e-01, -5.5908e-02, -1.3379e-01, -4.9072e-02,  7.7209e-03,\n",
       "           3.3398e-01,  2.4872e-03, -3.3008e-01, -2.0996e-01, -1.6406e-01,\n",
       "           1.0889e-01,  2.2070e-01, -2.0898e-01, -9.3994e-03,  3.2959e-02,\n",
       "          -8.7891e-02,  2.6172e-01, -4.0820e-01,  4.6484e-01,  4.4141e-01,\n",
       "           8.1250e-01,  2.1362e-03,  6.6406e-02,  5.9375e-01, -5.3711e-02,\n",
       "           2.5977e-01,  2.3804e-02, -3.1641e-01, -1.0938e-01, -2.8516e-01,\n",
       "           3.0273e-01, -1.2500e+00, -3.2031e-01,  8.8867e-02,  2.9492e-01,\n",
       "          -1.6699e-01,  1.9062e+00,  3.5938e-01, -2.5391e-01, -1.0547e-01,\n",
       "          -1.9531e-01, -2.5391e-01, -7.0801e-02,  2.3535e-01, -1.3281e-01,\n",
       "           2.0020e-01, -3.3203e-02,  4.8633e-01, -5.3223e-02,  2.7344e-01,\n",
       "           8.8867e-02,  2.0410e-01, -2.2500e+00,  7.4609e-01, -1.5820e-01,\n",
       "           5.0391e-01,  8.9844e-01, -1.1328e+00, -9.5312e-01, -5.1953e-01,\n",
       "           5.5469e-01, -7.6562e-01,  2.8320e-01,  6.3281e-01,  9.6680e-02,\n",
       "          -6.1951e-03, -1.4258e-01, -2.5586e-01,  2.6733e-02,  5.7031e-01,\n",
       "           3.1281e-03, -2.8711e-01,  7.6660e-02, -2.3047e-01, -5.5176e-02,\n",
       "          -4.5312e-01,  2.4414e-01, -3.3594e-01,  1.6406e-01, -3.8330e-02,\n",
       "          -6.8750e-01,  1.3916e-02, -5.7617e-02,  2.0801e-01,  4.5703e-01,\n",
       "           2.5879e-02, -2.0215e-01, -5.9766e-01,  2.0703e-01, -1.0596e-01,\n",
       "          -4.6631e-02,  5.5237e-03, -7.9102e-02,  2.0605e-01, -1.3672e-01,\n",
       "           2.1729e-02, -3.2031e-01, -2.2168e-01, -2.0801e-01,  2.8687e-02,\n",
       "           1.2422e+00,  5.9326e-02, -1.0742e-01, -1.0352e-01,  5.8594e-02,\n",
       "          -1.2109e-01, -1.4258e-01,  3.7842e-02,  2.2168e-01,  1.7871e-01,\n",
       "           8.8867e-02, -3.8086e-01,  2.4805e-01,  2.0410e-01, -8.3008e-02,\n",
       "           3.5742e-01, -3.6719e-01, -2.5000e-01, -4.8828e-01, -4.0312e+00,\n",
       "           1.0254e-01,  3.9844e-01, -1.3906e+00, -5.9766e-01, -8.5547e-01,\n",
       "          -4.9219e-01, -4.9414e-01,  2.3145e-01,  1.6992e-01, -3.5742e-01,\n",
       "           6.6797e-01,  8.9844e-02, -1.4160e-01,  1.5430e-01, -2.4805e-01,\n",
       "           2.1777e-01, -4.3457e-02, -2.2656e-01,  1.5820e-01,  3.0060e-03,\n",
       "           5.8594e-01, -6.8848e-02,  3.7891e-01, -2.4121e-01,  1.3086e-01,\n",
       "          -4.1602e-01, -2.4707e-01, -1.8457e-01,  8.5938e-02, -4.0234e-01,\n",
       "          -4.8828e-01,  1.7969e-01, -3.9864e-04, -5.9766e-01, -2.7734e-01,\n",
       "          -1.1016e+00, -2.5482e-03,  7.9590e-02, -1.2109e-01,  1.4160e-01,\n",
       "           6.3281e-01, -5.8594e-02,  1.1475e-02, -2.7344e-02,  1.9531e-01,\n",
       "          -1.3574e-01, -1.9141e-01,  2.3730e-01,  1.1963e-01, -5.1025e-02,\n",
       "          -2.6094e+00,  8.6426e-02,  2.0898e-01,  9.9121e-02, -1.7773e-01,\n",
       "          -2.5000e-01, -1.6797e-01, -2.6953e-01,  6.2500e-02,  8.0566e-02,\n",
       "          -7.5391e-01,  1.5137e-01,  8.2422e-01, -1.2656e+00,  8.0078e-01,\n",
       "           1.7676e-01, -9.3750e-02, -1.8457e-01,  1.4941e-01, -2.7734e-01,\n",
       "          -7.7148e-02, -6.6895e-02,  3.4375e-01, -1.0645e-01, -2.5781e-01,\n",
       "          -1.2891e-01,  2.5586e-01, -1.7676e-01,  3.3594e-01, -4.0625e-01,\n",
       "           1.7383e-01,  6.8848e-02,  4.3359e-01, -9.1309e-02,  8.6426e-02,\n",
       "          -3.4180e-01, -2.2095e-02,  4.4922e-02, -7.9297e-01,  2.8198e-02,\n",
       "           5.0781e-01,  8.5156e-01,  4.6387e-02,  3.7891e-01, -3.2227e-01,\n",
       "           1.9165e-02, -4.7119e-02,  1.4453e-01,  1.3672e-01,  1.3184e-01,\n",
       "          -9.8047e-01, -1.6403e-03,  1.0156e-01,  1.1094e+00,  1.2878e-02,\n",
       "           6.5918e-02,  3.2031e-01,  2.1094e-01, -7.8613e-02, -4.3750e-01,\n",
       "           8.4375e-01,  4.2578e-01,  1.1377e-01, -1.8984e+00,  3.3984e-01,\n",
       "           1.0620e-02,  2.8229e-03, -8.7891e-02, -1.8457e-01, -6.8750e-01,\n",
       "          -7.9590e-02,  7.5391e-01,  1.4062e-01,  2.5195e-01, -2.4023e-01,\n",
       "          -2.8125e+00, -3.1250e-01, -1.2188e+00, -3.5547e-01, -1.1250e+00,\n",
       "          -3.3398e-01, -9.0332e-02,  9.6680e-02, -2.6733e-02, -1.2305e-01,\n",
       "          -1.4941e-01,  1.2988e-01,  1.0059e-01,  1.2695e-01, -1.9922e-01,\n",
       "           1.5527e-01, -3.0273e-01,  7.9956e-03, -9.4238e-02,  2.9688e-01,\n",
       "          -1.3281e-01, -3.4766e-01,  1.3379e-01, -4.8828e-01,  1.8262e-01,\n",
       "           4.7266e-01,  1.0254e-01,  1.4453e-01, -2.0215e-01, -3.0469e-01,\n",
       "          -6.6895e-02, -5.0391e-01,  1.5234e-01, -7.3828e-01, -1.9238e-01,\n",
       "           4.8340e-02, -1.7500e+00,  2.0508e-02,  8.9844e-02,  5.8594e-01,\n",
       "           5.0049e-02,  2.0898e-01,  8.0469e-01,  3.4424e-02, -1.6699e-01,\n",
       "           2.1719e+00,  1.0010e-01, -1.9922e-01,  5.3906e-01,  1.0925e-02,\n",
       "          -1.2344e+00,  1.1377e-01, -1.8281e+00, -1.3184e-01,  1.2354e-01,\n",
       "          -1.6016e-01,  8.4961e-02, -9.2285e-02,  5.8594e-01,  3.9062e-01,\n",
       "           4.4531e-01,  3.2227e-01,  5.1562e-01,  9.8145e-02,  1.3047e+00,\n",
       "          -3.6328e-01, -7.1777e-02,  6.0059e-02,  6.0156e-01, -1.3750e+00,\n",
       "           9.3359e-01,  3.3203e-01,  8.2812e-01, -4.2773e-01, -3.4766e-01,\n",
       "           1.8457e-01,  5.5469e-01, -1.9922e-01,  5.8984e-01, -5.6885e-02,\n",
       "           7.4609e-01, -3.4375e-01, -1.0469e+00,  9.7656e-02,  2.5513e-02,\n",
       "           2.7832e-02,  9.2188e-01, -2.6611e-02, -1.5625e-01, -6.2988e-02,\n",
       "          -1.1016e+00, -1.2988e-01, -2.0117e-01, -7.0312e-02,  2.9102e-01,\n",
       "           5.4297e-01,  4.5508e-01,  2.1777e-01, -2.3633e-01, -6.6406e-02,\n",
       "           1.6250e+00, -1.6406e-01,  1.0742e-01,  2.9492e-01, -3.1836e-01,\n",
       "           3.4961e-01, -3.2422e-01,  2.0215e-01,  5.1025e-02,  2.9419e-02,\n",
       "           1.6016e-01,  4.2578e-01,  7.1484e-01,  4.5898e-02,  7.6172e-01,\n",
       "          -3.1836e-01, -3.3447e-02,  2.4512e-01,  1.8262e-01,  4.4531e-01,\n",
       "           1.6235e-02,  1.2451e-01, -6.3281e-01, -4.6289e-01, -1.9727e-01,\n",
       "          -1.6719e+00,  5.5908e-02,  1.3672e-01,  2.7930e-01,  3.0781e+00,\n",
       "           6.4062e-01, -1.7285e-01,  8.7500e-01, -1.4941e-01,  4.7852e-02,\n",
       "           7.9102e-02,  2.0020e-01, -1.7188e-01,  9.9219e-01,  3.1494e-02,\n",
       "           2.8906e-01,  6.8750e-01,  5.6250e-01, -1.1133e-01,  1.0352e-01,\n",
       "          -2.6172e-01, -1.3379e-01,  1.4453e-01, -8.2031e-02,  2.7734e-01,\n",
       "           2.5391e-02, -1.0986e-01, -7.0801e-02,  5.2246e-02,  7.6660e-02,\n",
       "           1.5234e-01, -3.4766e-01,  9.7266e-01,  1.1406e+00, -1.1768e-01,\n",
       "           5.8984e-01,  5.2734e-01,  1.1865e-01,  3.4180e-01, -2.5000e-01,\n",
       "          -2.3926e-01, -3.7354e-02,  4.0820e-01, -1.9531e-01,  6.3965e-02,\n",
       "          -2.0312e-01,  1.5000e+00,  1.8164e-01, -4.3750e-01,  5.7129e-02,\n",
       "          -1.6211e-01,  6.5625e-01, -1.7500e+00,  6.0156e-01, -1.5625e-01,\n",
       "          -6.8359e-01, -3.2422e-01,  2.6953e-01,  2.5977e-01, -2.9492e-01,\n",
       "           2.1582e-01,  1.6016e+00,  1.9629e-01, -2.2070e-01,  3.2422e-01,\n",
       "           5.0781e-01, -6.0938e-01, -4.8633e-01,  3.4375e+00, -3.7354e-02,\n",
       "          -5.7373e-02, -5.4688e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0432,  0.0173,  0.0157,  ..., -0.0233,  0.0214,  0.0019],\n",
       "          [ 0.0152, -0.0179, -0.0261,  ...,  0.0184, -0.0095, -0.0243],\n",
       "          [-0.0198,  0.0586, -0.0157,  ..., -0.0239, -0.0281,  0.0276],\n",
       "          ...,\n",
       "          [ 0.0854, -0.0874, -0.0317,  ..., -0.0151, -0.0138,  0.0432],\n",
       "          [ 0.0259,  0.0562, -0.0276,  ...,  0.0615, -0.0070, -0.0182],\n",
       "          [-0.0391, -0.0317, -0.0115,  ..., -0.0154, -0.0033, -0.0732]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 3.6133e-01,  7.7148e-02, -2.3926e-01,  1.9336e-01,  3.1494e-02,\n",
       "          -2.7832e-02,  2.0605e-01,  2.3438e-01, -9.3994e-03, -5.2002e-02,\n",
       "           3.0396e-02, -1.4258e-01, -2.8125e-01,  5.7861e-02,  1.0986e-01,\n",
       "          -1.7285e-01, -1.4062e-01, -3.8477e-01, -1.9141e-01, -1.2891e-01,\n",
       "           1.8359e-01,  1.6724e-02,  6.6406e-02, -3.2617e-01,  2.9492e-01,\n",
       "           1.4648e-02, -1.8457e-01,  1.9727e-01, -2.2656e-01,  1.8359e-01,\n",
       "           8.3984e-02,  1.0059e-01, -3.1836e-01,  1.2891e-01,  5.0781e-02,\n",
       "           3.3203e-01, -1.6309e-01, -3.9551e-02,  2.8125e-01, -1.5137e-01,\n",
       "           1.7822e-02, -3.6914e-01, -1.0864e-02,  1.8555e-02, -2.3828e-01,\n",
       "           2.6172e-01, -1.2109e-01, -1.1377e-01,  1.9629e-01, -2.3730e-01,\n",
       "           1.3672e-01,  7.4707e-02,  2.6001e-02, -2.7148e-01,  2.2363e-01,\n",
       "          -1.0156e-01,  8.6914e-02, -1.2512e-02,  1.9727e-01, -2.4707e-01,\n",
       "           1.6602e-01, -2.0605e-01,  3.3594e-01,  3.3398e-01, -1.2012e-01,\n",
       "          -1.0693e-01,  5.7129e-02, -8.7891e-02, -1.6504e-01, -1.7773e-01,\n",
       "           7.0190e-03, -5.1270e-02, -1.3770e-01,  6.6895e-02,  8.0078e-02,\n",
       "           1.5625e-01,  1.5527e-01, -2.3242e-01, -5.9082e-02,  5.7129e-02,\n",
       "           1.6235e-02,  2.8076e-02, -2.5391e-01, -2.2656e-01,  2.2168e-01,\n",
       "          -1.9043e-01, -6.1035e-02,  3.8867e-01,  2.6123e-02,  2.4512e-01,\n",
       "           9.2285e-02,  4.4434e-02, -3.0469e-01,  1.6895e-01, -6.8848e-02,\n",
       "           1.8945e-01,  6.3477e-02,  4.1260e-02,  9.6321e-05, -1.1719e-01,\n",
       "           3.1055e-01,  2.8516e-01, -2.8320e-01, -7.1777e-02,  1.3855e-02,\n",
       "           4.7070e-01, -1.7578e-01, -2.9883e-01, -2.0898e-01,  2.1875e-01,\n",
       "           4.9316e-02, -3.8086e-02, -1.2988e-01, -2.4170e-02, -4.6484e-01,\n",
       "           4.7363e-02, -2.1875e-01,  2.1667e-03,  1.8750e-01, -2.3730e-01,\n",
       "           8.0078e-02, -2.8076e-02,  2.3145e-01,  1.7090e-01, -1.5820e-01,\n",
       "          -2.7148e-01,  1.2988e-01, -6.9336e-02,  3.9648e-01,  1.7383e-01,\n",
       "          -1.1279e-01, -1.3184e-01,  3.4424e-02, -3.6914e-01,  2.3047e-01,\n",
       "          -9.4727e-02, -6.2500e-02,  8.1055e-02,  1.9629e-01,  8.3984e-02,\n",
       "           2.3242e-01, -1.4258e-01,  1.4160e-01, -1.7578e-01,  2.3340e-01,\n",
       "          -1.1768e-01, -1.0010e-02, -1.5625e-01, -1.2207e-03,  3.2471e-02,\n",
       "          -1.3965e-01, -4.1992e-01,  5.3406e-03, -2.7539e-01,  1.0693e-01,\n",
       "          -3.5547e-01, -9.8145e-02,  2.2559e-01,  8.9844e-02, -3.4375e-01,\n",
       "          -3.4668e-02,  5.7373e-02, -1.4941e-01, -1.6211e-01,  6.2988e-02,\n",
       "           3.4180e-02, -4.5312e-01, -1.0596e-01, -2.2070e-01, -2.7930e-01,\n",
       "          -2.7100e-02, -3.5938e-01, -1.6113e-01, -4.1211e-01, -1.1377e-01,\n",
       "           9.1309e-02,  5.3223e-02, -1.1597e-02, -8.3984e-02,  1.6357e-02,\n",
       "          -4.1211e-01, -2.8125e-01,  2.5195e-01,  6.2012e-02, -2.1582e-01,\n",
       "          -1.0498e-01, -1.6406e-01, -1.7285e-01,  1.9141e-01, -4.4434e-02,\n",
       "           1.7188e-01, -1.6992e-01, -3.3203e-01,  2.4023e-01, -6.8125e+00,\n",
       "          -5.0781e-01,  3.8281e-01, -2.3340e-01,  2.9297e-01, -4.0430e-01,\n",
       "           3.4961e-01,  1.6895e-01,  1.7383e-01,  1.4746e-01,  1.0596e-01,\n",
       "          -8.8867e-02,  5.0354e-03, -3.3398e-01,  1.3184e-01, -3.1494e-02,\n",
       "           2.2168e-01, -3.5645e-02, -5.7373e-02,  2.4414e-01,  3.4570e-01,\n",
       "           4.9805e-02,  6.8848e-02,  1.9434e-01,  4.2480e-02, -8.1543e-02,\n",
       "          -8.1543e-02, -1.0645e-01,  5.0391e-01, -3.5547e-01,  6.5430e-02,\n",
       "          -1.3086e-01, -1.6797e-01, -4.5312e-01,  3.0273e-01,  7.9590e-02,\n",
       "           1.5747e-02,  2.1094e-01,  2.3242e-01,  3.9258e-01,  2.0898e-01,\n",
       "          -3.9795e-02,  5.9082e-02,  2.2461e-01,  9.4727e-02, -2.7344e-02,\n",
       "          -1.3281e-01, -1.2451e-01, -6.5430e-02,  3.1738e-02, -3.8867e-01,\n",
       "           2.3633e-01, -2.1875e-01, -1.8652e-01, -2.6855e-02,  1.5039e-01,\n",
       "           2.7710e-02, -2.8125e-01, -3.5400e-02,  7.1094e-01, -1.6504e-01,\n",
       "          -1.0107e-01,  3.3691e-02,  1.4258e-01, -1.4551e-01,  7.5781e-01,\n",
       "           8.9844e-02,  5.5908e-02, -1.6504e-01, -2.2656e-01,  6.0059e-02,\n",
       "          -1.7456e-02, -3.6377e-02,  5.0293e-02,  2.6611e-02,  2.7930e-01,\n",
       "          -1.8158e-03, -9.2773e-02, -7.3730e-02, -6.9336e-02, -5.0781e-02,\n",
       "           2.3193e-03, -8.7891e-02,  6.9336e-02,  4.4678e-02, -8.0078e-02,\n",
       "          -5.2979e-02, -1.1670e-01,  1.3281e-01,  2.2461e-02, -1.8262e-01,\n",
       "           1.6406e-01,  1.2695e-02,  4.7266e-01, -1.5723e-01, -7.9590e-02,\n",
       "          -5.7373e-02,  2.6123e-02, -1.1279e-01, -7.3242e-02, -1.2451e-02,\n",
       "          -5.5420e-02, -1.1621e-01, -1.6992e-01, -7.2266e-02,  6.2988e-02,\n",
       "           7.8201e-04,  5.5664e-02, -2.1191e-01, -6.3965e-02, -1.2573e-02,\n",
       "           4.1260e-02, -6.4941e-02,  2.2168e-01, -8.9844e-02, -8.0078e-02,\n",
       "          -1.2256e-01, -7.6660e-02,  2.3145e-01,  4.6997e-03, -6.4697e-03,\n",
       "           9.0820e-02, -3.9551e-02, -1.1768e-01, -2.4536e-02, -1.1084e-01,\n",
       "          -9.9121e-02,  9.9121e-02,  9.1309e-02, -9.5703e-02, -1.2598e-01,\n",
       "          -5.9570e-02,  8.2031e-02, -2.6001e-02,  7.8125e-02,  2.2217e-02,\n",
       "           3.1738e-02, -1.1768e-01,  1.3086e-01,  3.1445e-01,  5.8838e-02,\n",
       "           2.2559e-01, -7.7637e-02, -1.9043e-01, -4.7852e-02, -4.4556e-03,\n",
       "          -9.3750e-02, -9.4238e-02, -6.3782e-03,  1.2793e-01, -4.7119e-02,\n",
       "           1.8066e-02,  5.4199e-02, -1.1230e-01, -1.0400e-01, -8.3984e-02,\n",
       "           5.4443e-02, -1.2109e-01, -1.4465e-02, -1.6968e-02,  1.2891e-01,\n",
       "          -1.5234e-01,  4.3945e-02,  3.6621e-02,  3.3447e-02, -2.8198e-02,\n",
       "          -3.0396e-02, -7.3730e-02, -1.3574e-01, -1.4160e-01,  5.4321e-03,\n",
       "          -1.9629e-01, -1.1914e-01,  3.7695e-01, -2.1729e-02, -2.1094e-01,\n",
       "          -9.5215e-02, -3.7598e-02,  3.7598e-02, -1.9922e-01, -1.7944e-02,\n",
       "          -1.3281e-01, -3.9551e-02,  5.2490e-02,  5.6763e-03, -1.6699e-01,\n",
       "           9.5215e-02,  1.2939e-02, -1.2109e-01,  1.0449e-01, -4.6484e-01,\n",
       "          -3.0469e-01,  3.6719e-01, -1.2109e-01, -4.6875e-01,  2.6758e-01,\n",
       "          -2.3633e-01, -1.5234e-01, -5.1172e-01, -4.3359e-01,  2.9102e-01,\n",
       "           7.4707e-02,  2.4121e-01,  1.6113e-01,  2.5195e-01, -7.0312e-01,\n",
       "          -3.0078e-01, -1.1719e-01, -2.2949e-01, -1.2305e-01, -3.5938e-01,\n",
       "           4.4141e-01,  2.0020e-01,  3.8477e-01, -3.1738e-02, -8.3984e-02,\n",
       "           3.0273e-01,  6.7969e-01, -2.4414e-01,  7.3730e-02,  3.7891e-01,\n",
       "           3.6914e-01,  5.9326e-02, -4.4678e-02, -4.1211e-01, -9.5215e-02,\n",
       "          -8.8379e-02,  9.9609e-02,  2.3926e-01, -2.3730e-01, -6.5625e-01,\n",
       "           3.1836e-01,  3.0664e-01, -2.7734e-01,  2.0703e-01,  2.2363e-01,\n",
       "          -1.8945e-01,  1.9434e-01, -4.3945e-01,  2.6978e-02, -1.4453e-01,\n",
       "           5.3906e-01,  5.8594e-01, -2.3242e-01,  1.8945e-01, -2.8125e-01,\n",
       "          -2.0703e-01,  5.1172e-01, -5.1172e-01,  7.9688e-01, -3.5742e-01,\n",
       "          -6.4062e-01, -4.1797e-01, -2.0117e-01, -1.6113e-01, -2.7344e-01,\n",
       "           9.2773e-02,  3.9844e-01, -5.5859e-01, -5.3516e-01,  1.3379e-01,\n",
       "          -3.6328e-01, -1.4844e-01, -3.1836e-01, -5.7812e-01, -6.6016e-01,\n",
       "          -7.0312e-02, -7.0703e-01,  8.9844e-01,  1.6968e-02,  4.3164e-01,\n",
       "          -3.3984e-01, -2.4316e-01, -4.1211e-01,  4.5703e-01, -1.2988e-01,\n",
       "           2.0312e+00,  5.3125e-01,  2.1094e-01,  4.0527e-02, -5.1562e-01,\n",
       "          -1.2305e-01,  6.2500e-01, -1.6016e-01, -6.2500e-01, -4.9023e-01,\n",
       "           3.5742e-01,  2.6758e-01, -4.3750e-01,  1.0156e-01, -2.0996e-01,\n",
       "           5.3906e-01,  2.3730e-01, -2.7734e-01, -2.5977e-01,  6.9336e-02,\n",
       "           3.4375e-01, -2.5024e-02,  3.3008e-01, -1.9922e-01, -6.5625e-01,\n",
       "          -5.3516e-01,  6.1719e-01,  6.2500e-01, -6.2500e-01,  2.8320e-01,\n",
       "          -1.1084e-01,  1.3672e-01, -5.5420e-02, -1.5137e-01, -8.0078e-02,\n",
       "          -1.6309e-01,  6.6406e-01,  1.1084e-01,  9.0332e-02,  4.9805e-02,\n",
       "          -5.0391e-01,  1.8945e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-2.3651e-04,  6.6528e-03,  1.2390e-02,  ...,  8.2397e-03,\n",
       "           -1.9897e-02, -1.1841e-02],\n",
       "          [-1.0010e-02,  2.6245e-02, -3.2227e-02,  ...,  1.0010e-02,\n",
       "            6.4850e-05, -2.3651e-03],\n",
       "          [ 5.2490e-02, -3.5858e-04,  1.7578e-02,  ..., -3.7231e-03,\n",
       "           -1.5869e-02,  1.2329e-02],\n",
       "          ...,\n",
       "          [-1.0803e-02,  8.0566e-03, -2.8076e-03,  ...,  1.0437e-02,\n",
       "           -1.5747e-02,  2.0630e-02],\n",
       "          [-6.8054e-03, -1.7853e-03, -3.4424e-02,  ..., -6.5308e-03,\n",
       "            7.2479e-04,  1.7776e-03],\n",
       "          [ 1.1047e-02,  2.0752e-02,  1.3550e-02,  ...,  1.6098e-03,\n",
       "            3.9368e-03, -3.4180e-02]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0142, -0.0082, -0.0089,  ..., -0.0089,  0.0042, -0.0265],\n",
       "          [-0.0018, -0.0332, -0.0043,  ..., -0.0052, -0.0046, -0.0122],\n",
       "          [-0.0020,  0.0081,  0.0203,  ...,  0.0261, -0.0226,  0.0125],\n",
       "          ...,\n",
       "          [ 0.0275, -0.0193,  0.0008,  ..., -0.0212,  0.0069,  0.0104],\n",
       "          [ 0.0113, -0.0376,  0.0087,  ...,  0.0082, -0.0088,  0.0100],\n",
       "          [ 0.0109, -0.0063,  0.0042,  ...,  0.0018,  0.0153,  0.0045]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0019,  0.0187, -0.0025,  ...,  0.0033, -0.0239,  0.0132],\n",
       "          [-0.0061,  0.0077, -0.0090,  ...,  0.0034, -0.0148, -0.0097],\n",
       "          [ 0.0461, -0.0195,  0.0061,  ...,  0.0138, -0.0386, -0.0302],\n",
       "          ...,\n",
       "          [-0.0043,  0.0079, -0.0002,  ..., -0.0139,  0.0024,  0.0234],\n",
       "          [-0.0200, -0.0275, -0.0144,  ...,  0.0376,  0.0143,  0.0173],\n",
       "          [ 0.0008, -0.0112, -0.0131,  ..., -0.0096, -0.0347, -0.0013]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-1.1658e-02, -3.9978e-03, -2.6093e-03,  ...,  1.5945e-03,\n",
       "            1.8311e-03, -2.9419e-02],\n",
       "          [-1.3855e-02,  2.1484e-02, -5.2795e-03,  ..., -2.2583e-02,\n",
       "            3.7109e-02,  2.4902e-02],\n",
       "          [ 1.3428e-02,  1.5198e-02, -1.3367e-02,  ..., -1.7456e-02,\n",
       "            1.6846e-02,  2.3193e-02],\n",
       "          ...,\n",
       "          [-2.7466e-02,  1.5106e-03,  3.8330e-02,  ...,  2.3560e-02,\n",
       "            1.5793e-03,  1.6861e-03],\n",
       "          [ 3.2227e-02, -1.9775e-02,  9.8267e-03,  ..., -1.6174e-03,\n",
       "           -3.5553e-03, -3.5248e-03],\n",
       "          [-3.0060e-03, -9.7046e-03,  1.1353e-02,  ..., -2.5391e-02,\n",
       "           -9.5825e-03,  1.2100e-05]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([0.7773, 0.5977, 0.6602,  ..., 0.6992, 0.6953, 0.7070],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([1.6562, 1.6406, 1.6875,  ..., 1.5469, 1.6719, 1.6641],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0116, -0.0134, -0.0129,  ...,  0.0107,  0.0122,  0.0063],\n",
       "          [ 0.0200,  0.0068, -0.0004,  ..., -0.0147,  0.0095,  0.0239],\n",
       "          [-0.0160,  0.0139,  0.0066,  ...,  0.0008, -0.0201,  0.0095],\n",
       "          ...,\n",
       "          [-0.0289,  0.0210,  0.0408,  ..., -0.0123, -0.0201, -0.0239],\n",
       "          [ 0.0036,  0.0059, -0.0559,  ...,  0.0381, -0.0083,  0.0123],\n",
       "          [-0.0087,  0.0007, -0.0165,  ...,  0.0299, -0.0112, -0.0134]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 0.0208, -0.2773,  0.2041,  ...,  0.1079, -4.0625,  0.3945],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-2.4261e-03, -1.2268e-02, -1.5747e-02,  ...,  1.2436e-03,\n",
       "            7.8735e-03, -1.8158e-03],\n",
       "          [ 5.7220e-04,  5.4626e-03,  8.6060e-03,  ...,  2.3651e-03,\n",
       "            3.0756e-05,  3.9978e-03],\n",
       "          [ 1.2451e-02, -1.2451e-02, -9.7046e-03,  ..., -4.1504e-03,\n",
       "            1.2085e-02,  5.4626e-03],\n",
       "          ...,\n",
       "          [-1.0757e-03,  5.1880e-03, -4.3640e-03,  ..., -1.9043e-02,\n",
       "           -1.1780e-02,  3.1586e-03],\n",
       "          [-2.4170e-02,  2.5635e-03, -1.1963e-02,  ..., -3.3417e-03,\n",
       "           -1.2146e-02,  5.0659e-03],\n",
       "          [-1.6479e-02, -6.2561e-04,  2.2430e-03,  ..., -1.8433e-02,\n",
       "           -1.1292e-02, -3.6926e-03]], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([ 1.7285e-01, -5.0537e-02, -1.0156e-01, -1.7480e-01,  1.9531e-01,\n",
       "          -1.9531e-01, -3.4375e-01, -1.3477e-01,  1.8066e-01, -1.4844e-01,\n",
       "           1.5234e-01, -4.0625e-01, -5.5664e-02, -1.9824e-01, -2.0703e-01,\n",
       "          -1.6797e-01,  1.8750e-01, -1.7090e-01,  5.5859e-01,  4.6680e-01,\n",
       "           1.9238e-01, -3.1836e-01,  2.4707e-01, -6.0156e-01, -2.7148e-01,\n",
       "          -2.1582e-01,  9.4141e-01, -1.9531e-01,  4.8633e-01,  3.1445e-01,\n",
       "          -8.5449e-02, -3.7695e-01, -1.8555e-01,  1.7871e-01,  2.2583e-02,\n",
       "          -1.0625e+00, -3.9062e-01,  4.2969e-01, -1.3065e-04,  2.4531e+00,\n",
       "          -4.4189e-02,  3.1055e-01, -1.9531e-01,  1.0312e+00, -2.8516e-01,\n",
       "           6.3477e-02, -6.7969e-01,  3.4570e-01, -3.2422e-01, -5.6641e-01,\n",
       "          -4.8242e-01,  4.3945e-01, -7.2266e-01,  4.4727e-01, -2.8516e-01,\n",
       "           9.9219e-01,  1.1719e+00, -5.9766e-01,  1.2969e+00,  1.5781e+00,\n",
       "          -4.0820e-01,  6.0938e-01,  3.9648e-01,  2.0410e-01,  8.3984e-02,\n",
       "          -1.9824e-01,  2.4219e-01,  7.8125e-02, -2.3926e-01, -2.9175e-02,\n",
       "           1.6211e-01,  1.0352e-01,  2.7930e-01, -1.9434e-01, -7.3242e-02,\n",
       "           6.5430e-02, -1.8262e-01,  2.5586e-01,  2.9883e-01, -5.3906e-01,\n",
       "          -6.9824e-02, -2.8320e-01,  2.4219e-01, -3.8477e-01, -5.7031e-01,\n",
       "           5.3125e-01,  3.4912e-02,  4.0039e-01,  6.9141e-01,  6.2500e-01,\n",
       "          -2.0215e-01,  6.1719e-01, -1.0742e-01,  6.8359e-01,  3.1836e-01,\n",
       "           1.8672e+00, -1.0391e+00, -1.0400e-01, -2.7148e-01, -1.1641e+00,\n",
       "          -1.1279e-01,  3.5156e-01,  1.7871e-01, -3.4570e-01, -2.1875e-01,\n",
       "          -2.3438e-01, -7.6172e-02,  2.7812e+00,  2.1582e-01,  8.2812e-01,\n",
       "          -3.4375e-01,  1.3750e+00,  3.3281e+00, -3.5938e-01, -1.9434e-01,\n",
       "           1.9727e-01,  7.8516e-01,  4.9805e-01, -4.2188e-01,  1.9375e+00,\n",
       "          -6.8750e-01,  2.0020e-01,  6.0156e-01,  5.2188e+00, -3.0859e-01,\n",
       "           1.5625e+00, -5.5469e-01, -1.0703e+00,  2.3125e+00,  1.5547e+00,\n",
       "           1.3750e+00,  4.9023e-01,  6.5625e-01,  1.4453e+00,  1.5078e+00,\n",
       "           9.9219e-01, -5.4297e-01, -1.0391e+00, -5.6250e-01, -4.7266e-01,\n",
       "           2.6875e+00, -6.1328e-01,  1.5039e-01,  8.6719e-01, -9.8047e-01,\n",
       "          -2.8564e-02, -1.2734e+00, -2.6611e-02, -6.9922e-01, -2.8438e+00,\n",
       "          -4.9219e-01, -9.5312e-01,  1.0156e+00, -2.9375e+00,  6.9336e-02,\n",
       "          -2.0938e+00,  1.0469e+00, -6.0938e-01,  1.6406e-01,  2.8076e-02,\n",
       "           2.8125e-01,  8.5449e-02,  1.1484e+00,  1.7656e+00, -1.7734e+00,\n",
       "           1.2109e-01, -6.7969e-01, -7.0312e-01,  6.9141e-01,  1.4531e+00,\n",
       "           7.3828e-01,  4.0625e-01,  2.6875e+00, -4.3438e+00, -1.3250e+01,\n",
       "          -1.1500e+01, -1.2969e+00,  5.5078e-01,  1.0875e+01, -6.3750e+01,\n",
       "          -1.0938e+00, -4.6500e+01, -4.5938e+00,  2.7875e+01,  9.3500e+01,\n",
       "          -6.2750e+01,  1.8600e+02,  8.9500e+01,  2.1500e+02, -4.3000e+01,\n",
       "          -4.0000e+02, -3.0000e+02,  2.2344e+00,  1.6953e+00,  5.3516e-01,\n",
       "          -2.3750e+00,  9.5312e-01, -7.2656e-01,  6.9531e-01, -9.9121e-02,\n",
       "           3.8477e-01, -2.5625e+00, -1.5391e+00, -4.8633e-01, -1.2031e+00,\n",
       "          -4.7070e-01,  5.4688e-01,  1.1172e+00, -1.0625e+00, -1.5547e+00,\n",
       "           4.3359e-01, -4.1504e-02,  5.3125e-01, -1.8828e+00, -6.2500e-01,\n",
       "           1.6094e+00, -1.1797e+00,  2.5000e+00,  1.1875e+00,  8.6719e-01,\n",
       "           6.1328e-01, -2.5781e-01,  9.3750e-01,  1.4219e+00,  1.2891e+00,\n",
       "          -6.5234e-01, -1.4062e+00,  9.6094e-01, -9.3750e-01, -8.2422e-01,\n",
       "          -2.5781e+00,  9.2969e-01,  7.3438e-01, -1.2656e+00,  6.2891e-01,\n",
       "           2.5330e-03, -3.6562e+00,  3.3125e+00,  1.2438e+01,  1.1688e+01,\n",
       "          -1.8750e+00,  1.9531e+00, -1.3062e+01, -1.4875e+01,  1.0703e+00,\n",
       "           1.8000e+01,  8.0625e+00, -2.8125e+00,  2.3625e+01,  9.2000e+01,\n",
       "          -1.5800e+02, -2.0100e+02,  2.3300e+02, -3.7000e+02, -2.1900e+02,\n",
       "          -4.1400e+02, -3.3203e-01,  1.7773e-01,  1.6895e-01,  3.3936e-02,\n",
       "           1.4893e-02,  3.0151e-02, -2.8711e-01, -1.2500e-01, -2.0215e-01,\n",
       "          -1.6992e-01,  5.1880e-03, -7.9346e-03,  3.6133e-01, -1.8164e-01,\n",
       "          -1.4453e-01,  8.7891e-03,  2.5000e-01, -3.3008e-01,  5.8984e-01,\n",
       "          -2.5391e-01, -6.2500e-02,  7.1484e-01,  1.4258e-01,  8.3203e-01,\n",
       "          -6.1719e-01,  3.2422e-01, -1.2793e-01, -4.1016e-01, -1.9043e-01,\n",
       "          -1.4160e-01,  7.5781e-01, -1.0938e-01, -7.9590e-02,  4.2725e-02,\n",
       "           1.2812e+00,  1.4355e-01,  2.6953e-01, -1.4648e-01, -2.3242e-01,\n",
       "           2.2969e+00, -1.5039e-01,  2.3340e-01,  8.1055e-02,  1.8359e+00,\n",
       "          -5.8838e-02,  2.7344e-02, -2.5391e-01,  8.7500e-01,  3.1250e+00,\n",
       "           3.2812e-01, -4.0625e-01,  2.3535e-01,  4.6680e-01, -1.8652e-01,\n",
       "           1.0703e+00, -7.5781e-01, -5.1562e+00, -5.5859e-01,  1.1172e+00,\n",
       "          -3.1836e-01,  6.0156e-01,  4.1016e-01, -8.3008e-02,  3.4570e-01,\n",
       "          -1.0205e-01,  1.6895e-01,  1.2891e-01,  2.7734e-01, -3.0396e-02,\n",
       "          -5.5176e-02,  4.3640e-03, -1.8652e-01, -5.9326e-02, -1.3672e-01,\n",
       "           2.4316e-01,  1.4941e-01, -1.2891e-01,  2.0117e-01, -1.3477e-01,\n",
       "           4.0039e-01, -2.7930e-01,  1.9531e-01, -2.4414e-01,  1.8433e-02,\n",
       "          -2.0508e-01, -5.6641e-01, -8.3008e-02, -6.0156e-01, -8.5938e-02,\n",
       "          -3.7305e-01, -2.0605e-01, -1.2188e+00, -3.6133e-01, -2.3242e-01,\n",
       "           5.7812e-01,  1.8047e+00,  2.6367e-01, -4.2969e-01, -8.4766e-01,\n",
       "           4.6289e-01,  1.2109e-01, -8.9355e-02,  7.2266e-02,  4.3164e-01,\n",
       "           4.2188e-01, -1.3867e-01,  1.0059e-01,  2.2500e+00,  3.7891e-01,\n",
       "           1.8677e-02, -1.0469e+00, -5.0000e-01, -4.3164e-01, -4.1199e-03,\n",
       "           3.4375e-01,  5.1514e-02, -5.8594e-01,  7.8516e-01,  1.1182e-01,\n",
       "          -8.9453e-01,  1.5938e+00,  6.1719e-01, -7.3438e-01,  1.6641e+00,\n",
       "          -8.1250e-01, -8.3203e-01,  3.1250e-01, -1.4355e-01, -6.4941e-02,\n",
       "           2.9883e-01,  3.1641e-01, -1.6113e-01,  4.7070e-01,  2.1387e-01,\n",
       "           1.7676e-01, -4.1406e-01, -3.2422e-01, -1.5918e-01,  2.1362e-02,\n",
       "          -3.1982e-02,  1.8750e-01,  2.3828e-01, -5.8203e-01,  1.2207e-01,\n",
       "          -1.3770e-01, -1.0010e-01,  2.2266e-01, -4.5312e-01, -3.3594e-01,\n",
       "          -3.7305e-01, -3.5156e-01, -3.9648e-01,  2.9102e-01, -1.7578e-01,\n",
       "          -9.1406e-01,  2.0117e-01,  5.7422e-01, -2.8516e-01, -7.4005e-04,\n",
       "           2.6172e-01,  7.5000e-01, -1.4941e-01,  2.8320e-01,  1.6895e-01,\n",
       "          -8.6719e-01, -3.2227e-01, -1.7734e+00,  3.0884e-02, -1.9336e-01,\n",
       "           1.5991e-02,  1.0234e+00, -2.4121e-01,  4.9023e-01, -1.2354e-01,\n",
       "          -3.8086e-02,  2.9297e-01, -2.0625e+00,  2.5000e-01, -3.7500e-01,\n",
       "           1.2061e-01, -7.1777e-02,  6.7969e-01, -1.7773e-01,  4.3359e-01,\n",
       "           1.2656e+00,  8.9844e-01, -2.7344e-01,  7.4219e-01,  1.0596e-01,\n",
       "          -6.1328e-01,  1.7773e-01,  7.9956e-03, -3.2471e-02, -1.6406e-01,\n",
       "           4.9744e-03, -9.7656e-02, -2.8442e-02, -6.0059e-02,  2.1289e-01,\n",
       "          -8.6914e-02,  4.8340e-02, -2.4805e-01, -4.9414e-01, -4.1602e-01,\n",
       "          -3.5742e-01, -6.9336e-02,  1.8750e-01, -4.0234e-01, -1.4160e-01,\n",
       "           8.7891e-02,  7.9688e-01,  1.4453e-01,  5.2002e-02,  3.9795e-02,\n",
       "          -6.4062e-01,  5.3516e-01,  7.5000e-01, -2.3828e-01,  3.6133e-01,\n",
       "           6.6895e-02,  9.4141e-01, -4.1211e-01, -2.5391e-01,  1.7812e+00,\n",
       "          -3.0664e-01, -2.6562e-01, -3.8672e-01,  6.3477e-03, -6.4844e-01,\n",
       "           8.2520e-02, -6.9531e-01,  4.3945e-02, -4.9805e-01, -1.3062e-02,\n",
       "           2.5469e+00, -4.0820e-01, -5.4688e-02, -3.9673e-03,  3.2812e-01,\n",
       "           1.7422e+00, -3.4180e-01, -3.1875e+00, -1.8555e-01, -1.4941e-01,\n",
       "          -7.7148e-02,  6.2891e-01,  3.1250e-01,  3.7500e-01,  4.0312e+00,\n",
       "          -5.7031e-01, -1.0312e+00,  1.1719e+00, -9.8145e-02, -1.0469e+00,\n",
       "          -1.1523e-01, -5.7422e-01], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0209,  0.0114,  0.0396,  ...,  0.0216, -0.0003,  0.0508],\n",
       "          [ 0.0330, -0.0081, -0.0016,  ..., -0.0035, -0.0354, -0.0199],\n",
       "          [-0.0109, -0.0488, -0.0542,  ..., -0.0149,  0.0302,  0.0344],\n",
       "          ...,\n",
       "          [-0.0391,  0.0183,  0.0044,  ..., -0.0437,  0.0081, -0.0184],\n",
       "          [ 0.0280, -0.0141,  0.0244,  ..., -0.0522,  0.0017, -0.0059],\n",
       "          [-0.0164, -0.0194,  0.0067,  ..., -0.0070,  0.0198,  0.0018]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([-2.0605e-01,  3.0273e-01,  4.5898e-01, -8.6426e-02,  2.1582e-01,\n",
       "           1.6211e-01,  2.6758e-01, -2.0410e-01,  1.8945e-01,  1.7090e-01,\n",
       "          -2.1484e-02, -2.7148e-01, -1.1719e-01,  3.8867e-01,  3.1250e-01,\n",
       "          -3.2227e-01,  4.1602e-01, -3.6914e-01, -8.6914e-02, -2.7930e-01,\n",
       "          -1.6113e-01, -5.1953e-01,  2.3926e-02,  3.2422e-01, -3.3789e-01,\n",
       "          -1.4453e-01, -3.5352e-01,  2.3047e-01, -9.5703e-02, -2.4121e-01,\n",
       "          -2.9297e-01,  2.0996e-01, -2.9492e-01,  1.2207e-01, -2.5391e-01,\n",
       "          -6.1523e-02,  3.2617e-01, -2.1680e-01, -2.2363e-01,  2.9688e-01,\n",
       "          -1.9141e-01,  2.7930e-01,  2.3047e-01,  3.5938e-01,  2.4414e-01,\n",
       "           1.4844e-01,  1.5527e-01,  2.3828e-01, -1.1377e-01,  2.4609e-01,\n",
       "          -2.2949e-01, -2.9492e-01,  1.7188e-01,  6.8359e-02,  4.4922e-02,\n",
       "          -1.2012e-01, -4.7266e-01,  2.4805e-01, -1.0205e-01,  4.1602e-01,\n",
       "           2.1777e-01,  1.5332e-01,  2.4121e-01, -2.1680e-01,  1.5039e-01,\n",
       "          -1.6602e-01, -8.3008e-02, -1.2598e-01, -2.8906e-01, -9.6191e-02,\n",
       "           1.5332e-01,  1.2695e-01,  2.4512e-01, -2.9883e-01, -3.8867e-01,\n",
       "           3.0078e-01,  4.3945e-01, -3.3984e-01,  2.5977e-01, -3.3594e-01,\n",
       "          -8.4473e-02,  4.2578e-01, -5.3125e-01,  1.5820e-01, -4.1992e-02,\n",
       "          -5.1758e-02,  3.1641e-01, -2.0703e-01, -1.8066e-01,  3.2812e-01,\n",
       "          -4.8828e-01, -2.9492e-01,  3.1445e-01,  1.3086e-01, -1.0840e-01,\n",
       "          -2.1387e-01, -2.8320e-01,  1.4062e-01,  3.7891e-01,  1.2012e-01,\n",
       "           4.9072e-02, -1.7480e-01,  6.3477e-02, -4.0820e-01,  4.1016e-01,\n",
       "          -1.0986e-01, -2.9883e-01, -3.3008e-01,  5.1172e-01,  1.6992e-01,\n",
       "           1.9434e-01, -2.5586e-01,  2.0508e-01,  4.7461e-01,  1.8262e-01,\n",
       "           4.3750e-01, -7.5684e-02, -2.2363e-01, -4.2383e-01, -1.4746e-01,\n",
       "           7.2266e-02,  1.5234e-01, -1.5820e-01,  1.1865e-01, -1.9141e-01,\n",
       "           7.8125e-02, -2.3340e-01, -2.8125e-01,  2.2188e+00, -1.8750e+00,\n",
       "          -2.2188e+00,  3.4844e+00, -2.5156e+00,  1.7871e-01,  2.0996e-01,\n",
       "          -3.3438e+00, -7.7344e-01,  2.8281e+00,  3.0625e+00, -3.0625e+00,\n",
       "          -2.9375e+00,  5.1562e-01,  2.4688e+00,  1.3203e+00,  1.3906e+00,\n",
       "           3.0000e+00,  2.9531e+00,  2.5469e+00, -3.2344e+00,  2.0781e+00,\n",
       "           1.5625e+00, -2.6094e+00, -2.8281e+00,  1.3125e+00,  3.0625e+00,\n",
       "           2.3594e+00,  1.6094e+00, -2.5000e+00, -2.7344e+00,  2.6875e+00,\n",
       "          -4.6289e-01, -1.8594e+00,  2.4844e+00,  5.0938e+00, -2.8906e+00,\n",
       "           3.8594e+00,  1.5234e+00, -2.1406e+00, -2.4688e+00, -5.1250e+00,\n",
       "          -3.0938e+00,  2.0625e+00,  2.0605e-01, -2.8750e+00, -3.1094e+00,\n",
       "           1.6406e+00,  1.5938e+00,  2.7344e+00, -2.3281e+00,  2.6250e+00,\n",
       "          -2.1719e+00, -2.0156e+00, -2.4062e+00,  2.7812e+00, -2.9375e+00,\n",
       "           2.0156e+00, -2.9531e+00,  3.4375e+00, -1.3281e+00,  2.4375e+00,\n",
       "          -2.3594e+00,  2.6562e+00,  2.8750e+00, -2.4531e+00,  2.0312e+00,\n",
       "           2.4531e+00,  4.8828e-01, -5.7812e-01, -2.5156e+00, -1.9062e+00,\n",
       "          -2.3750e+00, -2.4219e+00,  3.0312e+00,  2.8594e+00, -2.7656e+00,\n",
       "          -2.5781e+00, -9.7046e-03,  2.2656e+00,  3.3594e+00,  2.1875e+00,\n",
       "          -1.8125e+00,  2.5781e+00,  2.2656e+00,  2.3594e+00,  2.4688e+00,\n",
       "          -4.6289e-01, -3.6562e+00,  3.0312e+00,  3.1875e+00, -2.1719e+00,\n",
       "          -2.8594e+00,  3.2188e+00,  7.8906e-01, -1.1719e+00,  3.7656e+00,\n",
       "          -3.5000e+00, -2.5469e+00,  2.9688e+00,  2.1875e+00,  2.4688e+00,\n",
       "          -3.0000e+00, -2.8750e+00,  2.1094e+00,  1.2969e+00, -2.7500e+00,\n",
       "           2.5781e+00, -1.2969e+00,  2.6250e+00, -7.3828e-01, -2.2188e+00,\n",
       "           2.1562e+00, -3.6406e+00,  1.5312e+00, -2.2031e+00, -3.4375e+00,\n",
       "          -2.7188e+00, -3.7891e-01,  1.4531e+00, -2.2969e+00,  4.3750e-01,\n",
       "          -2.1562e+00, -3.2500e+00,  1.5312e+00,  3.6094e+00, -2.8125e+00,\n",
       "          -1.5469e+00, -1.4258e-01,  5.0391e-01, -4.0820e-01, -9.8633e-02,\n",
       "           3.0859e-01,  4.3213e-02, -5.1880e-03,  1.2402e-01, -1.5039e-01,\n",
       "          -1.3123e-02,  6.7578e-01, -3.6523e-01,  4.3164e-01,  1.6797e-01,\n",
       "          -2.8906e-01,  2.2461e-01, -2.6758e-01, -7.7148e-02,  1.2354e-01,\n",
       "          -6.1035e-02, -1.4465e-02,  1.1279e-01,  4.1992e-01, -1.6309e-01,\n",
       "           2.5781e-01, -1.1230e-01, -3.4570e-01,  4.3945e-03, -1.5430e-01,\n",
       "          -6.6406e-02,  2.8320e-01, -4.1748e-02,  2.7539e-01,  4.1992e-01,\n",
       "          -5.7422e-01,  1.5234e-01,  3.7695e-01, -2.2339e-02,  6.8359e-02,\n",
       "           2.8125e-01, -3.2031e-01, -2.5977e-01, -6.7383e-02, -2.4023e-01,\n",
       "          -2.6489e-02, -2.4536e-02, -6.7871e-02, -6.7383e-02,  1.3477e-01,\n",
       "          -2.5977e-01,  1.0010e-01, -6.0059e-02, -5.3906e-01, -2.0996e-01,\n",
       "           2.2852e-01, -2.1973e-01, -6.2500e-02,  7.6660e-02,  6.0938e-01,\n",
       "           2.1484e-02,  1.5723e-01, -3.2617e-01,  1.4453e-01,  1.3086e-01,\n",
       "           3.0762e-02, -6.3965e-02,  1.5234e-01,  6.1279e-02,  3.2227e-01,\n",
       "          -1.2500e-01, -2.2949e-02, -2.3340e-01,  7.9956e-03, -4.5703e-01,\n",
       "           2.7930e-01, -3.2031e-01, -4.7461e-01, -3.3984e-01,  1.8750e-01,\n",
       "           1.0254e-01,  4.5117e-01, -1.5723e-01, -3.2031e-01,  3.0708e-04,\n",
       "           2.1387e-01,  8.1177e-03,  3.9258e-01, -3.2031e-01,  2.7930e-01,\n",
       "           4.1406e-01,  6.6895e-02, -2.0020e-01, -2.6489e-02, -2.5586e-01,\n",
       "          -1.9165e-02, -7.8125e-02,  1.2061e-01, -7.7148e-02, -6.2256e-02,\n",
       "          -1.3550e-02, -8.2520e-02,  1.2305e-01,  5.9375e-01, -7.3242e-02,\n",
       "           4.2383e-01,  2.1680e-01,  2.6758e-01, -3.0640e-02, -1.2793e-01,\n",
       "           1.6504e-01, -1.3770e-01,  2.1777e-01, -2.7734e-01, -1.2988e-01,\n",
       "           5.1514e-02,  9.7168e-02, -1.2268e-02,  3.5938e-01,  3.4668e-02,\n",
       "          -1.6895e-01, -8.6426e-02,  2.0801e-01,  4.2969e-01, -1.9141e-01,\n",
       "          -3.2959e-03, -3.6719e-01, -1.5430e-01, -3.0469e-01,  2.2070e-01,\n",
       "          -1.5820e-01, -3.2715e-02, -1.6309e-01, -1.6113e-01,  1.7871e-01,\n",
       "           1.7676e-01, -6.8848e-02, -1.4648e-01,  1.1133e-01,  6.2256e-02,\n",
       "          -1.3477e-01, -9.3750e-02, -2.4219e-01, -1.0205e-01, -3.6133e-02,\n",
       "           5.1758e-02,  1.1621e-01,  3.9844e-01, -3.4912e-02, -9.2285e-02,\n",
       "           7.4463e-03,  4.8584e-02, -6.4453e-02,  2.7344e-01,  1.9727e-01,\n",
       "          -2.0312e-01,  1.0681e-03,  5.1514e-02, -5.9570e-02,  2.6367e-01,\n",
       "           1.5918e-01,  3.0859e-01, -4.2969e-02, -1.4355e-01,  1.7285e-01,\n",
       "           1.3184e-01,  4.9805e-02, -8.1055e-02,  5.2185e-03,  8.9355e-02,\n",
       "          -3.7842e-02,  9.8145e-02,  2.0117e-01, -2.2266e-01,  3.1055e-01,\n",
       "          -3.0273e-01,  1.9409e-02,  8.4961e-02, -2.1973e-01, -8.3984e-02,\n",
       "          -4.0283e-02,  1.2012e-01,  2.1777e-01, -5.2002e-02, -5.0781e-02,\n",
       "           1.7285e-01, -6.4941e-02, -2.5781e-01,  1.1816e-01,  1.5430e-01,\n",
       "          -6.8848e-02,  4.0625e-01, -9.6191e-02, -5.5859e-01,  8.6060e-03,\n",
       "          -9.0942e-03,  8.8379e-02, -2.1582e-01,  3.8147e-03, -4.0820e-01,\n",
       "          -4.7461e-01, -2.7954e-02,  9.7168e-02,  9.6191e-02,  1.2988e-01,\n",
       "          -1.9043e-01,  8.3496e-02,  6.0625e+00, -1.6602e-01,  8.3496e-02,\n",
       "          -1.2158e-01, -2.5000e-01, -1.4844e-01, -7.2266e-02, -3.6523e-01,\n",
       "           7.6172e-02, -2.9602e-03,  1.3184e-01, -1.2085e-02, -6.0791e-02,\n",
       "          -1.0156e-01,  2.0410e-01,  3.6133e-01, -2.5586e-01, -1.1670e-01,\n",
       "           2.1118e-02,  1.7383e-01, -1.6992e-01, -1.6113e-01,  1.0889e-01,\n",
       "          -2.5586e-01, -2.2949e-01,  1.0107e-01,  1.2695e-01,  5.6250e+00,\n",
       "           2.9785e-02,  1.1780e-02,  1.8066e-01, -5.9570e-02, -3.0664e-01,\n",
       "          -4.0039e-01,  1.2207e-01,  2.7734e-01, -7.1289e-02,  2.6172e-01,\n",
       "           1.5527e-01, -1.1414e-02,  3.7354e-02,  5.1758e-02, -2.5146e-02,\n",
       "           7.1777e-02, -5.3467e-02,  3.7305e-01, -3.3112e-03, -3.6328e-01,\n",
       "           6.2500e-02,  7.8613e-02], dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0123, -0.0413,  0.0058,  ...,  0.0005, -0.0055, -0.0103],\n",
       "          [ 0.0166,  0.0159,  0.0021,  ...,  0.0244,  0.0084, -0.0107],\n",
       "          [ 0.0005,  0.0126,  0.0237,  ..., -0.0095, -0.0145,  0.0139],\n",
       "          ...,\n",
       "          [ 0.0082, -0.0164,  0.0123,  ...,  0.0014, -0.0122,  0.0072],\n",
       "          [-0.0073,  0.0032,  0.0143,  ..., -0.0057,  0.0262,  0.0016],\n",
       "          [-0.0186, -0.0464,  0.0242,  ..., -0.0339,  0.0444, -0.0415]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[-0.0157,  0.0028,  0.0177,  ..., -0.0046,  0.0030, -0.0054],\n",
       "          [ 0.0101,  0.0117, -0.0278,  ...,  0.0039, -0.0028,  0.0320],\n",
       "          [ 0.0139, -0.0322,  0.0081,  ..., -0.0295, -0.0068, -0.0084],\n",
       "          ...,\n",
       "          [ 0.0140,  0.0114, -0.0371,  ..., -0.0099, -0.0120,  0.0008],\n",
       "          [ 0.0168,  0.0008,  0.0186,  ..., -0.0258, -0.0034, -0.0034],\n",
       "          [ 0.0161,  0.0148,  0.0187,  ...,  0.0159,  0.0059, -0.0135]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0177, -0.0391, -0.0084,  ...,  0.0374,  0.0128, -0.0084],\n",
       "          [-0.0014,  0.0195, -0.0017,  ..., -0.0110,  0.0047,  0.0337],\n",
       "          [ 0.0167,  0.0112,  0.0498,  ..., -0.0014, -0.0009, -0.0203],\n",
       "          ...,\n",
       "          [-0.0009, -0.0172,  0.0420,  ...,  0.0320, -0.0259, -0.0116],\n",
       "          [-0.0042, -0.0011, -0.0009,  ..., -0.0305,  0.0014, -0.0225],\n",
       "          [-0.0206, -0.0104, -0.0248,  ..., -0.0114, -0.0034,  0.0081]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([[ 0.0166, -0.0012,  0.0110,  ...,  0.0086, -0.0027,  0.0170],\n",
       "          [ 0.0122, -0.0008, -0.0282,  ...,  0.0159, -0.0076,  0.0237],\n",
       "          [ 0.0038,  0.0085,  0.0075,  ..., -0.0219, -0.0308,  0.0053],\n",
       "          ...,\n",
       "          [-0.0098, -0.0028,  0.0205,  ..., -0.0232,  0.0084,  0.0304],\n",
       "          [-0.0103,  0.0092,  0.0361,  ...,  0.0220, -0.0073,  0.0288],\n",
       "          [-0.0068, -0.0186, -0.0398,  ...,  0.0177,  0.0035, -0.0105]],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([0.6094, 0.5352, 0.5625,  ..., 0.5781, 0.5391, 0.5234],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05},\n",
       " {'params': Parameter containing:\n",
       "  tensor([2.0938, 2.0781, 2.0781,  ..., 2.0781, 2.0625, 2.0781],\n",
       "         dtype=torch.bfloat16, requires_grad=True),\n",
       "  'lr': 5e-05}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen_optim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_optim = optim.AdamW(vae_optim_dict, lr=lr)\n",
    "qwen_optim = optim.AdamW(qwen_optim_dict, lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "dataset_dir = 'D:/info/program/NLP/asm/dataset/'\n",
    "\n",
    "train_list = glob.glob(dataset_dir + 'train*.json')\n",
    "val_list = glob.glob(dataset_dir + 'val*.json')\n",
    "len(train_list), len(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruozhi_dataset import Ruozhi_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train1 = Ruozhi_dataset(train_list[0], tokenizer = tokenizer, sequence_length = sequence_length)\n",
    "val1 = Ruozhi_dataset(val_list[0], tokenizer = tokenizer, sequence_length = sequence_length)\n",
    "\n",
    "train_loader1 = DataLoader(train1, batch_size=batch_size, shuffle=True)\n",
    "val_loader1 = DataLoader(val1, batch_size=len(val1), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-01 16:46:15.008950\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "2024-11-01 16:46:47.173572 Epoch 0 start\n",
      "torch.Size([4, 128, 3584])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "could not create a primitive descriptor for an LSTM forward propagation primitive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m input_masking \u001b[38;5;241m=\u001b[39m input_masking\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m output_token \u001b[38;5;241m=\u001b[39m output_token\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m               \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_masking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\info\\program\\NLP\\asm\\Qwen_vae\\qwen_source.py:1173\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[0;32m   1170\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\info\\program\\NLP\\asm\\Qwen_vae\\qwen_source.py:953\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    951\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28mprint\u001b[39m(hidden_states\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 953\u001b[0m vae_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    954\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m vae_out[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    956\u001b[0m \u001b[38;5;66;03m# create position embeddings to be shared across the decoder layers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\info\\program\\NLP\\asm\\Qwen_vae\\beta_VAE.py:176\u001b[0m, in \u001b[0;36mBetaVAE.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tensor]:\n\u001b[1;32m--> 176\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z), \u001b[38;5;28minput\u001b[39m, mu, log_var]\n",
      "File \u001b[1;32md:\\info\\program\\NLP\\asm\\Qwen_vae\\beta_VAE.py:137\u001b[0m, in \u001b[0;36mBetaVAE.encode\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03mEncodes the input by passing through the encoder network\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03mand returns the latent codes.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m:param input: (Tensor) Input tensor to encoder [N x L x D]\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m:return: (Tensor) List of latent codes\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lstm, bn, relu \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_lstm_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_bn_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_relu_layers):\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28minput\u001b[39m, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bn(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\17539\\anaconda3\\envs\\nlpasm\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:917\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    921\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: could not create a primitive descriptor for an LSTM forward propagation primitive"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(\"Start training\")\n",
    "for epoch in range(1):\n",
    "    print(\"{} Epoch {} start\".format(datetime.datetime.now(), epoch))\n",
    "    i = 0\n",
    "    for input_token, input_masking, output_token in train_loader1:\n",
    "        i += 1\n",
    "        input_token = input_token.squeeze(1)\n",
    "        input_masking = input_masking.squeeze(1)\n",
    "        output_token = output_token.squeeze(1)\n",
    "\n",
    "        \n",
    "        input_token = input_token.to(device)\n",
    "        input_masking = input_masking.to(device)\n",
    "        output_token = output_token.to(device)\n",
    "        \n",
    "        \n",
    "        output = model(input_ids = input_token, \n",
    "                       attention_mask = input_masking,\n",
    "                       labels = output_token)\n",
    "        print(\"forward end\")\n",
    "        loss = output.loss\n",
    "        vae_out = output.vae_out\n",
    "        loss_dict = model.vae.loss_function(*vae_out, M_N=16 / 3200)\n",
    "        vae_loss  = loss_dict[\"loss\"]\n",
    "\n",
    "\n",
    "        qwen_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        qwen_optim.step()\n",
    "\n",
    "        vae_optim.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optim.step()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{datetime.datetime.now()} Epoch {epoch + 1}, Training Loss: {loss.item()}\")\n",
    "    # for input_token, output_token in val_loader1:\n",
    "    #     with torch.no_grad():\n",
    "    #         input_token = input_token.view(batch_size, sequence_length)\n",
    "    #         output_token = output_token.view(batch_size, sequence_length)\n",
    "\n",
    "    #         input_token = input_token.to(device)\n",
    "    #         output_token = output_token.to(device)\n",
    "\n",
    "\n",
    "    #         output = model(input_ids = input_token, labels = output_token)\n",
    "    #         loss = output.loss\n",
    "\n",
    "    #         print(f\"{datetime.datetime.now()} Epoch {epoch + 1}, Validation Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight    False\n",
      "model.vae.encoder_lstm_layers.0.weight_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.0.weight_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.0.bias_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.0.bias_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.0.weight_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.weight_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.bias_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.bias_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.weight_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.0.weight_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.0.bias_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.0.bias_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.0.weight_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.weight_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.bias_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.0.bias_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.weight_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.1.weight_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.1.bias_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.1.bias_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.1.weight_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.weight_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.bias_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.bias_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.weight_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.1.weight_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.1.bias_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.1.bias_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.1.weight_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.weight_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.bias_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.1.bias_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.weight_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.2.weight_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.2.bias_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.2.bias_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.2.weight_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.weight_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.bias_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.bias_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.weight_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.2.weight_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.2.bias_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.2.bias_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.2.weight_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.weight_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.bias_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.2.bias_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.weight_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.3.weight_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.3.bias_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.3.bias_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.3.weight_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.weight_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.bias_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.bias_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.weight_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.3.weight_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.3.bias_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.3.bias_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.3.weight_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.weight_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.bias_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.3.bias_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.weight_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.4.weight_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.4.bias_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.4.bias_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.4.weight_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.weight_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.bias_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.bias_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.weight_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.4.weight_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.4.bias_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.4.bias_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.4.weight_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.weight_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.bias_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.4.bias_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.weight_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.5.weight_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.5.bias_ih_l0    True\n",
      "model.vae.encoder_lstm_layers.5.bias_hh_l0    True\n",
      "model.vae.encoder_lstm_layers.5.weight_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.weight_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.bias_ih_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.bias_hh_l0_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.weight_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.5.weight_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.5.bias_ih_l1    True\n",
      "model.vae.encoder_lstm_layers.5.bias_hh_l1    True\n",
      "model.vae.encoder_lstm_layers.5.weight_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.weight_hh_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.bias_ih_l1_reverse    True\n",
      "model.vae.encoder_lstm_layers.5.bias_hh_l1_reverse    True\n",
      "model.vae.encoder_bn_layers.0.weight    True\n",
      "model.vae.encoder_bn_layers.0.bias    True\n",
      "model.vae.encoder_bn_layers.1.weight    True\n",
      "model.vae.encoder_bn_layers.1.bias    True\n",
      "model.vae.encoder_bn_layers.2.weight    True\n",
      "model.vae.encoder_bn_layers.2.bias    True\n",
      "model.vae.encoder_bn_layers.3.weight    True\n",
      "model.vae.encoder_bn_layers.3.bias    True\n",
      "model.vae.encoder_bn_layers.4.weight    True\n",
      "model.vae.encoder_bn_layers.4.bias    True\n",
      "model.vae.encoder_bn_layers.5.weight    True\n",
      "model.vae.encoder_bn_layers.5.bias    True\n",
      "model.vae.fc_mu.weight    True\n",
      "model.vae.fc_mu.bias    True\n",
      "model.vae.fc_var.weight    True\n",
      "model.vae.fc_var.bias    True\n",
      "model.vae.decoder_input.weight    True\n",
      "model.vae.decoder_input.bias    True\n",
      "model.vae.decoder_lstm_layers.0.weight_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.0.weight_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.0.bias_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.0.bias_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.0.weight_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.0.weight_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.0.bias_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.0.bias_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.1.weight_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.1.weight_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.1.bias_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.1.bias_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.1.weight_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.1.weight_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.1.bias_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.1.bias_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.2.weight_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.2.weight_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.2.bias_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.2.bias_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.2.weight_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.2.weight_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.2.bias_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.2.bias_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.3.weight_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.3.weight_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.3.bias_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.3.bias_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.3.weight_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.3.weight_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.3.bias_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.3.bias_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.4.weight_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.4.weight_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.4.bias_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.4.bias_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.4.weight_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.4.weight_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.4.bias_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.4.bias_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.5.weight_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.5.weight_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.5.bias_ih_l0    True\n",
      "model.vae.decoder_lstm_layers.5.bias_hh_l0    True\n",
      "model.vae.decoder_lstm_layers.5.weight_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.5.weight_hh_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.5.bias_ih_l0_reverse    True\n",
      "model.vae.decoder_lstm_layers.5.bias_hh_l0_reverse    True\n",
      "model.vae.decoder_bn_layers.0.weight    True\n",
      "model.vae.decoder_bn_layers.0.bias    True\n",
      "model.vae.decoder_bn_layers.1.weight    True\n",
      "model.vae.decoder_bn_layers.1.bias    True\n",
      "model.vae.decoder_bn_layers.2.weight    True\n",
      "model.vae.decoder_bn_layers.2.bias    True\n",
      "model.vae.decoder_bn_layers.3.weight    True\n",
      "model.vae.decoder_bn_layers.3.bias    True\n",
      "model.vae.decoder_bn_layers.4.weight    True\n",
      "model.vae.decoder_bn_layers.4.bias    True\n",
      "model.vae.decoder_bn_layers.5.weight    True\n",
      "model.vae.decoder_bn_layers.5.bias    True\n",
      "model.layers.0.self_attn.q_proj.weight    False\n",
      "model.layers.0.self_attn.q_proj.bias    False\n",
      "model.layers.0.self_attn.k_proj.weight    False\n",
      "model.layers.0.self_attn.k_proj.bias    False\n",
      "model.layers.0.self_attn.v_proj.weight    False\n",
      "model.layers.0.self_attn.v_proj.bias    False\n",
      "model.layers.0.self_attn.o_proj.weight    False\n",
      "model.layers.0.mlp.gate_proj.weight    False\n",
      "model.layers.0.mlp.up_proj.weight    False\n",
      "model.layers.0.mlp.down_proj.weight    False\n",
      "model.layers.0.input_layernorm.weight    False\n",
      "model.layers.0.post_attention_layernorm.weight    False\n",
      "model.layers.1.self_attn.q_proj.weight    False\n",
      "model.layers.1.self_attn.q_proj.bias    False\n",
      "model.layers.1.self_attn.k_proj.weight    False\n",
      "model.layers.1.self_attn.k_proj.bias    False\n",
      "model.layers.1.self_attn.v_proj.weight    False\n",
      "model.layers.1.self_attn.v_proj.bias    False\n",
      "model.layers.1.self_attn.o_proj.weight    False\n",
      "model.layers.1.mlp.gate_proj.weight    False\n",
      "model.layers.1.mlp.up_proj.weight    False\n",
      "model.layers.1.mlp.down_proj.weight    False\n",
      "model.layers.1.input_layernorm.weight    False\n",
      "model.layers.1.post_attention_layernorm.weight    False\n",
      "model.layers.2.self_attn.q_proj.weight    False\n",
      "model.layers.2.self_attn.q_proj.bias    False\n",
      "model.layers.2.self_attn.k_proj.weight    False\n",
      "model.layers.2.self_attn.k_proj.bias    False\n",
      "model.layers.2.self_attn.v_proj.weight    False\n",
      "model.layers.2.self_attn.v_proj.bias    False\n",
      "model.layers.2.self_attn.o_proj.weight    False\n",
      "model.layers.2.mlp.gate_proj.weight    False\n",
      "model.layers.2.mlp.up_proj.weight    False\n",
      "model.layers.2.mlp.down_proj.weight    False\n",
      "model.layers.2.input_layernorm.weight    False\n",
      "model.layers.2.post_attention_layernorm.weight    False\n",
      "model.layers.3.self_attn.q_proj.weight    False\n",
      "model.layers.3.self_attn.q_proj.bias    False\n",
      "model.layers.3.self_attn.k_proj.weight    False\n",
      "model.layers.3.self_attn.k_proj.bias    False\n",
      "model.layers.3.self_attn.v_proj.weight    False\n",
      "model.layers.3.self_attn.v_proj.bias    False\n",
      "model.layers.3.self_attn.o_proj.weight    False\n",
      "model.layers.3.mlp.gate_proj.weight    False\n",
      "model.layers.3.mlp.up_proj.weight    False\n",
      "model.layers.3.mlp.down_proj.weight    False\n",
      "model.layers.3.input_layernorm.weight    False\n",
      "model.layers.3.post_attention_layernorm.weight    False\n",
      "model.layers.4.self_attn.q_proj.weight    False\n",
      "model.layers.4.self_attn.q_proj.bias    False\n",
      "model.layers.4.self_attn.k_proj.weight    False\n",
      "model.layers.4.self_attn.k_proj.bias    False\n",
      "model.layers.4.self_attn.v_proj.weight    False\n",
      "model.layers.4.self_attn.v_proj.bias    False\n",
      "model.layers.4.self_attn.o_proj.weight    False\n",
      "model.layers.4.mlp.gate_proj.weight    False\n",
      "model.layers.4.mlp.up_proj.weight    False\n",
      "model.layers.4.mlp.down_proj.weight    False\n",
      "model.layers.4.input_layernorm.weight    False\n",
      "model.layers.4.post_attention_layernorm.weight    False\n",
      "model.layers.5.self_attn.q_proj.weight    False\n",
      "model.layers.5.self_attn.q_proj.bias    False\n",
      "model.layers.5.self_attn.k_proj.weight    False\n",
      "model.layers.5.self_attn.k_proj.bias    False\n",
      "model.layers.5.self_attn.v_proj.weight    False\n",
      "model.layers.5.self_attn.v_proj.bias    False\n",
      "model.layers.5.self_attn.o_proj.weight    False\n",
      "model.layers.5.mlp.gate_proj.weight    False\n",
      "model.layers.5.mlp.up_proj.weight    False\n",
      "model.layers.5.mlp.down_proj.weight    False\n",
      "model.layers.5.input_layernorm.weight    False\n",
      "model.layers.5.post_attention_layernorm.weight    False\n",
      "model.layers.6.self_attn.q_proj.weight    False\n",
      "model.layers.6.self_attn.q_proj.bias    False\n",
      "model.layers.6.self_attn.k_proj.weight    False\n",
      "model.layers.6.self_attn.k_proj.bias    False\n",
      "model.layers.6.self_attn.v_proj.weight    False\n",
      "model.layers.6.self_attn.v_proj.bias    False\n",
      "model.layers.6.self_attn.o_proj.weight    False\n",
      "model.layers.6.mlp.gate_proj.weight    False\n",
      "model.layers.6.mlp.up_proj.weight    False\n",
      "model.layers.6.mlp.down_proj.weight    False\n",
      "model.layers.6.input_layernorm.weight    False\n",
      "model.layers.6.post_attention_layernorm.weight    False\n",
      "model.layers.7.self_attn.q_proj.weight    False\n",
      "model.layers.7.self_attn.q_proj.bias    False\n",
      "model.layers.7.self_attn.k_proj.weight    False\n",
      "model.layers.7.self_attn.k_proj.bias    False\n",
      "model.layers.7.self_attn.v_proj.weight    False\n",
      "model.layers.7.self_attn.v_proj.bias    False\n",
      "model.layers.7.self_attn.o_proj.weight    False\n",
      "model.layers.7.mlp.gate_proj.weight    False\n",
      "model.layers.7.mlp.up_proj.weight    False\n",
      "model.layers.7.mlp.down_proj.weight    False\n",
      "model.layers.7.input_layernorm.weight    False\n",
      "model.layers.7.post_attention_layernorm.weight    False\n",
      "model.layers.8.self_attn.q_proj.weight    False\n",
      "model.layers.8.self_attn.q_proj.bias    False\n",
      "model.layers.8.self_attn.k_proj.weight    False\n",
      "model.layers.8.self_attn.k_proj.bias    False\n",
      "model.layers.8.self_attn.v_proj.weight    False\n",
      "model.layers.8.self_attn.v_proj.bias    False\n",
      "model.layers.8.self_attn.o_proj.weight    False\n",
      "model.layers.8.mlp.gate_proj.weight    False\n",
      "model.layers.8.mlp.up_proj.weight    False\n",
      "model.layers.8.mlp.down_proj.weight    False\n",
      "model.layers.8.input_layernorm.weight    False\n",
      "model.layers.8.post_attention_layernorm.weight    False\n",
      "model.layers.9.self_attn.q_proj.weight    False\n",
      "model.layers.9.self_attn.q_proj.bias    False\n",
      "model.layers.9.self_attn.k_proj.weight    False\n",
      "model.layers.9.self_attn.k_proj.bias    False\n",
      "model.layers.9.self_attn.v_proj.weight    False\n",
      "model.layers.9.self_attn.v_proj.bias    False\n",
      "model.layers.9.self_attn.o_proj.weight    False\n",
      "model.layers.9.mlp.gate_proj.weight    False\n",
      "model.layers.9.mlp.up_proj.weight    False\n",
      "model.layers.9.mlp.down_proj.weight    False\n",
      "model.layers.9.input_layernorm.weight    False\n",
      "model.layers.9.post_attention_layernorm.weight    False\n",
      "model.layers.10.self_attn.q_proj.weight    False\n",
      "model.layers.10.self_attn.q_proj.bias    False\n",
      "model.layers.10.self_attn.k_proj.weight    False\n",
      "model.layers.10.self_attn.k_proj.bias    False\n",
      "model.layers.10.self_attn.v_proj.weight    False\n",
      "model.layers.10.self_attn.v_proj.bias    False\n",
      "model.layers.10.self_attn.o_proj.weight    False\n",
      "model.layers.10.mlp.gate_proj.weight    False\n",
      "model.layers.10.mlp.up_proj.weight    False\n",
      "model.layers.10.mlp.down_proj.weight    False\n",
      "model.layers.10.input_layernorm.weight    False\n",
      "model.layers.10.post_attention_layernorm.weight    False\n",
      "model.layers.11.self_attn.q_proj.weight    False\n",
      "model.layers.11.self_attn.q_proj.bias    False\n",
      "model.layers.11.self_attn.k_proj.weight    False\n",
      "model.layers.11.self_attn.k_proj.bias    False\n",
      "model.layers.11.self_attn.v_proj.weight    False\n",
      "model.layers.11.self_attn.v_proj.bias    False\n",
      "model.layers.11.self_attn.o_proj.weight    False\n",
      "model.layers.11.mlp.gate_proj.weight    False\n",
      "model.layers.11.mlp.up_proj.weight    False\n",
      "model.layers.11.mlp.down_proj.weight    False\n",
      "model.layers.11.input_layernorm.weight    False\n",
      "model.layers.11.post_attention_layernorm.weight    False\n",
      "model.layers.12.self_attn.q_proj.weight    False\n",
      "model.layers.12.self_attn.q_proj.bias    False\n",
      "model.layers.12.self_attn.k_proj.weight    False\n",
      "model.layers.12.self_attn.k_proj.bias    False\n",
      "model.layers.12.self_attn.v_proj.weight    False\n",
      "model.layers.12.self_attn.v_proj.bias    False\n",
      "model.layers.12.self_attn.o_proj.weight    False\n",
      "model.layers.12.mlp.gate_proj.weight    False\n",
      "model.layers.12.mlp.up_proj.weight    False\n",
      "model.layers.12.mlp.down_proj.weight    False\n",
      "model.layers.12.input_layernorm.weight    False\n",
      "model.layers.12.post_attention_layernorm.weight    False\n",
      "model.layers.13.self_attn.q_proj.weight    False\n",
      "model.layers.13.self_attn.q_proj.bias    False\n",
      "model.layers.13.self_attn.k_proj.weight    False\n",
      "model.layers.13.self_attn.k_proj.bias    False\n",
      "model.layers.13.self_attn.v_proj.weight    False\n",
      "model.layers.13.self_attn.v_proj.bias    False\n",
      "model.layers.13.self_attn.o_proj.weight    False\n",
      "model.layers.13.mlp.gate_proj.weight    False\n",
      "model.layers.13.mlp.up_proj.weight    False\n",
      "model.layers.13.mlp.down_proj.weight    False\n",
      "model.layers.13.input_layernorm.weight    False\n",
      "model.layers.13.post_attention_layernorm.weight    False\n",
      "model.layers.14.self_attn.q_proj.weight    False\n",
      "model.layers.14.self_attn.q_proj.bias    False\n",
      "model.layers.14.self_attn.k_proj.weight    False\n",
      "model.layers.14.self_attn.k_proj.bias    False\n",
      "model.layers.14.self_attn.v_proj.weight    False\n",
      "model.layers.14.self_attn.v_proj.bias    False\n",
      "model.layers.14.self_attn.o_proj.weight    False\n",
      "model.layers.14.mlp.gate_proj.weight    False\n",
      "model.layers.14.mlp.up_proj.weight    False\n",
      "model.layers.14.mlp.down_proj.weight    False\n",
      "model.layers.14.input_layernorm.weight    False\n",
      "model.layers.14.post_attention_layernorm.weight    False\n",
      "model.layers.15.self_attn.q_proj.weight    False\n",
      "model.layers.15.self_attn.q_proj.bias    False\n",
      "model.layers.15.self_attn.k_proj.weight    False\n",
      "model.layers.15.self_attn.k_proj.bias    False\n",
      "model.layers.15.self_attn.v_proj.weight    False\n",
      "model.layers.15.self_attn.v_proj.bias    False\n",
      "model.layers.15.self_attn.o_proj.weight    False\n",
      "model.layers.15.mlp.gate_proj.weight    False\n",
      "model.layers.15.mlp.up_proj.weight    False\n",
      "model.layers.15.mlp.down_proj.weight    False\n",
      "model.layers.15.input_layernorm.weight    False\n",
      "model.layers.15.post_attention_layernorm.weight    False\n",
      "model.layers.16.self_attn.q_proj.weight    False\n",
      "model.layers.16.self_attn.q_proj.bias    False\n",
      "model.layers.16.self_attn.k_proj.weight    False\n",
      "model.layers.16.self_attn.k_proj.bias    False\n",
      "model.layers.16.self_attn.v_proj.weight    False\n",
      "model.layers.16.self_attn.v_proj.bias    False\n",
      "model.layers.16.self_attn.o_proj.weight    False\n",
      "model.layers.16.mlp.gate_proj.weight    False\n",
      "model.layers.16.mlp.up_proj.weight    False\n",
      "model.layers.16.mlp.down_proj.weight    False\n",
      "model.layers.16.input_layernorm.weight    False\n",
      "model.layers.16.post_attention_layernorm.weight    False\n",
      "model.layers.17.self_attn.q_proj.weight    False\n",
      "model.layers.17.self_attn.q_proj.bias    False\n",
      "model.layers.17.self_attn.k_proj.weight    False\n",
      "model.layers.17.self_attn.k_proj.bias    False\n",
      "model.layers.17.self_attn.v_proj.weight    False\n",
      "model.layers.17.self_attn.v_proj.bias    False\n",
      "model.layers.17.self_attn.o_proj.weight    False\n",
      "model.layers.17.mlp.gate_proj.weight    False\n",
      "model.layers.17.mlp.up_proj.weight    False\n",
      "model.layers.17.mlp.down_proj.weight    False\n",
      "model.layers.17.input_layernorm.weight    False\n",
      "model.layers.17.post_attention_layernorm.weight    False\n",
      "model.layers.18.self_attn.q_proj.weight    False\n",
      "model.layers.18.self_attn.q_proj.bias    False\n",
      "model.layers.18.self_attn.k_proj.weight    False\n",
      "model.layers.18.self_attn.k_proj.bias    False\n",
      "model.layers.18.self_attn.v_proj.weight    False\n",
      "model.layers.18.self_attn.v_proj.bias    False\n",
      "model.layers.18.self_attn.o_proj.weight    False\n",
      "model.layers.18.mlp.gate_proj.weight    False\n",
      "model.layers.18.mlp.up_proj.weight    False\n",
      "model.layers.18.mlp.down_proj.weight    False\n",
      "model.layers.18.input_layernorm.weight    False\n",
      "model.layers.18.post_attention_layernorm.weight    False\n",
      "model.layers.19.self_attn.q_proj.weight    False\n",
      "model.layers.19.self_attn.q_proj.bias    False\n",
      "model.layers.19.self_attn.k_proj.weight    False\n",
      "model.layers.19.self_attn.k_proj.bias    False\n",
      "model.layers.19.self_attn.v_proj.weight    False\n",
      "model.layers.19.self_attn.v_proj.bias    False\n",
      "model.layers.19.self_attn.o_proj.weight    False\n",
      "model.layers.19.mlp.gate_proj.weight    False\n",
      "model.layers.19.mlp.up_proj.weight    False\n",
      "model.layers.19.mlp.down_proj.weight    False\n",
      "model.layers.19.input_layernorm.weight    False\n",
      "model.layers.19.post_attention_layernorm.weight    False\n",
      "model.layers.20.self_attn.q_proj.weight    False\n",
      "model.layers.20.self_attn.q_proj.bias    False\n",
      "model.layers.20.self_attn.k_proj.weight    False\n",
      "model.layers.20.self_attn.k_proj.bias    False\n",
      "model.layers.20.self_attn.v_proj.weight    False\n",
      "model.layers.20.self_attn.v_proj.bias    False\n",
      "model.layers.20.self_attn.o_proj.weight    False\n",
      "model.layers.20.mlp.gate_proj.weight    False\n",
      "model.layers.20.mlp.up_proj.weight    False\n",
      "model.layers.20.mlp.down_proj.weight    False\n",
      "model.layers.20.input_layernorm.weight    False\n",
      "model.layers.20.post_attention_layernorm.weight    False\n",
      "model.layers.21.self_attn.q_proj.weight    False\n",
      "model.layers.21.self_attn.q_proj.bias    False\n",
      "model.layers.21.self_attn.k_proj.weight    False\n",
      "model.layers.21.self_attn.k_proj.bias    False\n",
      "model.layers.21.self_attn.v_proj.weight    False\n",
      "model.layers.21.self_attn.v_proj.bias    False\n",
      "model.layers.21.self_attn.o_proj.weight    False\n",
      "model.layers.21.mlp.gate_proj.weight    False\n",
      "model.layers.21.mlp.up_proj.weight    False\n",
      "model.layers.21.mlp.down_proj.weight    False\n",
      "model.layers.21.input_layernorm.weight    False\n",
      "model.layers.21.post_attention_layernorm.weight    False\n",
      "model.layers.22.self_attn.q_proj.weight    False\n",
      "model.layers.22.self_attn.q_proj.bias    False\n",
      "model.layers.22.self_attn.k_proj.weight    False\n",
      "model.layers.22.self_attn.k_proj.bias    False\n",
      "model.layers.22.self_attn.v_proj.weight    False\n",
      "model.layers.22.self_attn.v_proj.bias    False\n",
      "model.layers.22.self_attn.o_proj.weight    False\n",
      "model.layers.22.mlp.gate_proj.weight    False\n",
      "model.layers.22.mlp.up_proj.weight    False\n",
      "model.layers.22.mlp.down_proj.weight    False\n",
      "model.layers.22.input_layernorm.weight    False\n",
      "model.layers.22.post_attention_layernorm.weight    False\n",
      "model.layers.23.self_attn.q_proj.weight    True\n",
      "model.layers.23.self_attn.q_proj.bias    True\n",
      "model.layers.23.self_attn.k_proj.weight    True\n",
      "model.layers.23.self_attn.k_proj.bias    True\n",
      "model.layers.23.self_attn.v_proj.weight    True\n",
      "model.layers.23.self_attn.v_proj.bias    True\n",
      "model.layers.23.self_attn.o_proj.weight    True\n",
      "model.layers.23.mlp.gate_proj.weight    True\n",
      "model.layers.23.mlp.up_proj.weight    True\n",
      "model.layers.23.mlp.down_proj.weight    True\n",
      "model.layers.23.input_layernorm.weight    True\n",
      "model.layers.23.post_attention_layernorm.weight    True\n",
      "model.layers.24.self_attn.q_proj.weight    True\n",
      "model.layers.24.self_attn.q_proj.bias    True\n",
      "model.layers.24.self_attn.k_proj.weight    True\n",
      "model.layers.24.self_attn.k_proj.bias    True\n",
      "model.layers.24.self_attn.v_proj.weight    True\n",
      "model.layers.24.self_attn.v_proj.bias    True\n",
      "model.layers.24.self_attn.o_proj.weight    True\n",
      "model.layers.24.mlp.gate_proj.weight    True\n",
      "model.layers.24.mlp.up_proj.weight    True\n",
      "model.layers.24.mlp.down_proj.weight    True\n",
      "model.layers.24.input_layernorm.weight    True\n",
      "model.layers.24.post_attention_layernorm.weight    True\n",
      "model.layers.25.self_attn.q_proj.weight    True\n",
      "model.layers.25.self_attn.q_proj.bias    True\n",
      "model.layers.25.self_attn.k_proj.weight    True\n",
      "model.layers.25.self_attn.k_proj.bias    True\n",
      "model.layers.25.self_attn.v_proj.weight    True\n",
      "model.layers.25.self_attn.v_proj.bias    True\n",
      "model.layers.25.self_attn.o_proj.weight    True\n",
      "model.layers.25.mlp.gate_proj.weight    True\n",
      "model.layers.25.mlp.up_proj.weight    True\n",
      "model.layers.25.mlp.down_proj.weight    True\n",
      "model.layers.25.input_layernorm.weight    True\n",
      "model.layers.25.post_attention_layernorm.weight    True\n",
      "model.layers.26.self_attn.q_proj.weight    True\n",
      "model.layers.26.self_attn.q_proj.bias    True\n",
      "model.layers.26.self_attn.k_proj.weight    True\n",
      "model.layers.26.self_attn.k_proj.bias    True\n",
      "model.layers.26.self_attn.v_proj.weight    True\n",
      "model.layers.26.self_attn.v_proj.bias    True\n",
      "model.layers.26.self_attn.o_proj.weight    True\n",
      "model.layers.26.mlp.gate_proj.weight    True\n",
      "model.layers.26.mlp.up_proj.weight    True\n",
      "model.layers.26.mlp.down_proj.weight    True\n",
      "model.layers.26.input_layernorm.weight    True\n",
      "model.layers.26.post_attention_layernorm.weight    True\n",
      "model.layers.27.self_attn.q_proj.weight    True\n",
      "model.layers.27.self_attn.q_proj.bias    True\n",
      "model.layers.27.self_attn.k_proj.weight    True\n",
      "model.layers.27.self_attn.k_proj.bias    True\n",
      "model.layers.27.self_attn.v_proj.weight    True\n",
      "model.layers.27.self_attn.v_proj.bias    True\n",
      "model.layers.27.self_attn.o_proj.weight    True\n",
      "model.layers.27.mlp.gate_proj.weight    True\n",
      "model.layers.27.mlp.up_proj.weight    True\n",
      "model.layers.27.mlp.down_proj.weight    True\n",
      "model.layers.27.input_layernorm.weight    True\n",
      "model.layers.27.post_attention_layernorm.weight    True\n",
      "model.norm.weight    True\n",
      "lm_head.weight    True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}    {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpasm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
